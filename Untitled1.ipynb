{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69eb73ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube-search-python in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (1.6.6)\n",
      "Requirement already satisfied: httpx>=0.14.2 in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from youtube-search-python) (0.23.1)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from httpx>=0.14.2->youtube-search-python) (1.5.0)\n",
      "Requirement already satisfied: certifi in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from httpx>=0.14.2->youtube-search-python) (2022.6.15)\n",
      "Requirement already satisfied: sniffio in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from httpx>=0.14.2->youtube-search-python) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from httpx>=0.14.2->youtube-search-python) (0.15.0)\n",
      "Requirement already satisfied: anyio==3.* in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from httpcore<0.17.0,>=0.15.0->httpx>=0.14.2->youtube-search-python) (3.6.2)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from httpcore<0.17.0,>=0.15.0->httpx>=0.14.2->youtube-search-python) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages (from anyio==3.*->httpcore<0.17.0,>=0.15.0->httpx>=0.14.2->youtube-search-python) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-search-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163187e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtubesearchpython import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5cfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = Playlist('https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3635ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos Retrieved: 23\n"
     ]
    }
   ],
   "source": [
    "print(f'Videos Retrieved: {len(playlist.videos)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019c8cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found all the videos.\n"
     ]
    }
   ],
   "source": [
    "while playlist.hasMoreVideos:\n",
    "    print('Getting more videos...')\n",
    "    playlist.getNextVideos()\n",
    "    print(f'Videos Retrieved: {len(playlist.videos)}')\n",
    "\n",
    "print('Found all the videos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt-dlp -x --audio-format mp3 -o {mp3_file} -- {youtube_video_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08935fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = []\n",
    "for i in range(len(playlist.videos)):\n",
    "    URLS.append(playlist.videos[i]['link'])\n",
    "#     print(playlist.videos[i]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Downloading playlist PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ - add --no-playlist to download just the video rmVRLeJRkl4\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Playlist Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021: Downloading 23 videos of 23\n",
      "[download] Downloading video 1 of 23\n",
      "[youtube] rmVRLeJRkl4: Downloading webpage\n",
      "[youtube] rmVRLeJRkl4: Downloading android player API JSON\n",
      "[info] rmVRLeJRkl4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm\n",
      "[download] 100% of   62.45MiB in 00:00:03 at 15.94MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm (pass -k to keep)\n",
      "[download] Downloading video 2 of 23\n",
      "[youtube] gqaHkPEZAew: Downloading webpage\n",
      "[youtube] gqaHkPEZAew: Downloading android player API JSON\n",
      "[info] gqaHkPEZAew: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm\n",
      "[download] 100% of   55.26MiB in 00:00:03 at 16.86MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm (pass -k to keep)\n",
      "[download] Downloading video 3 of 23\n",
      "[youtube] X0Jw4kgaFlg: Downloading webpage\n",
      "[youtube] X0Jw4kgaFlg: Downloading android player API JSON\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[info] X0Jw4kgaFlg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm\n",
      "[download] 100% of   78.70MiB in 00:00:05 at 14.71MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm (pass -k to keep)\n",
      "[download] Downloading video 4 of 23\n",
      "[youtube] PSGIodTN3KE: Downloading webpage\n",
      "[youtube] PSGIodTN3KE: Downloading android player API JSON\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[info] PSGIodTN3KE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm\n",
      "[download] 100% of   76.19MiB in 00:00:04 at 16.37MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].mp3\n",
      "Deleting original file Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm (pass -k to keep)\n",
      "[download] Downloading video 5 of 23\n",
      "[youtube] PLryWeHPcBs: Downloading webpage\n",
      "[youtube] PLryWeHPcBs: Downloading android player API JSON\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[info] PLryWeHPcBs: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm\n",
      "[download] 100% of   75.52MiB in 00:00:04 at 16.44MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].mp3\n",
      "Deleting original file Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm (pass -k to keep)\n",
      "[download] Downloading video 6 of 23\n",
      "[youtube] 0LixFSa7yts: Downloading webpage\n",
      "[youtube] 0LixFSa7yts: Downloading android player API JSON\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[info] 0LixFSa7yts: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm\n",
      "[download] 100% of   78.35MiB in 00:00:04 at 16.97MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm (pass -k to keep)\n",
      "[download] Downloading video 7 of 23\n",
      "[youtube] wzfWHP6SXxY: Downloading webpage\n",
      "[youtube] wzfWHP6SXxY: Downloading android player API JSON\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[info] wzfWHP6SXxY: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm\n",
      "[download] 100% of   75.59MiB in 00:00:04 at 16.31MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm (pass -k to keep)\n",
      "[download] Downloading video 8 of 23\n",
      "[youtube] gKD7jPAdbpE: Downloading webpage\n",
      "[youtube] gKD7jPAdbpE: Downloading android player API JSON\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[info] gKD7jPAdbpE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm\n",
      "[download] 100% of   75.69MiB in 00:00:04 at 16.63MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm (pass -k to keep)\n",
      "[download] Downloading video 9 of 23\n",
      "[youtube] ptuGllU5SQQ: Downloading webpage\n",
      "[youtube] ptuGllU5SQQ: Downloading android player API JSON\n",
      "[info] ptuGllU5SQQ: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm\n",
      "[download] 100% of   62.90MiB in 00:00:03 at 16.48MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm (pass -k to keep)\n",
      "[download] Downloading video 10 of 23\n",
      "[youtube] j9AcEI98C0o: Downloading webpage\n",
      "[youtube] j9AcEI98C0o: Downloading android player API JSON\n",
      "[info] j9AcEI98C0o: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm\n",
      "[download] 100% of   63.70MiB in 00:00:03 at 17.12MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm (pass -k to keep)\n",
      "[download] Downloading video 11 of 23\n",
      "[youtube] NcqfHa0_YmU: Downloading webpage\n",
      "[youtube] NcqfHa0_YmU: Downloading android player API JSON\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] NcqfHa0_YmU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm\n",
      "[download] 100% of  105.24MiB in 00:00:06 at 17.45MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm (pass -k to keep)\n",
      "[download] Downloading video 12 of 23\n",
      "[youtube] 1uMo8olr5ng: Downloading webpage\n",
      "[youtube] 1uMo8olr5ng: Downloading android player API JSON\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[info] 1uMo8olr5ng: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm\n",
      "[download] 100% of   81.56MiB in 00:00:04 at 17.25MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm (pass -k to keep)\n",
      "[download] Downloading video 13 of 23\n",
      "[youtube] FFRnDRcbQQU: Downloading webpage\n",
      "[youtube] FFRnDRcbQQU: Downloading android player API JSON\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[info] FFRnDRcbQQU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm\n",
      "[download] 100% of   81.34MiB in 00:00:04 at 16.86MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm (pass -k to keep)\n",
      "[download] Downloading video 14 of 23\n",
      "[youtube] iHWkLvoSpTg: Downloading webpage\n",
      "[youtube] iHWkLvoSpTg: Downloading android player API JSON\n",
      "[info] iHWkLvoSpTg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm\n",
      "[download] 100% of   94.67MiB in 00:00:05 at 16.46MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm (pass -k to keep)\n",
      "[download] Downloading video 15 of 23\n",
      "[youtube] y68RJVfGoto: Downloading webpage\n",
      "[youtube] y68RJVfGoto: Downloading android player API JSON\n",
      "[info] y68RJVfGoto: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm\n",
      "[download] 100% of   74.64MiB in 00:00:04 at 15.10MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm (pass -k to keep)\n",
      "[download] Downloading video 16 of 23\n",
      "[youtube] -Ldg4qFL6bU: Downloading webpage\n",
      "[youtube] -Ldg4qFL6bU: Downloading android player API JSON\n",
      "[info] -Ldg4qFL6bU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm\n",
      "[download] 100% of  102.55MiB in 00:00:05 at 17.12MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm (pass -k to keep)\n",
      "[download] Downloading video 17 of 23\n",
      "[youtube] f_qmSSBWV_E: Downloading webpage\n",
      "[youtube] f_qmSSBWV_E: Downloading android player API JSON\n",
      "[info] f_qmSSBWV_E: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm\n",
      "[download] 100% of   57.83MiB in 00:00:03 at 15.84MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm (pass -k to keep)\n",
      "[download] Downloading video 18 of 23\n",
      "[youtube] 2t7Q9WVUaf8: Downloading webpage\n",
      "[youtube] 2t7Q9WVUaf8: Downloading android player API JSON\n",
      "[info] 2t7Q9WVUaf8: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm\n",
      "[download] 100% of   73.18MiB in 00:00:04 at 17.47MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm (pass -k to keep)\n",
      "[download] Downloading video 19 of 23\n",
      "[youtube] mp95Z5yM92c: Downloading webpage\n",
      "[youtube] mp95Z5yM92c: Downloading android player API JSON\n",
      "[info] mp95Z5yM92c: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm\n",
      "[download] 100% of   56.56MiB in 00:00:03 at 14.95MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm (pass -k to keep)\n",
      "[download] Downloading video 20 of 23\n",
      "[youtube] knTc-NQSjKA: Downloading webpage\n",
      "[youtube] knTc-NQSjKA: Downloading android player API JSON\n",
      "[info] knTc-NQSjKA: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm\n",
      "[download] 100% of   39.62MiB in 00:00:02 at 16.26MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm (pass -k to keep)\n",
      "[download] Downloading video 21 of 23\n",
      "[youtube] iXjtJmUQBZk: Downloading webpage\n",
      "[youtube] iXjtJmUQBZk: Downloading android player API JSON\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[info] iXjtJmUQBZk: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm\n",
      "[download] 100% of   87.39MiB in 00:00:05 at 17.48MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].mp3\n",
      "Deleting original file Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm (pass -k to keep)\n",
      "[download] Downloading video 22 of 23\n",
      "[youtube] 4ynrGLIuPv4: Downloading webpage\n",
      "[youtube] 4ynrGLIuPv4: Downloading android player API JSON\n",
      "[info] 4ynrGLIuPv4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm\n",
      "[download] 100% of   57.17MiB in 00:00:03 at 17.52MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm (pass -k to keep)\n",
      "[download] Downloading video 23 of 23\n",
      "[youtube] UFem7xa3Q2Q: Downloading webpage\n",
      "[youtube] UFem7xa3Q2Q: Downloading android player API JSON\n",
      "[info] UFem7xa3Q2Q: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm\n",
      "[download] 100% of   56.61MiB in 00:00:03 at 16.32MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm (pass -k to keep)\n",
      "[download] Finished downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Downloading playlist PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ - add --no-playlist to download just the video gqaHkPEZAew\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Playlist Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021: Downloading 23 videos of 23\n",
      "[download] Downloading video 1 of 23\n",
      "[youtube] rmVRLeJRkl4: Downloading webpage\n",
      "[youtube] rmVRLeJRkl4: Downloading android player API JSON\n",
      "[info] rmVRLeJRkl4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm\n",
      "[download] 100% of   62.45MiB in 00:00:03 at 17.08MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm (pass -k to keep)\n",
      "[download] Downloading video 2 of 23\n",
      "[youtube] gqaHkPEZAew: Downloading webpage\n",
      "[youtube] gqaHkPEZAew: Downloading android player API JSON\n",
      "[info] gqaHkPEZAew: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm\n",
      "[download] 100% of   55.26MiB in 00:00:03 at 16.35MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm (pass -k to keep)\n",
      "[download] Downloading video 3 of 23\n",
      "[youtube] X0Jw4kgaFlg: Downloading webpage\n",
      "[youtube] X0Jw4kgaFlg: Downloading android player API JSON\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[info] X0Jw4kgaFlg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm\n",
      "[download] 100% of   78.70MiB in 00:00:04 at 16.98MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm (pass -k to keep)\n",
      "[download] Downloading video 4 of 23\n",
      "[youtube] PSGIodTN3KE: Downloading webpage\n",
      "[youtube] PSGIodTN3KE: Downloading android player API JSON\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[info] PSGIodTN3KE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm\n",
      "[download] 100% of   76.19MiB in 00:00:09 at 8.15MiB/s     \n",
      "[ExtractAudio] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].mp3\n",
      "Deleting original file Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm (pass -k to keep)\n",
      "[download] Downloading video 5 of 23\n",
      "[youtube] PLryWeHPcBs: Downloading webpage\n",
      "[youtube] PLryWeHPcBs: Downloading android player API JSON\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[info] PLryWeHPcBs: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm\n",
      "[download] 100% of   75.52MiB in 00:00:04 at 17.42MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].mp3\n",
      "Deleting original file Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm (pass -k to keep)\n",
      "[download] Downloading video 6 of 23\n",
      "[youtube] 0LixFSa7yts: Downloading webpage\n",
      "[youtube] 0LixFSa7yts: Downloading android player API JSON\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[info] 0LixFSa7yts: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm\n",
      "[download] 100% of   78.35MiB in 00:00:04 at 16.78MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm (pass -k to keep)\n",
      "[download] Downloading video 7 of 23\n",
      "[youtube] wzfWHP6SXxY: Downloading webpage\n",
      "[youtube] wzfWHP6SXxY: Downloading android player API JSON\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[info] wzfWHP6SXxY: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm\n",
      "[download] 100% of   75.59MiB in 00:00:03 at 18.94MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm (pass -k to keep)\n",
      "[download] Downloading video 8 of 23\n",
      "[youtube] gKD7jPAdbpE: Downloading webpage\n",
      "[youtube] gKD7jPAdbpE: Downloading android player API JSON\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[info] gKD7jPAdbpE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm\n",
      "[download] 100% of   75.69MiB in 00:00:04 at 17.59MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm (pass -k to keep)\n",
      "[download] Downloading video 9 of 23\n",
      "[youtube] ptuGllU5SQQ: Downloading webpage\n",
      "[youtube] ptuGllU5SQQ: Downloading android player API JSON\n",
      "[info] ptuGllU5SQQ: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm\n",
      "[download] 100% of   62.90MiB in 00:00:03 at 17.21MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm (pass -k to keep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Downloading video 10 of 23\n",
      "[youtube] j9AcEI98C0o: Downloading webpage\n",
      "[youtube] j9AcEI98C0o: Downloading android player API JSON\n",
      "[info] j9AcEI98C0o: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm\n",
      "[download] 100% of   63.70MiB in 00:00:03 at 17.46MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm (pass -k to keep)\n",
      "[download] Downloading video 11 of 23\n",
      "[youtube] NcqfHa0_YmU: Downloading webpage\n",
      "[youtube] NcqfHa0_YmU: Downloading android player API JSON\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[info] NcqfHa0_YmU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm\n",
      "[download] 100% of  105.24MiB in 00:00:05 at 17.73MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm (pass -k to keep)\n",
      "[download] Downloading video 12 of 23\n",
      "[youtube] 1uMo8olr5ng: Downloading webpage\n",
      "[youtube] 1uMo8olr5ng: Downloading android player API JSON\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[info] 1uMo8olr5ng: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm\n",
      "[download] 100% of   81.56MiB in 00:00:04 at 16.90MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm (pass -k to keep)\n",
      "[download] Downloading video 13 of 23\n",
      "[youtube] FFRnDRcbQQU: Downloading webpage\n",
      "[youtube] FFRnDRcbQQU: Downloading android player API JSON\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[info] FFRnDRcbQQU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm\n",
      "[download] 100% of   81.34MiB in 00:00:04 at 16.43MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm (pass -k to keep)\n",
      "[download] Downloading video 14 of 23\n",
      "[youtube] iHWkLvoSpTg: Downloading webpage\n",
      "[youtube] iHWkLvoSpTg: Downloading android player API JSON\n",
      "[info] iHWkLvoSpTg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm\n",
      "[download] 100% of   94.67MiB in 00:00:05 at 16.66MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm (pass -k to keep)\n",
      "[download] Downloading video 15 of 23\n",
      "[youtube] y68RJVfGoto: Downloading webpage\n",
      "[youtube] y68RJVfGoto: Downloading android player API JSON\n",
      "[info] y68RJVfGoto: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm\n",
      "[download] 100% of   74.64MiB in 00:00:04 at 16.23MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm (pass -k to keep)\n",
      "[download] Downloading video 16 of 23\n",
      "[youtube] -Ldg4qFL6bU: Downloading webpage\n",
      "[youtube] -Ldg4qFL6bU: Downloading android player API JSON\n",
      "[info] -Ldg4qFL6bU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm\n",
      "[download] 100% of  102.55MiB in 00:00:05 at 17.65MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm (pass -k to keep)\n",
      "[download] Downloading video 17 of 23\n",
      "[youtube] f_qmSSBWV_E: Downloading webpage\n",
      "[youtube] f_qmSSBWV_E: Downloading android player API JSON\n",
      "[info] f_qmSSBWV_E: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm\n",
      "[download] 100% of   57.83MiB in 00:00:03 at 17.45MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm (pass -k to keep)\n",
      "[download] Downloading video 18 of 23\n",
      "[youtube] 2t7Q9WVUaf8: Downloading webpage\n",
      "[youtube] 2t7Q9WVUaf8: Downloading android player API JSON\n",
      "[info] 2t7Q9WVUaf8: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm\n",
      "[download] 100% of   73.18MiB in 00:00:03 at 18.74MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm (pass -k to keep)\n",
      "[download] Downloading video 19 of 23\n",
      "[youtube] mp95Z5yM92c: Downloading webpage\n",
      "[youtube] mp95Z5yM92c: Downloading android player API JSON\n",
      "[info] mp95Z5yM92c: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm\n",
      "[download] 100% of   56.56MiB in 00:00:03 at 17.96MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm (pass -k to keep)\n",
      "[download] Downloading video 20 of 23\n",
      "[youtube] knTc-NQSjKA: Downloading webpage\n",
      "[youtube] knTc-NQSjKA: Downloading android player API JSON\n",
      "[info] knTc-NQSjKA: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm\n",
      "[download] 100% of   39.62MiB in 00:00:02 at 17.32MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm (pass -k to keep)\n",
      "[download] Downloading video 21 of 23\n",
      "[youtube] iXjtJmUQBZk: Downloading webpage\n",
      "[youtube] iXjtJmUQBZk: Downloading android player API JSON\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[info] iXjtJmUQBZk: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] 100% of   87.39MiB in 00:00:05 at 17.30MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].mp3\n",
      "Deleting original file Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm (pass -k to keep)\n",
      "[download] Downloading video 22 of 23\n",
      "[youtube] 4ynrGLIuPv4: Downloading webpage\n",
      "[youtube] 4ynrGLIuPv4: Downloading android player API JSON\n",
      "[info] 4ynrGLIuPv4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm\n",
      "[download] 100% of   57.17MiB in 00:00:03 at 17.44MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm (pass -k to keep)\n",
      "[download] Downloading video 23 of 23\n",
      "[youtube] UFem7xa3Q2Q: Downloading webpage\n",
      "[youtube] UFem7xa3Q2Q: Downloading android player API JSON\n",
      "[info] UFem7xa3Q2Q: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm\n",
      "[download] 100% of   56.61MiB in 00:00:03 at 16.99MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm (pass -k to keep)\n",
      "[download] Finished downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Downloading playlist PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ - add --no-playlist to download just the video X0Jw4kgaFlg\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Playlist Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021: Downloading 23 videos of 23\n",
      "[download] Downloading video 1 of 23\n",
      "[youtube] rmVRLeJRkl4: Downloading webpage\n",
      "[youtube] rmVRLeJRkl4: Downloading android player API JSON\n",
      "[info] rmVRLeJRkl4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm\n",
      "[download] 100% of   62.45MiB in 00:00:03 at 17.55MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm (pass -k to keep)\n",
      "[download] Downloading video 2 of 23\n",
      "[youtube] gqaHkPEZAew: Downloading webpage\n",
      "[youtube] gqaHkPEZAew: Downloading android player API JSON\n",
      "[info] gqaHkPEZAew: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm\n",
      "[download] 100% of   55.26MiB in 00:00:04 at 13.51MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm (pass -k to keep)\n",
      "[download] Downloading video 3 of 23\n",
      "[youtube] X0Jw4kgaFlg: Downloading webpage\n",
      "[youtube] X0Jw4kgaFlg: Downloading android player API JSON\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[info] X0Jw4kgaFlg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm\n",
      "[download] 100% of   78.70MiB in 00:00:04 at 17.28MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm (pass -k to keep)\n",
      "[download] Downloading video 4 of 23\n",
      "[youtube] PSGIodTN3KE: Downloading webpage\n",
      "[youtube] PSGIodTN3KE: Downloading android player API JSON\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[info] PSGIodTN3KE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm\n",
      "[download] 100% of   76.19MiB in 00:00:04 at 17.49MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].mp3\n",
      "Deleting original file Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm (pass -k to keep)\n",
      "[download] Downloading video 5 of 23\n",
      "[youtube] PLryWeHPcBs: Downloading webpage\n",
      "[youtube] PLryWeHPcBs: Downloading android player API JSON\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[info] PLryWeHPcBs: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm\n",
      "[download] 100% of   75.52MiB in 00:00:04 at 17.56MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].mp3\n",
      "Deleting original file Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm (pass -k to keep)\n",
      "[download] Downloading video 6 of 23\n",
      "[youtube] 0LixFSa7yts: Downloading webpage\n",
      "[youtube] 0LixFSa7yts: Downloading android player API JSON\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[info] 0LixFSa7yts: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm\n",
      "[download] 100% of   78.35MiB in 00:00:04 at 18.05MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm (pass -k to keep)\n",
      "[download] Downloading video 7 of 23\n",
      "[youtube] wzfWHP6SXxY: Downloading webpage\n",
      "[youtube] wzfWHP6SXxY: Downloading android player API JSON\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[info] wzfWHP6SXxY: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm\n",
      "[download] 100% of   75.59MiB in 00:00:04 at 17.98MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm (pass -k to keep)\n",
      "[download] Downloading video 8 of 23\n",
      "[youtube] gKD7jPAdbpE: Downloading webpage\n",
      "[youtube] gKD7jPAdbpE: Downloading android player API JSON\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[info] gKD7jPAdbpE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm\n",
      "[download] 100% of   75.69MiB in 00:00:04 at 16.79MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm (pass -k to keep)\n",
      "[download] Downloading video 9 of 23\n",
      "[youtube] ptuGllU5SQQ: Downloading webpage\n",
      "[youtube] ptuGllU5SQQ: Downloading android player API JSON\n",
      "[info] ptuGllU5SQQ: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm\n",
      "[download] 100% of   62.90MiB in 00:00:03 at 15.79MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm (pass -k to keep)\n",
      "[download] Downloading video 10 of 23\n",
      "[youtube] j9AcEI98C0o: Downloading webpage\n",
      "[youtube] j9AcEI98C0o: Downloading android player API JSON\n",
      "[info] j9AcEI98C0o: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm\n",
      "[download] 100% of   63.70MiB in 00:00:03 at 17.33MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm (pass -k to keep)\n",
      "[download] Downloading video 11 of 23\n",
      "[youtube] NcqfHa0_YmU: Downloading webpage\n",
      "[youtube] NcqfHa0_YmU: Downloading android player API JSON\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[info] NcqfHa0_YmU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm\n",
      "[download] 100% of  105.24MiB in 00:00:05 at 17.73MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm (pass -k to keep)\n",
      "[download] Downloading video 12 of 23\n",
      "[youtube] 1uMo8olr5ng: Downloading webpage\n",
      "[youtube] 1uMo8olr5ng: Downloading android player API JSON\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[info] 1uMo8olr5ng: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm\n",
      "[download] 100% of   81.56MiB in 00:00:04 at 17.33MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm (pass -k to keep)\n",
      "[download] Downloading video 13 of 23\n",
      "[youtube] FFRnDRcbQQU: Downloading webpage\n",
      "[youtube] FFRnDRcbQQU: Downloading android player API JSON\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[info] FFRnDRcbQQU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm\n",
      "[download] 100% of   81.34MiB in 00:00:04 at 16.97MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm (pass -k to keep)\n",
      "[download] Downloading video 14 of 23\n",
      "[youtube] iHWkLvoSpTg: Downloading webpage\n",
      "[youtube] iHWkLvoSpTg: Downloading android player API JSON\n",
      "[info] iHWkLvoSpTg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm\n",
      "[download] 100% of   94.67MiB in 00:00:05 at 16.03MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm (pass -k to keep)\n",
      "[download] Downloading video 15 of 23\n",
      "[youtube] y68RJVfGoto: Downloading webpage\n",
      "[youtube] y68RJVfGoto: Downloading android player API JSON\n",
      "[info] y68RJVfGoto: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm\n",
      "[download] 100% of   74.64MiB in 00:00:04 at 17.77MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm (pass -k to keep)\n",
      "[download] Downloading video 16 of 23\n",
      "[youtube] -Ldg4qFL6bU: Downloading webpage\n",
      "[youtube] -Ldg4qFL6bU: Downloading android player API JSON\n",
      "[info] -Ldg4qFL6bU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm\n",
      "[download] 100% of  102.55MiB in 00:00:06 at 16.73MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm (pass -k to keep)\n",
      "[download] Downloading video 17 of 23\n",
      "[youtube] f_qmSSBWV_E: Downloading webpage\n",
      "[youtube] f_qmSSBWV_E: Downloading android player API JSON\n",
      "[info] f_qmSSBWV_E: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm\n",
      "[download] 100% of   57.83MiB in 00:00:03 at 17.08MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm (pass -k to keep)\n",
      "[download] Downloading video 18 of 23\n",
      "[youtube] 2t7Q9WVUaf8: Downloading webpage\n",
      "[youtube] 2t7Q9WVUaf8: Downloading android player API JSON\n",
      "[info] 2t7Q9WVUaf8: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm\n",
      "[download] 100% of   73.18MiB in 00:00:04 at 17.17MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm (pass -k to keep)\n",
      "[download] Downloading video 19 of 23\n",
      "[youtube] mp95Z5yM92c: Downloading webpage\n",
      "[youtube] mp95Z5yM92c: Downloading android player API JSON\n",
      "[info] mp95Z5yM92c: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm\n",
      "[download] 100% of   56.56MiB in 00:00:03 at 15.44MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm (pass -k to keep)\n",
      "[download] Downloading video 20 of 23\n",
      "[youtube] knTc-NQSjKA: Downloading webpage\n",
      "[youtube] knTc-NQSjKA: Downloading android player API JSON\n",
      "[info] knTc-NQSjKA: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] 100% of   39.62MiB in 00:00:02 at 16.24MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm (pass -k to keep)\n",
      "[download] Downloading video 21 of 23\n",
      "[youtube] iXjtJmUQBZk: Downloading webpage\n",
      "[youtube] iXjtJmUQBZk: Downloading android player API JSON\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[info] iXjtJmUQBZk: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm\n",
      "[download] 100% of   87.39MiB in 00:00:05 at 16.89MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].mp3\n",
      "Deleting original file Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm (pass -k to keep)\n",
      "[download] Downloading video 22 of 23\n",
      "[youtube] 4ynrGLIuPv4: Downloading webpage\n",
      "[youtube] 4ynrGLIuPv4: Downloading android player API JSON\n",
      "[info] 4ynrGLIuPv4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm\n",
      "[download] 100% of   57.17MiB in 00:00:04 at 13.72MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm (pass -k to keep)\n",
      "[download] Downloading video 23 of 23\n",
      "[youtube] UFem7xa3Q2Q: Downloading webpage\n",
      "[youtube] UFem7xa3Q2Q: Downloading android player API JSON\n",
      "[info] UFem7xa3Q2Q: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm\n",
      "[download] 100% of   56.61MiB in 00:00:03 at 17.23MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm (pass -k to keep)\n",
      "[download] Finished downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Downloading playlist PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ - add --no-playlist to download just the video PSGIodTN3KE\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Playlist Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021: Downloading 23 videos of 23\n",
      "[download] Downloading video 1 of 23\n",
      "[youtube] rmVRLeJRkl4: Downloading webpage\n",
      "[youtube] rmVRLeJRkl4: Downloading android player API JSON\n",
      "[info] rmVRLeJRkl4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm\n",
      "[download] 100% of   62.45MiB in 00:00:03 at 15.87MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm (pass -k to keep)\n",
      "[download] Downloading video 2 of 23\n",
      "[youtube] gqaHkPEZAew: Downloading webpage\n",
      "[youtube] gqaHkPEZAew: Downloading android player API JSON\n",
      "[info] gqaHkPEZAew: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm\n",
      "[download] 100% of   55.26MiB in 00:00:03 at 15.19MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm (pass -k to keep)\n",
      "[download] Downloading video 3 of 23\n",
      "[youtube] X0Jw4kgaFlg: Downloading webpage\n",
      "[youtube] X0Jw4kgaFlg: Downloading android player API JSON\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[info] X0Jw4kgaFlg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm\n",
      "[download] 100% of   78.70MiB in 00:00:04 at 16.56MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm (pass -k to keep)\n",
      "[download] Downloading video 4 of 23\n",
      "[youtube] PSGIodTN3KE: Downloading webpage\n",
      "[youtube] PSGIodTN3KE: Downloading android player API JSON\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[info] PSGIodTN3KE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm\n",
      "[download] 100% of   76.19MiB in 00:00:04 at 17.61MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].mp3\n",
      "Deleting original file Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm (pass -k to keep)\n",
      "[download] Downloading video 5 of 23\n",
      "[youtube] PLryWeHPcBs: Downloading webpage\n",
      "[youtube] PLryWeHPcBs: Downloading android player API JSON\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[info] PLryWeHPcBs: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm\n",
      "[download] 100% of   75.52MiB in 00:00:04 at 17.25MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].mp3\n",
      "Deleting original file Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm (pass -k to keep)\n",
      "[download] Downloading video 6 of 23\n",
      "[youtube] 0LixFSa7yts: Downloading webpage\n",
      "[youtube] 0LixFSa7yts: Downloading android player API JSON\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[info] 0LixFSa7yts: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm\n",
      "[download] 100% of   78.35MiB in 00:00:04 at 17.37MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm (pass -k to keep)\n",
      "[download] Downloading video 7 of 23\n",
      "[youtube] wzfWHP6SXxY: Downloading webpage\n",
      "[youtube] wzfWHP6SXxY: Downloading android player API JSON\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[info] wzfWHP6SXxY: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm\n",
      "[download] 100% of   75.59MiB in 00:00:04 at 17.49MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm (pass -k to keep)\n",
      "[download] Downloading video 8 of 23\n",
      "[youtube] gKD7jPAdbpE: Downloading webpage\n",
      "[youtube] gKD7jPAdbpE: Downloading android player API JSON\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[info] gKD7jPAdbpE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm\n",
      "[download] 100% of   75.69MiB in 00:00:04 at 17.72MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm (pass -k to keep)\n",
      "[download] Downloading video 9 of 23\n",
      "[youtube] ptuGllU5SQQ: Downloading webpage\n",
      "[youtube] ptuGllU5SQQ: Downloading android player API JSON\n",
      "[info] ptuGllU5SQQ: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm\n",
      "[download] 100% of   62.90MiB in 00:00:03 at 17.12MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm (pass -k to keep)\n",
      "[download] Downloading video 10 of 23\n",
      "[youtube] j9AcEI98C0o: Downloading webpage\n",
      "[youtube] j9AcEI98C0o: Downloading android player API JSON\n",
      "[info] j9AcEI98C0o: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm\n",
      "[download] 100% of   63.70MiB in 00:00:03 at 17.49MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm (pass -k to keep)\n",
      "[download] Downloading video 11 of 23\n",
      "[youtube] NcqfHa0_YmU: Downloading webpage\n",
      "[youtube] NcqfHa0_YmU: Downloading android player API JSON\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[info] NcqfHa0_YmU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm\n",
      "[download] 100% of  105.24MiB in 00:00:05 at 17.58MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm (pass -k to keep)\n",
      "[download] Downloading video 12 of 23\n",
      "[youtube] 1uMo8olr5ng: Downloading webpage\n",
      "[youtube] 1uMo8olr5ng: Downloading android player API JSON\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[info] 1uMo8olr5ng: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm\n",
      "[download] 100% of   81.56MiB in 00:00:04 at 17.58MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm (pass -k to keep)\n",
      "[download] Downloading video 13 of 23\n",
      "[youtube] FFRnDRcbQQU: Downloading webpage\n",
      "[youtube] FFRnDRcbQQU: Downloading android player API JSON\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[youtube] FFRnDRcbQQU: Downloading MPD manifest\n",
      "[info] FFRnDRcbQQU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm\n",
      "[download] 100% of   81.34MiB in 00:00:04 at 16.56MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 13 - Coreference Resolution [FFRnDRcbQQU].webm (pass -k to keep)\n",
      "[download] Downloading video 14 of 23\n",
      "[youtube] iHWkLvoSpTg: Downloading webpage\n",
      "[youtube] iHWkLvoSpTg: Downloading android player API JSON\n",
      "[info] iHWkLvoSpTg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm\n",
      "[download] 100% of   94.67MiB in 00:00:05 at 16.83MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 14 - T5 and Large Language Models [iHWkLvoSpTg].webm (pass -k to keep)\n",
      "[download] Downloading video 15 of 23\n",
      "[youtube] y68RJVfGoto: Downloading webpage\n",
      "[youtube] y68RJVfGoto: Downloading android player API JSON\n",
      "[info] y68RJVfGoto: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm\n",
      "[download] 100% of   74.64MiB in 00:00:04 at 17.08MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 15 - Add Knowledge to Language Models [y68RJVfGoto].webm (pass -k to keep)\n",
      "[download] Downloading video 16 of 23\n",
      "[youtube] -Ldg4qFL6bU: Downloading webpage\n",
      "[youtube] -Ldg4qFL6bU: Downloading android player API JSON\n",
      "[info] -Ldg4qFL6bU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm\n",
      "[download] 100% of  102.55MiB in 00:00:06 at 16.84MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social & Ethical Considerations [-Ldg4qFL6bU].webm (pass -k to keep)\n",
      "[download] Downloading video 17 of 23\n",
      "[youtube] f_qmSSBWV_E: Downloading webpage\n",
      "[youtube] f_qmSSBWV_E: Downloading android player API JSON\n",
      "[info] f_qmSSBWV_E: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm\n",
      "[download] 100% of   57.83MiB in 00:00:03 at 18.36MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 17 - Model Analysis and Explanation [f_qmSSBWV_E].webm (pass -k to keep)\n",
      "[download] Downloading video 18 of 23\n",
      "[youtube] 2t7Q9WVUaf8: Downloading webpage\n",
      "[youtube] 2t7Q9WVUaf8: Downloading android player API JSON\n",
      "[info] 2t7Q9WVUaf8: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm\n",
      "[download] 100% of   73.18MiB in 00:00:04 at 17.26MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 18 - Future of NLP + Deep Learning [2t7Q9WVUaf8].webm (pass -k to keep)\n",
      "[download] Downloading video 19 of 23\n",
      "[youtube] mp95Z5yM92c: Downloading webpage\n",
      "[youtube] mp95Z5yM92c: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] mp95Z5yM92c: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm\n",
      "[download] 100% of   56.56MiB in 00:00:03 at 16.28MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation [mp95Z5yM92c].webm (pass -k to keep)\n",
      "[download] Downloading video 20 of 23\n",
      "[youtube] knTc-NQSjKA: Downloading webpage\n",
      "[youtube] knTc-NQSjKA: Downloading android player API JSON\n",
      "[info] knTc-NQSjKA: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm\n",
      "[download] 100% of   39.62MiB in 00:00:02 at 16.13MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models [knTc-NQSjKA].webm (pass -k to keep)\n",
      "[download] Downloading video 21 of 23\n",
      "[youtube] iXjtJmUQBZk: Downloading webpage\n",
      "[youtube] iXjtJmUQBZk: Downloading android player API JSON\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[youtube] iXjtJmUQBZk: Downloading MPD manifest\n",
      "[info] iXjtJmUQBZk: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm\n",
      "[download] 100% of   87.39MiB in 00:00:05 at 17.21MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].mp3\n",
      "Deleting original file Stanford CS224N I NLP with Deep Learning ｜ Spring 2022 ｜ Socially Intelligent NLP Systems [iXjtJmUQBZk].webm (pass -k to keep)\n",
      "[download] Downloading video 22 of 23\n",
      "[youtube] 4ynrGLIuPv4: Downloading webpage\n",
      "[youtube] 4ynrGLIuPv4: Downloading android player API JSON\n",
      "[info] 4ynrGLIuPv4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm\n",
      "[download] 100% of   57.17MiB in 00:00:03 at 17.62MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜Spring 2022｜Guest Lecture： Building Knowledge Representation [4ynrGLIuPv4].webm (pass -k to keep)\n",
      "[download] Downloading video 23 of 23\n",
      "[youtube] UFem7xa3Q2Q: Downloading webpage\n",
      "[youtube] UFem7xa3Q2Q: Downloading android player API JSON\n",
      "[info] UFem7xa3Q2Q: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm\n",
      "[download] 100% of   56.61MiB in 00:00:03 at 17.04MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Spring 2022 ｜ Guest Lecture：  Scaling Language Models [UFem7xa3Q2Q].webm (pass -k to keep)\n",
      "[download] Finished downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Downloading playlist PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ - add --no-playlist to download just the video PLryWeHPcBs\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021\n",
      "[youtube:tab] Playlist Stanford CS224N: Natural Language Processing with Deep Learning | Winter 2021: Downloading 23 videos of 23\n",
      "[download] Downloading video 1 of 23\n",
      "[youtube] rmVRLeJRkl4: Downloading webpage\n",
      "[youtube] rmVRLeJRkl4: Downloading android player API JSON\n",
      "[info] rmVRLeJRkl4: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm\n",
      "[download] 100% of   62.45MiB in 00:00:03 at 17.21MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].mp3\n",
      "Deleting original file Stanford CS224N： NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 1 - Intro & Word Vectors [rmVRLeJRkl4].webm (pass -k to keep)\n",
      "[download] Downloading video 2 of 23\n",
      "[youtube] gqaHkPEZAew: Downloading webpage\n",
      "[youtube] gqaHkPEZAew: Downloading android player API JSON\n",
      "[info] gqaHkPEZAew: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm\n",
      "[download] 100% of   55.26MiB in 00:00:03 at 16.94MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 2 - Neural Classifiers [gqaHkPEZAew].webm (pass -k to keep)\n",
      "[download] Downloading video 3 of 23\n",
      "[youtube] X0Jw4kgaFlg: Downloading webpage\n",
      "[youtube] X0Jw4kgaFlg: Downloading android player API JSON\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[youtube] X0Jw4kgaFlg: Downloading MPD manifest\n",
      "[info] X0Jw4kgaFlg: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm\n",
      "[download] 100% of   78.70MiB in 00:00:05 at 14.43MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 3 - Backprop and Neural Networks [X0Jw4kgaFlg].webm (pass -k to keep)\n",
      "[download] Downloading video 4 of 23\n",
      "[youtube] PSGIodTN3KE: Downloading webpage\n",
      "[youtube] PSGIodTN3KE: Downloading android player API JSON\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[youtube] PSGIodTN3KE: Downloading MPD manifest\n",
      "[info] PSGIodTN3KE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm\n",
      "[download] 100% of   76.19MiB in 00:00:04 at 17.20MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].mp3\n",
      "Deleting original file Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].webm (pass -k to keep)\n",
      "[download] Downloading video 5 of 23\n",
      "[youtube] PLryWeHPcBs: Downloading webpage\n",
      "[youtube] PLryWeHPcBs: Downloading android player API JSON\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[youtube] PLryWeHPcBs: Downloading MPD manifest\n",
      "[info] PLryWeHPcBs: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm\n",
      "[download] 100% of   75.52MiB in 00:00:04 at 16.25MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].mp3\n",
      "Deleting original file Stanford CS224N -  NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 5 - Recurrent Neural networks (RNNs) [PLryWeHPcBs].webm (pass -k to keep)\n",
      "[download] Downloading video 6 of 23\n",
      "[youtube] 0LixFSa7yts: Downloading webpage\n",
      "[youtube] 0LixFSa7yts: Downloading android player API JSON\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[youtube] 0LixFSa7yts: Downloading MPD manifest\n",
      "[info] 0LixFSa7yts: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm\n",
      "[download] 100% of   78.35MiB in 00:00:04 at 16.82MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 6 - Simple and LSTM RNNs [0LixFSa7yts].webm (pass -k to keep)\n",
      "[download] Downloading video 7 of 23\n",
      "[youtube] wzfWHP6SXxY: Downloading webpage\n",
      "[youtube] wzfWHP6SXxY: Downloading android player API JSON\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[youtube] wzfWHP6SXxY: Downloading MPD manifest\n",
      "[info] wzfWHP6SXxY: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm\n",
      "[download] 100% of   75.59MiB in 00:00:04 at 17.40MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 7 - Translation, Seq2Seq, Attention [wzfWHP6SXxY].webm (pass -k to keep)\n",
      "[download] Downloading video 8 of 23\n",
      "[youtube] gKD7jPAdbpE: Downloading webpage\n",
      "[youtube] gKD7jPAdbpE: Downloading android player API JSON\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[youtube] gKD7jPAdbpE: Downloading MPD manifest\n",
      "[info] gKD7jPAdbpE: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm\n",
      "[download] 100% of   75.69MiB in 00:00:04 at 17.20MiB/s  \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 8 - Final Projects; Practical Tips [gKD7jPAdbpE].webm (pass -k to keep)\n",
      "[download] Downloading video 9 of 23\n",
      "[youtube] ptuGllU5SQQ: Downloading webpage\n",
      "[youtube] ptuGllU5SQQ: Downloading android player API JSON\n",
      "[info] ptuGllU5SQQ: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm\n",
      "[download] 100% of   62.90MiB in 00:00:04 at 15.09MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 9 - Self- Attention and Transformers [ptuGllU5SQQ].webm (pass -k to keep)\n",
      "[download] Downloading video 10 of 23\n",
      "[youtube] j9AcEI98C0o: Downloading webpage\n",
      "[youtube] j9AcEI98C0o: Downloading android player API JSON\n",
      "[info] j9AcEI98C0o: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm\n",
      "[download] 100% of   63.70MiB in 00:00:03 at 16.93MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 10 - Transformers and Pretraining [j9AcEI98C0o].webm (pass -k to keep)\n",
      "[download] Downloading video 11 of 23\n",
      "[youtube] NcqfHa0_YmU: Downloading webpage\n",
      "[youtube] NcqfHa0_YmU: Downloading android player API JSON\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[youtube] NcqfHa0_YmU: Downloading MPD manifest\n",
      "[info] NcqfHa0_YmU: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm\n",
      "[download] 100% of  105.24MiB in 00:00:06 at 16.42MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].mp3\n",
      "Deleting original file Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 11 - Question Answering [NcqfHa0_YmU].webm (pass -k to keep)\n",
      "[download] Downloading video 12 of 23\n",
      "[youtube] 1uMo8olr5ng: Downloading webpage\n",
      "[youtube] 1uMo8olr5ng: Downloading android player API JSON\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[youtube] 1uMo8olr5ng: Downloading MPD manifest\n",
      "[info] 1uMo8olr5ng: Downloading 1 format(s): 251\n",
      "[download] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].webm\n",
      "[download] 100% of   81.56MiB in 00:00:05 at 14.99MiB/s    \n",
      "[ExtractAudio] Destination: Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 12 - Natural Language Generation [1uMo8olr5ng].mp3\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "# URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
    "    'postprocessors': [{  # Extract audio using ffmpeg\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "    }]\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    error_code = ydl.download(URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6bd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07021c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7c6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'seek': 0, 'start': 0.0, 'end': 12.48, 'text': \" Okay, so for today, we're actually going to take a bit of a change of pace from what\", 'tokens': [1033, 11, 370, 337, 965, 11, 321, 434, 767, 516, 281, 747, 257, 857, 295, 257, 1319, 295, 11638, 490, 437], 'temperature': 0.0, 'avg_logprob': -0.21641497825508688, 'compression_ratio': 1.606936416184971, 'no_speech_prob': 0.09206797182559967}, {'id': 1, 'seek': 0, 'start': 12.48, 'end': 19.28, 'text': \" the last couple of lectures are have been about and we're going to focus much more on linguistics\", 'tokens': [264, 1036, 1916, 295, 16564, 366, 362, 668, 466, 293, 321, 434, 516, 281, 1879, 709, 544, 322, 21766, 6006], 'temperature': 0.0, 'avg_logprob': -0.21641497825508688, 'compression_ratio': 1.606936416184971, 'no_speech_prob': 0.09206797182559967}, {'id': 2, 'seek': 0, 'start': 19.28, 'end': 25.92, 'text': \" and natural language processing and so in particular, we're going to start looking at the topic\", 'tokens': [293, 3303, 2856, 9007, 293, 370, 294, 1729, 11, 321, 434, 516, 281, 722, 1237, 412, 264, 4829], 'temperature': 0.0, 'avg_logprob': -0.21641497825508688, 'compression_ratio': 1.606936416184971, 'no_speech_prob': 0.09206797182559967}, {'id': 3, 'seek': 2592, 'start': 25.92, 'end': 35.44, 'text': \" of dependency parsing. And so this is the plan of what to go about through today. So I'm going\", 'tokens': [295, 33621, 21156, 278, 13, 400, 370, 341, 307, 264, 1393, 295, 437, 281, 352, 466, 807, 965, 13, 407, 286, 478, 516], 'temperature': 0.0, 'avg_logprob': -0.19454951402617665, 'compression_ratio': 1.758139534883721, 'no_speech_prob': 0.0001604437711648643}, {'id': 4, 'seek': 2592, 'start': 35.44, 'end': 40.96, 'text': ' to start out by going through some ideas that have been used in the syntactic structure of languages', 'tokens': [281, 722, 484, 538, 516, 807, 512, 3487, 300, 362, 668, 1143, 294, 264, 23980, 19892, 3877, 295, 8650], 'temperature': 0.0, 'avg_logprob': -0.19454951402617665, 'compression_ratio': 1.758139534883721, 'no_speech_prob': 0.0001604437711648643}, {'id': 5, 'seek': 2592, 'start': 40.96, 'end': 48.0, 'text': ' of constituency and dependency and introduce those and then focusing in more independent', 'tokens': [295, 46146, 293, 33621, 293, 5366, 729, 293, 550, 8416, 294, 544, 6695], 'temperature': 0.0, 'avg_logprob': -0.19454951402617665, 'compression_ratio': 1.758139534883721, 'no_speech_prob': 0.0001604437711648643}, {'id': 6, 'seek': 2592, 'start': 48.0, 'end': 55.44, 'text': \" dependency structure, I'm then going to look at dependency grammars and dependency tree banks\", 'tokens': [33621, 3877, 11, 286, 478, 550, 516, 281, 574, 412, 33621, 17570, 685, 293, 33621, 4230, 10237], 'temperature': 0.0, 'avg_logprob': -0.19454951402617665, 'compression_ratio': 1.758139534883721, 'no_speech_prob': 0.0001604437711648643}, {'id': 7, 'seek': 5544, 'start': 55.44, 'end': 60.08, 'text': \" and then having done that, we're then going to move back into thinking about how to build\", 'tokens': [293, 550, 1419, 1096, 300, 11, 321, 434, 550, 516, 281, 1286, 646, 666, 1953, 466, 577, 281, 1322], 'temperature': 0.0, 'avg_logprob': -0.08319582939147949, 'compression_ratio': 1.904564315352697, 'no_speech_prob': 5.0986025598831475e-05}, {'id': 8, 'seek': 5544, 'start': 60.08, 'end': 65.52, 'text': \" natural language processing systems and so I'm going to introduce the idea of transition-based\", 'tokens': [3303, 2856, 9007, 3652, 293, 370, 286, 478, 516, 281, 5366, 264, 1558, 295, 6034, 12, 6032], 'temperature': 0.0, 'avg_logprob': -0.08319582939147949, 'compression_ratio': 1.904564315352697, 'no_speech_prob': 5.0986025598831475e-05}, {'id': 9, 'seek': 5544, 'start': 65.52, 'end': 72.24, 'text': \" dependency parsing and then in particular having developed that idea, I'm going to talk about\", 'tokens': [33621, 21156, 278, 293, 550, 294, 1729, 1419, 4743, 300, 1558, 11, 286, 478, 516, 281, 751, 466], 'temperature': 0.0, 'avg_logprob': -0.08319582939147949, 'compression_ratio': 1.904564315352697, 'no_speech_prob': 5.0986025598831475e-05}, {'id': 10, 'seek': 5544, 'start': 72.24, 'end': 78.4, 'text': ' a way to build a simple but highly effective neural dependency parser. And so this simple', 'tokens': [257, 636, 281, 1322, 257, 2199, 457, 5405, 4942, 18161, 33621, 21156, 260, 13, 400, 370, 341, 2199], 'temperature': 0.0, 'avg_logprob': -0.08319582939147949, 'compression_ratio': 1.904564315352697, 'no_speech_prob': 5.0986025598831475e-05}, {'id': 11, 'seek': 5544, 'start': 78.4, 'end': 83.44, 'text': \" highly effective neural dependency parser is essentially what we'll be asking you to build\", 'tokens': [5405, 4942, 18161, 33621, 21156, 260, 307, 4476, 437, 321, 603, 312, 3365, 291, 281, 1322], 'temperature': 0.0, 'avg_logprob': -0.08319582939147949, 'compression_ratio': 1.904564315352697, 'no_speech_prob': 5.0986025598831475e-05}, {'id': 12, 'seek': 8344, 'start': 83.44, 'end': 89.12, 'text': \" in the third assignment. So in some sense, we're getting a little bit ahead of ourselves here\", 'tokens': [294, 264, 2636, 15187, 13, 407, 294, 512, 2020, 11, 321, 434, 1242, 257, 707, 857, 2286, 295, 4175, 510], 'temperature': 0.0, 'avg_logprob': -0.15561692913373312, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.00028912126435898244}, {'id': 13, 'seek': 8344, 'start': 89.12, 'end': 96.0, 'text': ' because in week two of the class, we teach you how to do both assignments two and three, but all', 'tokens': [570, 294, 1243, 732, 295, 264, 1508, 11, 321, 2924, 291, 577, 281, 360, 1293, 22546, 732, 293, 1045, 11, 457, 439], 'temperature': 0.0, 'avg_logprob': -0.15561692913373312, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.00028912126435898244}, {'id': 14, 'seek': 8344, 'start': 96.0, 'end': 102.64, 'text': ' of this material will come in really useful. Before I get underway, just a couple of announcements.', 'tokens': [295, 341, 2527, 486, 808, 294, 534, 4420, 13, 4546, 286, 483, 27534, 11, 445, 257, 1916, 295, 23785, 13], 'temperature': 0.0, 'avg_logprob': -0.15561692913373312, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.00028912126435898244}, {'id': 15, 'seek': 8344, 'start': 104.56, 'end': 111.84, 'text': \" So for a site, again for assignment two, you don't yet need to use the PyTorch framework,\", 'tokens': [407, 337, 257, 3621, 11, 797, 337, 15187, 732, 11, 291, 500, 380, 1939, 643, 281, 764, 264, 9953, 51, 284, 339, 8388, 11], 'temperature': 0.0, 'avg_logprob': -0.15561692913373312, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.00028912126435898244}, {'id': 16, 'seek': 11184, 'start': 111.84, 'end': 118.96000000000001, 'text': \" but now's a good time to work on getting PyTorch installed for your Python programming.\", 'tokens': [457, 586, 311, 257, 665, 565, 281, 589, 322, 1242, 9953, 51, 284, 339, 8899, 337, 428, 15329, 9410, 13], 'temperature': 0.0, 'avg_logprob': -0.13283756256103516, 'compression_ratio': 1.5909090909090908, 'no_speech_prob': 0.0001346906356047839}, {'id': 17, 'seek': 11184, 'start': 119.76, 'end': 125.60000000000001, 'text': \" Assignment three is in part also and in production using PyTorch, it's got a lot of scaffolding\", 'tokens': [6281, 41134, 1045, 307, 294, 644, 611, 293, 294, 4265, 1228, 9953, 51, 284, 339, 11, 309, 311, 658, 257, 688, 295, 44094, 278], 'temperature': 0.0, 'avg_logprob': -0.13283756256103516, 'compression_ratio': 1.5909090909090908, 'no_speech_prob': 0.0001346906356047839}, {'id': 18, 'seek': 11184, 'start': 125.60000000000001, 'end': 133.44, 'text': \" included in the assignment, but beyond that, this Friday, we've got a PyTorch tutorial and thoroughly\", 'tokens': [5556, 294, 264, 15187, 11, 457, 4399, 300, 11, 341, 6984, 11, 321, 600, 658, 257, 9953, 51, 284, 339, 7073, 293, 17987], 'temperature': 0.0, 'avg_logprob': -0.13283756256103516, 'compression_ratio': 1.5909090909090908, 'no_speech_prob': 0.0001346906356047839}, {'id': 19, 'seek': 11184, 'start': 133.44, 'end': 141.52, 'text': ' encourage you to come along to that as well, look for it under the Zoom tab. And in the second half', 'tokens': [5373, 291, 281, 808, 2051, 281, 300, 382, 731, 11, 574, 337, 309, 833, 264, 13453, 4421, 13, 400, 294, 264, 1150, 1922], 'temperature': 0.0, 'avg_logprob': -0.13283756256103516, 'compression_ratio': 1.5909090909090908, 'no_speech_prob': 0.0001346906356047839}, {'id': 20, 'seek': 14152, 'start': 141.52, 'end': 148.32000000000002, 'text': ' of the first day of week four, we have an explicit class that partly focuses on the final projects', 'tokens': [295, 264, 700, 786, 295, 1243, 1451, 11, 321, 362, 364, 13691, 1508, 300, 17031, 16109, 322, 264, 2572, 4455], 'temperature': 0.0, 'avg_logprob': -0.0757788261481091, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.00038883634260855615}, {'id': 21, 'seek': 14152, 'start': 148.32000000000002, 'end': 154.0, 'text': \" and what the choices are for those, but it's never too late to start thinking about the final project\", 'tokens': [293, 437, 264, 7994, 366, 337, 729, 11, 457, 309, 311, 1128, 886, 3469, 281, 722, 1953, 466, 264, 2572, 1716], 'temperature': 0.0, 'avg_logprob': -0.0757788261481091, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.00038883634260855615}, {'id': 22, 'seek': 14152, 'start': 154.0, 'end': 160.4, 'text': ' and what kind of things you want to do for the final project. So do come meet with people,', 'tokens': [293, 437, 733, 295, 721, 291, 528, 281, 360, 337, 264, 2572, 1716, 13, 407, 360, 808, 1677, 365, 561, 11], 'temperature': 0.0, 'avg_logprob': -0.0757788261481091, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.00038883634260855615}, {'id': 23, 'seek': 14152, 'start': 160.4, 'end': 166.08, 'text': \" there are sort of resources on the course pages about what different TAs know about. I've also\", 'tokens': [456, 366, 1333, 295, 3593, 322, 264, 1164, 7183, 466, 437, 819, 314, 10884, 458, 466, 13, 286, 600, 611], 'temperature': 0.0, 'avg_logprob': -0.0757788261481091, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.00038883634260855615}, {'id': 24, 'seek': 14152, 'start': 166.08, 'end': 170.8, 'text': \" talked to a number of people about final projects, but clearly I can't talk to everybody.\", 'tokens': [2825, 281, 257, 1230, 295, 561, 466, 2572, 4455, 11, 457, 4448, 286, 393, 380, 751, 281, 2201, 13], 'temperature': 0.0, 'avg_logprob': -0.0757788261481091, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.00038883634260855615}, {'id': 25, 'seek': 17080, 'start': 170.8, 'end': 174.4, 'text': ' So I encourage you to also be thinking about what you want to do for final projects.', 'tokens': [407, 286, 5373, 291, 281, 611, 312, 1953, 466, 437, 291, 528, 281, 360, 337, 2572, 4455, 13], 'temperature': 0.0, 'avg_logprob': -0.11168808937072754, 'compression_ratio': 1.600896860986547, 'no_speech_prob': 6.680777005385607e-05}, {'id': 26, 'seek': 17080, 'start': 176.16000000000003, 'end': 185.68, 'text': ' Okay, so what I wanted to do today was introduce how people think about the structure of sentences', 'tokens': [1033, 11, 370, 437, 286, 1415, 281, 360, 965, 390, 5366, 577, 561, 519, 466, 264, 3877, 295, 16579], 'temperature': 0.0, 'avg_logprob': -0.11168808937072754, 'compression_ratio': 1.600896860986547, 'no_speech_prob': 6.680777005385607e-05}, {'id': 27, 'seek': 17080, 'start': 186.8, 'end': 192.64000000000001, 'text': ' and put structure on top of them to explain how human language conveys meaning.', 'tokens': [293, 829, 3877, 322, 1192, 295, 552, 281, 2903, 577, 1952, 2856, 18053, 749, 3620, 13], 'temperature': 0.0, 'avg_logprob': -0.11168808937072754, 'compression_ratio': 1.600896860986547, 'no_speech_prob': 6.680777005385607e-05}, {'id': 28, 'seek': 17080, 'start': 193.44, 'end': 199.52, 'text': \" And so our starting point for meaning and essentially what we've dealt with with word vectors\", 'tokens': [400, 370, 527, 2891, 935, 337, 3620, 293, 4476, 437, 321, 600, 15991, 365, 365, 1349, 18875], 'temperature': 0.0, 'avg_logprob': -0.11168808937072754, 'compression_ratio': 1.600896860986547, 'no_speech_prob': 6.680777005385607e-05}, {'id': 29, 'seek': 19952, 'start': 199.52, 'end': 206.88000000000002, 'text': ' up until now is we have words. And words are obviously an important part of the', 'tokens': [493, 1826, 586, 307, 321, 362, 2283, 13, 400, 2283, 366, 2745, 364, 1021, 644, 295, 264], 'temperature': 0.0, 'avg_logprob': -0.10672424005907635, 'compression_ratio': 1.7570093457943925, 'no_speech_prob': 0.00013689078332390636}, {'id': 30, 'seek': 19952, 'start': 206.88000000000002, 'end': 215.76000000000002, 'text': \" meaning of human languages. But for words in human languages, there's more that we can do with them\", 'tokens': [3620, 295, 1952, 8650, 13, 583, 337, 2283, 294, 1952, 8650, 11, 456, 311, 544, 300, 321, 393, 360, 365, 552], 'temperature': 0.0, 'avg_logprob': -0.10672424005907635, 'compression_ratio': 1.7570093457943925, 'no_speech_prob': 0.00013689078332390636}, {'id': 31, 'seek': 19952, 'start': 215.76000000000002, 'end': 222.56, 'text': ' in thinking about how to structure sentences. So in particular, the first most basic way that we', 'tokens': [294, 1953, 466, 577, 281, 3877, 16579, 13, 407, 294, 1729, 11, 264, 700, 881, 3875, 636, 300, 321], 'temperature': 0.0, 'avg_logprob': -0.10672424005907635, 'compression_ratio': 1.7570093457943925, 'no_speech_prob': 0.00013689078332390636}, {'id': 32, 'seek': 19952, 'start': 222.56, 'end': 229.12, 'text': \" think about words when we are thinking about how sentences are structured is we give to them what's\", 'tokens': [519, 466, 2283, 562, 321, 366, 1953, 466, 577, 16579, 366, 18519, 307, 321, 976, 281, 552, 437, 311], 'temperature': 0.0, 'avg_logprob': -0.10672424005907635, 'compression_ratio': 1.7570093457943925, 'no_speech_prob': 0.00013689078332390636}, {'id': 33, 'seek': 22912, 'start': 229.12, 'end': 238.0, 'text': ' called a part of speech. We can say that cat is a noun, buy is a preposition, doors and', 'tokens': [1219, 257, 644, 295, 6218, 13, 492, 393, 584, 300, 3857, 307, 257, 23307, 11, 2256, 307, 257, 2666, 5830, 11, 8077, 293], 'temperature': 0.0, 'avg_logprob': -0.1588786979774376, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.00011003408872056752}, {'id': 34, 'seek': 22912, 'start': 238.0, 'end': 247.68, 'text': ' other noun, cuddly is an adjective. And then for the word, if it was given a different part of', 'tokens': [661, 23307, 11, 269, 26656, 356, 307, 364, 44129, 13, 400, 550, 337, 264, 1349, 11, 498, 309, 390, 2212, 257, 819, 644, 295], 'temperature': 0.0, 'avg_logprob': -0.1588786979774376, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.00011003408872056752}, {'id': 35, 'seek': 22912, 'start': 247.68, 'end': 252.8, 'text': ' speech, if you saw any parts of speech in school, it was probably your told it was an article.', 'tokens': [6218, 11, 498, 291, 1866, 604, 3166, 295, 6218, 294, 1395, 11, 309, 390, 1391, 428, 1907, 309, 390, 364, 7222, 13], 'temperature': 0.0, 'avg_logprob': -0.1588786979774376, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.00011003408872056752}, {'id': 36, 'seek': 25280, 'start': 252.8, 'end': 260.64, 'text': \" Sometimes that is just put into the class of adjectives in modern linguistics and what you'll see\", 'tokens': [4803, 300, 307, 445, 829, 666, 264, 1508, 295, 29378, 1539, 294, 4363, 21766, 6006, 293, 437, 291, 603, 536], 'temperature': 0.0, 'avg_logprob': -0.2102398715176425, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.00022519913909491152}, {'id': 37, 'seek': 25280, 'start': 260.64, 'end': 266.64, 'text': ' in the resources that we use, words like that are referred to as determiners. And the idea is', 'tokens': [294, 264, 3593, 300, 321, 764, 11, 2283, 411, 300, 366, 10839, 281, 382, 15957, 433, 13, 400, 264, 1558, 307], 'temperature': 0.0, 'avg_logprob': -0.2102398715176425, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.00022519913909491152}, {'id': 38, 'seek': 25280, 'start': 266.64, 'end': 271.92, 'text': \" that there's a bunch of words includes art and art, but also other words like this and that,\", 'tokens': [300, 456, 311, 257, 3840, 295, 2283, 5974, 1523, 293, 1523, 11, 457, 611, 661, 2283, 411, 341, 293, 300, 11], 'temperature': 0.0, 'avg_logprob': -0.2102398715176425, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.00022519913909491152}, {'id': 39, 'seek': 25280, 'start': 273.92, 'end': 282.64, 'text': ' or even every which are words that occur at the beginning of something like the cuddly cat', 'tokens': [420, 754, 633, 597, 366, 2283, 300, 5160, 412, 264, 2863, 295, 746, 411, 264, 269, 26656, 356, 3857], 'temperature': 0.0, 'avg_logprob': -0.2102398715176425, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.00022519913909491152}, {'id': 40, 'seek': 28264, 'start': 282.64, 'end': 289.59999999999997, 'text': \" which have a determinative function of sort of picking out which cats that they're referring to.\", 'tokens': [597, 362, 257, 15957, 1166, 2445, 295, 1333, 295, 8867, 484, 597, 11111, 300, 436, 434, 13761, 281, 13], 'temperature': 0.0, 'avg_logprob': -0.12537343014952956, 'compression_ratio': 1.7136752136752136, 'no_speech_prob': 0.00016295297245960683}, {'id': 41, 'seek': 28264, 'start': 289.59999999999997, 'end': 295.76, 'text': \" And so we refer to those as determiners. But it's not the case that when we want to communicate with\", 'tokens': [400, 370, 321, 2864, 281, 729, 382, 15957, 433, 13, 583, 309, 311, 406, 264, 1389, 300, 562, 321, 528, 281, 7890, 365], 'temperature': 0.0, 'avg_logprob': -0.12537343014952956, 'compression_ratio': 1.7136752136752136, 'no_speech_prob': 0.00016295297245960683}, {'id': 42, 'seek': 28264, 'start': 295.76, 'end': 303.76, 'text': ' language that we just have this word salad where we say a bunch of words, we just say you know whatever', 'tokens': [2856, 300, 321, 445, 362, 341, 1349, 12604, 689, 321, 584, 257, 3840, 295, 2283, 11, 321, 445, 584, 291, 458, 2035], 'temperature': 0.0, 'avg_logprob': -0.12537343014952956, 'compression_ratio': 1.7136752136752136, 'no_speech_prob': 0.00016295297245960683}, {'id': 43, 'seek': 28264, 'start': 304.4, 'end': 312.15999999999997, 'text': ' leaking kitchen tap and let the other person put it together, we put words together in a particular', 'tokens': [32856, 6525, 5119, 293, 718, 264, 661, 954, 829, 309, 1214, 11, 321, 829, 2283, 1214, 294, 257, 1729], 'temperature': 0.0, 'avg_logprob': -0.12537343014952956, 'compression_ratio': 1.7136752136752136, 'no_speech_prob': 0.00016295297245960683}, {'id': 44, 'seek': 31216, 'start': 312.16, 'end': 319.84000000000003, 'text': ' way to express meanings. And so therefore, languages have larger units of putting meaning together.', 'tokens': [636, 281, 5109, 28138, 13, 400, 370, 4412, 11, 8650, 362, 4833, 6815, 295, 3372, 3620, 1214, 13], 'temperature': 0.0, 'avg_logprob': -0.08529113550655178, 'compression_ratio': 1.5129533678756477, 'no_speech_prob': 9.154757572105154e-05}, {'id': 45, 'seek': 31216, 'start': 319.84000000000003, 'end': 330.24, 'text': ' And the question is how we represent and think about those. Now in modern work and particular', 'tokens': [400, 264, 1168, 307, 577, 321, 2906, 293, 519, 466, 729, 13, 823, 294, 4363, 589, 293, 1729], 'temperature': 0.0, 'avg_logprob': -0.08529113550655178, 'compression_ratio': 1.5129533678756477, 'no_speech_prob': 9.154757572105154e-05}, {'id': 46, 'seek': 31216, 'start': 330.24, 'end': 337.84000000000003, 'text': ' in modern United States linguistics or even what you see in computer science classes when thinking', 'tokens': [294, 4363, 2824, 3040, 21766, 6006, 420, 754, 437, 291, 536, 294, 3820, 3497, 5359, 562, 1953], 'temperature': 0.0, 'avg_logprob': -0.08529113550655178, 'compression_ratio': 1.5129533678756477, 'no_speech_prob': 9.154757572105154e-05}, {'id': 47, 'seek': 33784, 'start': 337.84, 'end': 344.64, 'text': ' about formal languages, the most common way to approach this is with the idea of context-free', 'tokens': [466, 9860, 8650, 11, 264, 881, 2689, 636, 281, 3109, 341, 307, 365, 264, 1558, 295, 4319, 12, 10792], 'temperature': 0.0, 'avg_logprob': -0.09615569419049202, 'compression_ratio': 1.626086956521739, 'no_speech_prob': 4.527644341578707e-05}, {'id': 48, 'seek': 33784, 'start': 344.64, 'end': 350.96, 'text': \" grammars which you see at least a little bit of in 103 if you've done 103, what a linguist would\", 'tokens': [17570, 685, 597, 291, 536, 412, 1935, 257, 707, 857, 295, 294, 48784, 498, 291, 600, 1096, 48784, 11, 437, 257, 21766, 468, 576], 'temperature': 0.0, 'avg_logprob': -0.09615569419049202, 'compression_ratio': 1.626086956521739, 'no_speech_prob': 4.527644341578707e-05}, {'id': 49, 'seek': 33784, 'start': 350.96, 'end': 357.67999999999995, 'text': ' often refer to as free structure grammars. And the idea there is to say well there are bigger', 'tokens': [2049, 2864, 281, 382, 1737, 3877, 17570, 685, 13, 400, 264, 1558, 456, 307, 281, 584, 731, 456, 366, 3801], 'temperature': 0.0, 'avg_logprob': -0.09615569419049202, 'compression_ratio': 1.626086956521739, 'no_speech_prob': 4.527644341578707e-05}, {'id': 50, 'seek': 33784, 'start': 357.67999999999995, 'end': 365.12, 'text': ' units in languages that we refer to as phrases. So something like the cuddly cat is a cat', 'tokens': [6815, 294, 8650, 300, 321, 2864, 281, 382, 20312, 13, 407, 746, 411, 264, 269, 26656, 356, 3857, 307, 257, 3857], 'temperature': 0.0, 'avg_logprob': -0.09615569419049202, 'compression_ratio': 1.626086956521739, 'no_speech_prob': 4.527644341578707e-05}, {'id': 51, 'seek': 36512, 'start': 365.12, 'end': 373.2, 'text': ' with some other words modifying it. And so we refer to that as a noun phrase. But then we have', 'tokens': [365, 512, 661, 2283, 42626, 309, 13, 400, 370, 321, 2864, 281, 300, 382, 257, 23307, 9535, 13, 583, 550, 321, 362], 'temperature': 0.0, 'avg_logprob': -0.1471539752584108, 'compression_ratio': 1.6647398843930636, 'no_speech_prob': 0.00012693229655269533}, {'id': 52, 'seek': 36512, 'start': 374.32, 'end': 384.32, 'text': ' ways in which phrases can get larger by building things inside phrases. So the door here is also a', 'tokens': [2098, 294, 597, 20312, 393, 483, 4833, 538, 2390, 721, 1854, 20312, 13, 407, 264, 2853, 510, 307, 611, 257], 'temperature': 0.0, 'avg_logprob': -0.1471539752584108, 'compression_ratio': 1.6647398843930636, 'no_speech_prob': 0.00012693229655269533}, {'id': 53, 'seek': 36512, 'start': 384.32, 'end': 390.48, 'text': ' noun phrase. But then we can build something bigger around it with a preposition. So this is a', 'tokens': [23307, 9535, 13, 583, 550, 321, 393, 1322, 746, 3801, 926, 309, 365, 257, 2666, 5830, 13, 407, 341, 307, 257], 'temperature': 0.0, 'avg_logprob': -0.1471539752584108, 'compression_ratio': 1.6647398843930636, 'no_speech_prob': 0.00012693229655269533}, {'id': 54, 'seek': 39048, 'start': 390.48, 'end': 397.04, 'text': ' preposition. And then we have a prepositional phrase. And in general we can keep going. So we', 'tokens': [2666, 5830, 13, 400, 550, 321, 362, 257, 2666, 329, 2628, 9535, 13, 400, 294, 2674, 321, 393, 1066, 516, 13, 407, 321], 'temperature': 0.0, 'avg_logprob': -0.09893945184084449, 'compression_ratio': 1.9842931937172774, 'no_speech_prob': 6.959062829992035e-06}, {'id': 55, 'seek': 39048, 'start': 397.04, 'end': 403.92, 'text': ' can then make something like the cuddly cat by the door. And then the door is a noun phrase.', 'tokens': [393, 550, 652, 746, 411, 264, 269, 26656, 356, 3857, 538, 264, 2853, 13, 400, 550, 264, 2853, 307, 257, 23307, 9535, 13], 'temperature': 0.0, 'avg_logprob': -0.09893945184084449, 'compression_ratio': 1.9842931937172774, 'no_speech_prob': 6.959062829992035e-06}, {'id': 56, 'seek': 39048, 'start': 403.92, 'end': 411.6, 'text': ' The cuddly cat is a noun phrase by the door is a prepositional phrase. But then when we put it', 'tokens': [440, 269, 26656, 356, 3857, 307, 257, 23307, 9535, 538, 264, 2853, 307, 257, 2666, 329, 2628, 9535, 13, 583, 550, 562, 321, 829, 309], 'temperature': 0.0, 'avg_logprob': -0.09893945184084449, 'compression_ratio': 1.9842931937172774, 'no_speech_prob': 6.959062829992035e-06}, {'id': 57, 'seek': 39048, 'start': 411.6, 'end': 418.8, 'text': \" all together the whole of this thing becomes a bigger noun phrase. And so it's working with these\", 'tokens': [439, 1214, 264, 1379, 295, 341, 551, 3643, 257, 3801, 23307, 9535, 13, 400, 370, 309, 311, 1364, 365, 613], 'temperature': 0.0, 'avg_logprob': -0.09893945184084449, 'compression_ratio': 1.9842931937172774, 'no_speech_prob': 6.959062829992035e-06}, {'id': 58, 'seek': 41880, 'start': 418.8, 'end': 427.12, 'text': ' ideas of nested phrases, what in context free grammar terms you would refer to as non-terminals.', 'tokens': [3487, 295, 15646, 292, 20312, 11, 437, 294, 4319, 1737, 22317, 2115, 291, 576, 2864, 281, 382, 2107, 12, 7039, 33961, 13], 'temperature': 0.0, 'avg_logprob': -0.10216302369770251, 'compression_ratio': 1.676991150442478, 'no_speech_prob': 4.256708416505717e-05}, {'id': 59, 'seek': 41880, 'start': 428.16, 'end': 433.92, 'text': ' So noun phrase and prepositional phrase would be non-terminals in the context free grammar.', 'tokens': [407, 23307, 9535, 293, 2666, 329, 2628, 9535, 576, 312, 2107, 12, 7039, 33961, 294, 264, 4319, 1737, 22317, 13], 'temperature': 0.0, 'avg_logprob': -0.10216302369770251, 'compression_ratio': 1.676991150442478, 'no_speech_prob': 4.256708416505717e-05}, {'id': 60, 'seek': 41880, 'start': 433.92, 'end': 440.64, 'text': \" We can build up a bigger structure of human languages. So let's just do that for a little bit\", 'tokens': [492, 393, 1322, 493, 257, 3801, 3877, 295, 1952, 8650, 13, 407, 718, 311, 445, 360, 300, 337, 257, 707, 857], 'temperature': 0.0, 'avg_logprob': -0.10216302369770251, 'compression_ratio': 1.676991150442478, 'no_speech_prob': 4.256708416505717e-05}, {'id': 61, 'seek': 44064, 'start': 440.64, 'end': 449.2, 'text': ' to review what happens here. So we start off saying okay you can say the cat and a dog. And so those', 'tokens': [281, 3131, 437, 2314, 510, 13, 407, 321, 722, 766, 1566, 1392, 291, 393, 584, 264, 3857, 293, 257, 3000, 13, 400, 370, 729], 'temperature': 0.0, 'avg_logprob': -0.11943945472623095, 'compression_ratio': 1.6368715083798884, 'no_speech_prob': 2.0136441889917478e-05}, {'id': 62, 'seek': 44064, 'start': 449.2, 'end': 455.68, 'text': ' are noun phrases. And so we want a rule that can explain those. So we could say a noun phrase goes', 'tokens': [366, 23307, 20312, 13, 400, 370, 321, 528, 257, 4978, 300, 393, 2903, 729, 13, 407, 321, 727, 584, 257, 23307, 9535, 1709], 'temperature': 0.0, 'avg_logprob': -0.11943945472623095, 'compression_ratio': 1.6368715083798884, 'no_speech_prob': 2.0136441889917478e-05}, {'id': 63, 'seek': 44064, 'start': 455.68, 'end': 463.68, 'text': \" to the termina noun. And then somewhere over the side we'd have a lexicon. And in our lexicon\", 'tokens': [281, 264, 1433, 1426, 23307, 13, 400, 550, 4079, 670, 264, 1252, 321, 1116, 362, 257, 476, 87, 11911, 13, 400, 294, 527, 476, 87, 11911], 'temperature': 0.0, 'avg_logprob': -0.11943945472623095, 'compression_ratio': 1.6368715083798884, 'no_speech_prob': 2.0136441889917478e-05}, {'id': 64, 'seek': 46368, 'start': 463.68, 'end': 474.32, 'text': \" we'd say that dog is a noun and cat is a noun and is a determiner and that is a determiner.\", 'tokens': [321, 1116, 584, 300, 3000, 307, 257, 23307, 293, 3857, 307, 257, 23307, 293, 307, 257, 3618, 4564, 293, 300, 307, 257, 3618, 4564, 13], 'temperature': 0.0, 'avg_logprob': -0.12594858375755516, 'compression_ratio': 1.7402597402597402, 'no_speech_prob': 2.045193105004728e-05}, {'id': 65, 'seek': 46368, 'start': 475.04, 'end': 482.88, 'text': ' Okay so then we notice you can do a bit more than that. So you can say things like the large cat', 'tokens': [1033, 370, 550, 321, 3449, 291, 393, 360, 257, 857, 544, 813, 300, 13, 407, 291, 393, 584, 721, 411, 264, 2416, 3857], 'temperature': 0.0, 'avg_logprob': -0.12594858375755516, 'compression_ratio': 1.7402597402597402, 'no_speech_prob': 2.045193105004728e-05}, {'id': 66, 'seek': 46368, 'start': 483.92, 'end': 491.12, 'text': ' a barking dog. So that suggests we can have a noun phrase after the determiner.', 'tokens': [257, 32995, 3000, 13, 407, 300, 13409, 321, 393, 362, 257, 23307, 9535, 934, 264, 3618, 4564, 13], 'temperature': 0.0, 'avg_logprob': -0.12594858375755516, 'compression_ratio': 1.7402597402597402, 'no_speech_prob': 2.045193105004728e-05}, {'id': 67, 'seek': 49112, 'start': 491.12, 'end': 497.76, 'text': \" There can optionally be an adjective and then there's the noun and that can explain some things we\", 'tokens': [821, 393, 3614, 379, 312, 364, 44129, 293, 550, 456, 311, 264, 23307, 293, 300, 393, 2903, 512, 721, 321], 'temperature': 0.0, 'avg_logprob': -0.12492645414252031, 'compression_ratio': 1.6610169491525424, 'no_speech_prob': 8.601768058724701e-05}, {'id': 68, 'seek': 49112, 'start': 497.76, 'end': 506.72, 'text': ' can say. But we can also say the cat by the door or a barking dog in a crate. And so we can also', 'tokens': [393, 584, 13, 583, 321, 393, 611, 584, 264, 3857, 538, 264, 2853, 420, 257, 32995, 3000, 294, 257, 42426, 13, 400, 370, 321, 393, 611], 'temperature': 0.0, 'avg_logprob': -0.12492645414252031, 'compression_ratio': 1.6610169491525424, 'no_speech_prob': 8.601768058724701e-05}, {'id': 69, 'seek': 49112, 'start': 506.72, 'end': 514.48, 'text': \" put a prepositional phrase at the end and that's optional. But you can combine it together with an\", 'tokens': [829, 257, 2666, 329, 2628, 9535, 412, 264, 917, 293, 300, 311, 17312, 13, 583, 291, 393, 10432, 309, 1214, 365, 364], 'temperature': 0.0, 'avg_logprob': -0.12492645414252031, 'compression_ratio': 1.6610169491525424, 'no_speech_prob': 8.601768058724701e-05}, {'id': 70, 'seek': 51448, 'start': 514.48, 'end': 522.5600000000001, 'text': ' adjective for the example I gave like a barking dog on the table. And so that a grids grammar can', 'tokens': [44129, 337, 264, 1365, 286, 2729, 411, 257, 32995, 3000, 322, 264, 3199, 13, 400, 370, 300, 257, 677, 3742, 22317, 393], 'temperature': 0.0, 'avg_logprob': -0.20176172892252603, 'compression_ratio': 1.6043956043956045, 'no_speech_prob': 1.862197859736625e-05}, {'id': 71, 'seek': 51448, 'start': 522.5600000000001, 'end': 531.2, 'text': \" handle that. So then we'll keep on and say well actually you can use multiple adjectives so you\", 'tokens': [4813, 300, 13, 407, 550, 321, 603, 1066, 322, 293, 584, 731, 767, 291, 393, 764, 3866, 29378, 1539, 370, 291], 'temperature': 0.0, 'avg_logprob': -0.20176172892252603, 'compression_ratio': 1.6043956043956045, 'no_speech_prob': 1.862197859736625e-05}, {'id': 72, 'seek': 51448, 'start': 531.2, 'end': 540.24, 'text': ' can say a large barking dog or a large barking cuddly cat. No maybe not. Well sentences like that.', 'tokens': [393, 584, 257, 2416, 32995, 3000, 420, 257, 2416, 32995, 269, 26656, 356, 3857, 13, 883, 1310, 406, 13, 1042, 16579, 411, 300, 13], 'temperature': 0.0, 'avg_logprob': -0.20176172892252603, 'compression_ratio': 1.6043956043956045, 'no_speech_prob': 1.862197859736625e-05}, {'id': 73, 'seek': 54024, 'start': 540.24, 'end': 545.04, 'text': ' So we have any number of adjectives which we can represent with a star. Pots referred to as the', 'tokens': [407, 321, 362, 604, 1230, 295, 29378, 1539, 597, 321, 393, 2906, 365, 257, 3543, 13, 430, 1971, 10839, 281, 382, 264], 'temperature': 0.0, 'avg_logprob': -0.16263470559749962, 'compression_ratio': 1.7180616740088106, 'no_speech_prob': 3.9995611587073654e-05}, {'id': 74, 'seek': 54024, 'start': 545.04, 'end': 554.24, 'text': \" cleanie star. So that's good. But I forgot a bit actually. For by the door I have to have a rule\", 'tokens': [2541, 414, 3543, 13, 407, 300, 311, 665, 13, 583, 286, 5298, 257, 857, 767, 13, 1171, 538, 264, 2853, 286, 362, 281, 362, 257, 4978], 'temperature': 0.0, 'avg_logprob': -0.16263470559749962, 'compression_ratio': 1.7180616740088106, 'no_speech_prob': 3.9995611587073654e-05}, {'id': 75, 'seek': 54024, 'start': 554.24, 'end': 561.04, 'text': \" for producing by the door. So I also need a rule that's a prepositional phrase goes to a preposition\", 'tokens': [337, 10501, 538, 264, 2853, 13, 407, 286, 611, 643, 257, 4978, 300, 311, 257, 2666, 329, 2628, 9535, 1709, 281, 257, 2666, 5830], 'temperature': 0.0, 'avg_logprob': -0.16263470559749962, 'compression_ratio': 1.7180616740088106, 'no_speech_prob': 3.9995611587073654e-05}, {'id': 76, 'seek': 54024, 'start': 561.04, 'end': 569.28, 'text': ' followed by a noun phrase. And so then I also have to have prepositions and that can be in or on', 'tokens': [6263, 538, 257, 23307, 9535, 13, 400, 370, 550, 286, 611, 362, 281, 362, 2666, 329, 2451, 293, 300, 393, 312, 294, 420, 322], 'temperature': 0.0, 'avg_logprob': -0.16263470559749962, 'compression_ratio': 1.7180616740088106, 'no_speech_prob': 3.9995611587073654e-05}, {'id': 77, 'seek': 56928, 'start': 569.28, 'end': 577.8399999999999, 'text': ' or by. Okay. And I can make other sentences of course with this as well like the large crate on', 'tokens': [420, 538, 13, 1033, 13, 400, 286, 393, 652, 661, 16579, 295, 1164, 365, 341, 382, 731, 411, 264, 2416, 42426, 322], 'temperature': 0.0, 'avg_logprob': -0.11437445322672526, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 0.00012927527131978422}, {'id': 78, 'seek': 56928, 'start': 577.8399999999999, 'end': 585.52, 'text': ' the table or something like that or the large crate on the large table. Okay. So I chug along', 'tokens': [264, 3199, 420, 746, 411, 300, 420, 264, 2416, 42426, 322, 264, 2416, 3199, 13, 1033, 13, 407, 286, 417, 697, 2051], 'temperature': 0.0, 'avg_logprob': -0.11437445322672526, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 0.00012927527131978422}, {'id': 79, 'seek': 56928, 'start': 586.48, 'end': 592.9599999999999, 'text': ' and then well I could have something like talk to the cat. And so now I need more stuff. So talk', 'tokens': [293, 550, 731, 286, 727, 362, 746, 411, 751, 281, 264, 3857, 13, 400, 370, 586, 286, 643, 544, 1507, 13, 407, 751], 'temperature': 0.0, 'avg_logprob': -0.11437445322672526, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 0.00012927527131978422}, {'id': 80, 'seek': 59296, 'start': 592.96, 'end': 600.48, 'text': ' is a verb and two is still looks like a preposition. So I need to be able to make up something', 'tokens': [307, 257, 9595, 293, 732, 307, 920, 1542, 411, 257, 2666, 5830, 13, 407, 286, 643, 281, 312, 1075, 281, 652, 493, 746], 'temperature': 0.0, 'avg_logprob': -0.08415393587909167, 'compression_ratio': 1.697674418604651, 'no_speech_prob': 2.0457848222577013e-05}, {'id': 81, 'seek': 59296, 'start': 601.2, 'end': 611.44, 'text': ' with that as well. Okay. So what I can do is say I can also have a rule for a verb phrase that', 'tokens': [365, 300, 382, 731, 13, 1033, 13, 407, 437, 286, 393, 360, 307, 584, 286, 393, 611, 362, 257, 4978, 337, 257, 9595, 9535, 300], 'temperature': 0.0, 'avg_logprob': -0.08415393587909167, 'compression_ratio': 1.697674418604651, 'no_speech_prob': 2.0457848222577013e-05}, {'id': 82, 'seek': 59296, 'start': 611.44, 'end': 619.52, 'text': ' goes to a verb and then after that for something like talk to the cat that it can take a prepositional', 'tokens': [1709, 281, 257, 9595, 293, 550, 934, 300, 337, 746, 411, 751, 281, 264, 3857, 300, 309, 393, 747, 257, 2666, 329, 2628], 'temperature': 0.0, 'avg_logprob': -0.08415393587909167, 'compression_ratio': 1.697674418604651, 'no_speech_prob': 2.0457848222577013e-05}, {'id': 83, 'seek': 61952, 'start': 619.52, 'end': 632.72, 'text': ' phrase after it. And then I can say that the verb goes to talk or walked. Okay. Then I can pass', 'tokens': [9535, 934, 309, 13, 400, 550, 286, 393, 584, 300, 264, 9595, 1709, 281, 751, 420, 7628, 13, 1033, 13, 1396, 286, 393, 1320], 'temperature': 0.0, 'avg_logprob': -0.10182409286499024, 'compression_ratio': 1.4806201550387597, 'no_speech_prob': 2.110319292114582e-05}, {'id': 84, 'seek': 61952, 'start': 632.72, 'end': 640.16, 'text': \" and then I can cover those sentences. Oops. Okay. So that's that's the end of what I have here.\", 'tokens': [293, 550, 286, 393, 2060, 729, 16579, 13, 21726, 13, 1033, 13, 407, 300, 311, 300, 311, 264, 917, 295, 437, 286, 362, 510, 13], 'temperature': 0.0, 'avg_logprob': -0.10182409286499024, 'compression_ratio': 1.4806201550387597, 'no_speech_prob': 2.110319292114582e-05}, {'id': 85, 'seek': 64016, 'start': 640.16, 'end': 649.1999999999999, 'text': \" So in this sort of a way I'm handwriting a grammar. So here is now I have this grammar\", 'tokens': [407, 294, 341, 1333, 295, 257, 636, 286, 478, 39179, 257, 22317, 13, 407, 510, 307, 586, 286, 362, 341, 22317], 'temperature': 0.0, 'avg_logprob': -0.14791408055265184, 'compression_ratio': 1.622093023255814, 'no_speech_prob': 5.647912257700227e-05}, {'id': 86, 'seek': 64016, 'start': 650.8, 'end': 659.6, 'text': \" and a lexicon. And for the examples that I've written down here, this grammar and this lexicon\", 'tokens': [293, 257, 476, 87, 11911, 13, 400, 337, 264, 5110, 300, 286, 600, 3720, 760, 510, 11, 341, 22317, 293, 341, 476, 87, 11911], 'temperature': 0.0, 'avg_logprob': -0.14791408055265184, 'compression_ratio': 1.622093023255814, 'no_speech_prob': 5.647912257700227e-05}, {'id': 87, 'seek': 65960, 'start': 659.6, 'end': 670.0, 'text': ' is sufficient to pause these sort of fragments of showing expansion that I just wrote down. I mean,', 'tokens': [307, 11563, 281, 10465, 613, 1333, 295, 29197, 295, 4099, 11260, 300, 286, 445, 4114, 760, 13, 286, 914, 11], 'temperature': 0.0, 'avg_logprob': -0.1503139573174554, 'compression_ratio': 1.546875, 'no_speech_prob': 4.059333150507882e-05}, {'id': 88, 'seek': 65960, 'start': 670.0, 'end': 676.4, 'text': \" of course there's a lot more to English than what you see here. Right. So if I have something like\", 'tokens': [295, 1164, 456, 311, 257, 688, 544, 281, 3669, 813, 437, 291, 536, 510, 13, 1779, 13, 407, 498, 286, 362, 746, 411], 'temperature': 0.0, 'avg_logprob': -0.1503139573174554, 'compression_ratio': 1.546875, 'no_speech_prob': 4.059333150507882e-05}, {'id': 89, 'seek': 65960, 'start': 677.12, 'end': 689.2, 'text': ' the cat walked behind the dog, then I need some more grammar rules. So it seems then I need a rule', 'tokens': [264, 3857, 7628, 2261, 264, 3000, 11, 550, 286, 643, 512, 544, 22317, 4474, 13, 407, 309, 2544, 550, 286, 643, 257, 4978], 'temperature': 0.0, 'avg_logprob': -0.1503139573174554, 'compression_ratio': 1.546875, 'no_speech_prob': 4.059333150507882e-05}, {'id': 90, 'seek': 68920, 'start': 689.2, 'end': 694.48, 'text': ' that says I can have a sentence that goes to a noun phrase followed by a verb phrase.', 'tokens': [300, 1619, 286, 393, 362, 257, 8174, 300, 1709, 281, 257, 23307, 9535, 6263, 538, 257, 9595, 9535, 13], 'temperature': 0.0, 'avg_logprob': -0.11930113572340745, 'compression_ratio': 1.4797687861271676, 'no_speech_prob': 4.259645356796682e-05}, {'id': 91, 'seek': 68920, 'start': 695.5200000000001, 'end': 706.08, 'text': \" And I can keep on doing things of this sort. Let's see one question that Ruth Ann asked was about\", 'tokens': [400, 286, 393, 1066, 322, 884, 721, 295, 341, 1333, 13, 961, 311, 536, 472, 1168, 300, 23544, 8860, 2351, 390, 466], 'temperature': 0.0, 'avg_logprob': -0.11930113572340745, 'compression_ratio': 1.4797687861271676, 'no_speech_prob': 4.259645356796682e-05}, {'id': 92, 'seek': 68920, 'start': 706.88, 'end': 713.2800000000001, 'text': ' what do the brackets mean and is the first np different from the second.', 'tokens': [437, 360, 264, 26179, 914, 293, 307, 264, 700, 297, 79, 819, 490, 264, 1150, 13], 'temperature': 0.0, 'avg_logprob': -0.11930113572340745, 'compression_ratio': 1.4797687861271676, 'no_speech_prob': 4.259645356796682e-05}, {'id': 93, 'seek': 71328, 'start': 713.28, 'end': 723.04, 'text': \" And so for this notation on the brackets here, I mean, this is actually a common notation that's\", 'tokens': [400, 370, 337, 341, 24657, 322, 264, 26179, 510, 11, 286, 914, 11, 341, 307, 767, 257, 2689, 24657, 300, 311], 'temperature': 0.0, 'avg_logprob': -0.12478119394053584, 'compression_ratio': 1.553191489361702, 'no_speech_prob': 0.00012515412527136505}, {'id': 94, 'seek': 71328, 'start': 723.04, 'end': 730.3199999999999, 'text': \" used in linguistics. It's sort of in some sense a little bit different to traditional computer\", 'tokens': [1143, 294, 21766, 6006, 13, 467, 311, 1333, 295, 294, 512, 2020, 257, 707, 857, 819, 281, 5164, 3820], 'temperature': 0.0, 'avg_logprob': -0.12478119394053584, 'compression_ratio': 1.553191489361702, 'no_speech_prob': 0.00012515412527136505}, {'id': 95, 'seek': 71328, 'start': 730.3199999999999, 'end': 738.4, 'text': ' science notation since the star is used in both to mean zero or more of something. So you could have', 'tokens': [3497, 24657, 1670, 264, 3543, 307, 1143, 294, 1293, 281, 914, 4018, 420, 544, 295, 746, 13, 407, 291, 727, 362], 'temperature': 0.0, 'avg_logprob': -0.12478119394053584, 'compression_ratio': 1.553191489361702, 'no_speech_prob': 0.00012515412527136505}, {'id': 96, 'seek': 73840, 'start': 738.4, 'end': 745.28, 'text': \" zero one two three four five adjectives. Somehow it's usual in linguistics that when you're using\", 'tokens': [4018, 472, 732, 1045, 1451, 1732, 29378, 1539, 13, 28357, 309, 311, 7713, 294, 21766, 6006, 300, 562, 291, 434, 1228], 'temperature': 0.0, 'avg_logprob': -0.11600356943467084, 'compression_ratio': 1.576086956521739, 'no_speech_prob': 1.567966137372423e-05}, {'id': 97, 'seek': 73840, 'start': 745.28, 'end': 752.0, 'text': \" the star, you also put parentheses around it to mean it's optional. So sort of parentheses and\", 'tokens': [264, 3543, 11, 291, 611, 829, 34153, 926, 309, 281, 914, 309, 311, 17312, 13, 407, 1333, 295, 34153, 293], 'temperature': 0.0, 'avg_logprob': -0.11600356943467084, 'compression_ratio': 1.576086956521739, 'no_speech_prob': 1.567966137372423e-05}, {'id': 98, 'seek': 73840, 'start': 752.0, 'end': 758.8, 'text': \" star are used together to mean any number of something. When it's parentheses just by themselves,\", 'tokens': [3543, 366, 1143, 1214, 281, 914, 604, 1230, 295, 746, 13, 1133, 309, 311, 34153, 445, 538, 2969, 11], 'temperature': 0.0, 'avg_logprob': -0.11600356943467084, 'compression_ratio': 1.576086956521739, 'no_speech_prob': 1.567966137372423e-05}, {'id': 99, 'seek': 75880, 'start': 758.8, 'end': 769.4399999999999, 'text': \" that's then meaning zero or one. And then four are these two noun phrases different? No, they're both\", 'tokens': [300, 311, 550, 3620, 4018, 420, 472, 13, 400, 550, 1451, 366, 613, 732, 23307, 20312, 819, 30, 883, 11, 436, 434, 1293], 'temperature': 0.0, 'avg_logprob': -0.13168614440494114, 'compression_ratio': 1.5561497326203209, 'no_speech_prob': 2.0779954866156913e-05}, {'id': 100, 'seek': 75880, 'start': 769.4399999999999, 'end': 776.56, 'text': ' noun phrase rules. And so in our grammar, we can have multiple rules that expand noun phrase in', 'tokens': [23307, 9535, 4474, 13, 400, 370, 294, 527, 22317, 11, 321, 393, 362, 3866, 4474, 300, 5268, 23307, 9535, 294], 'temperature': 0.0, 'avg_logprob': -0.13168614440494114, 'compression_ratio': 1.5561497326203209, 'no_speech_prob': 2.0779954866156913e-05}, {'id': 101, 'seek': 75880, 'start': 776.56, 'end': 784.88, 'text': ' different ways. But, you know, actually in my example here, my second rule because I wrote it', 'tokens': [819, 2098, 13, 583, 11, 291, 458, 11, 767, 294, 452, 1365, 510, 11, 452, 1150, 4978, 570, 286, 4114, 309], 'temperature': 0.0, 'avg_logprob': -0.13168614440494114, 'compression_ratio': 1.5561497326203209, 'no_speech_prob': 2.0779954866156913e-05}, {'id': 102, 'seek': 78488, 'start': 784.88, 'end': 791.28, 'text': ' quite generally, it actually covers the first rule as well. So actually at that point, I can cross out', 'tokens': [1596, 5101, 11, 309, 767, 10538, 264, 700, 4978, 382, 731, 13, 407, 767, 412, 300, 935, 11, 286, 393, 3278, 484], 'temperature': 0.0, 'avg_logprob': -0.08741406644328256, 'compression_ratio': 1.6178861788617886, 'no_speech_prob': 4.530909427558072e-05}, {'id': 103, 'seek': 78488, 'start': 791.28, 'end': 797.04, 'text': \" this first rule because I don't actually need it in my grammar. But in general, you know, you have\", 'tokens': [341, 700, 4978, 570, 286, 500, 380, 767, 643, 309, 294, 452, 22317, 13, 583, 294, 2674, 11, 291, 458, 11, 291, 362], 'temperature': 0.0, 'avg_logprob': -0.08741406644328256, 'compression_ratio': 1.6178861788617886, 'no_speech_prob': 4.530909427558072e-05}, {'id': 104, 'seek': 78488, 'start': 797.04, 'end': 805.28, 'text': ' a choice between writing multiple rules for noun phrase goes to categories, which effectively gives', 'tokens': [257, 3922, 1296, 3579, 3866, 4474, 337, 23307, 9535, 1709, 281, 10479, 11, 597, 8659, 2709], 'temperature': 0.0, 'avg_logprob': -0.08741406644328256, 'compression_ratio': 1.6178861788617886, 'no_speech_prob': 4.530909427558072e-05}, {'id': 105, 'seek': 80528, 'start': 805.28, 'end': 815.28, 'text': ' your disjunction or working out by various syntactic conventions how to compress them together. Okay.', 'tokens': [428, 717, 10010, 882, 420, 1364, 484, 538, 3683, 23980, 19892, 33520, 577, 281, 14778, 552, 1214, 13, 1033, 13], 'temperature': 0.0, 'avg_logprob': -0.1260507224036045, 'compression_ratio': 1.6079545454545454, 'no_speech_prob': 6.803297583246604e-05}, {'id': 106, 'seek': 80528, 'start': 816.48, 'end': 822.72, 'text': ' So that was what gets referred to in natural language processing as constituency grammars,', 'tokens': [407, 300, 390, 437, 2170, 10839, 281, 294, 3303, 2856, 9007, 382, 46146, 17570, 685, 11], 'temperature': 0.0, 'avg_logprob': -0.1260507224036045, 'compression_ratio': 1.6079545454545454, 'no_speech_prob': 6.803297583246604e-05}, {'id': 107, 'seek': 80528, 'start': 823.68, 'end': 830.0799999999999, 'text': ' where the standard form of constituency grammar is a context-free grammar of the sort that', 'tokens': [689, 264, 3832, 1254, 295, 46146, 22317, 307, 257, 4319, 12, 10792, 22317, 295, 264, 1333, 300], 'temperature': 0.0, 'avg_logprob': -0.1260507224036045, 'compression_ratio': 1.6079545454545454, 'no_speech_prob': 6.803297583246604e-05}, {'id': 108, 'seek': 83008, 'start': 830.08, 'end': 836.88, 'text': ' I trust you saw at least a teeny bit of either in CS 103 or something like a programming language', 'tokens': [286, 3361, 291, 1866, 412, 1935, 257, 48232, 857, 295, 2139, 294, 9460, 48784, 420, 746, 411, 257, 9410, 2856], 'temperature': 0.0, 'avg_logprob': -0.11140363415082295, 'compression_ratio': 1.636, 'no_speech_prob': 4.600559259415604e-05}, {'id': 109, 'seek': 83008, 'start': 836.88, 'end': 843.5200000000001, 'text': ' as compilers formal languages class. There are other forms of grammars that also pick out constituency.', 'tokens': [382, 715, 388, 433, 9860, 8650, 1508, 13, 821, 366, 661, 6422, 295, 17570, 685, 300, 611, 1888, 484, 46146, 13], 'temperature': 0.0, 'avg_logprob': -0.11140363415082295, 'compression_ratio': 1.636, 'no_speech_prob': 4.600559259415604e-05}, {'id': 110, 'seek': 83008, 'start': 844.1600000000001, 'end': 849.2800000000001, 'text': \" There are things like tree adjoining grammars, but I'm not going to really talk about any of those now.\", 'tokens': [821, 366, 721, 411, 4230, 614, 5134, 1760, 17570, 685, 11, 457, 286, 478, 406, 516, 281, 534, 751, 466, 604, 295, 729, 586, 13], 'temperature': 0.0, 'avg_logprob': -0.11140363415082295, 'compression_ratio': 1.636, 'no_speech_prob': 4.600559259415604e-05}, {'id': 111, 'seek': 83008, 'start': 849.2800000000001, 'end': 855.84, 'text': ' What I actually want to present is a somewhat different way of looking at grammar, which is referred to', 'tokens': [708, 286, 767, 528, 281, 1974, 307, 257, 8344, 819, 636, 295, 1237, 412, 22317, 11, 597, 307, 10839, 281], 'temperature': 0.0, 'avg_logprob': -0.11140363415082295, 'compression_ratio': 1.636, 'no_speech_prob': 4.600559259415604e-05}, {'id': 112, 'seek': 85584, 'start': 855.84, 'end': 864.1600000000001, 'text': ' as dependency grammar, which puts a dependency structure over sentences. Now actually,', 'tokens': [382, 33621, 22317, 11, 597, 8137, 257, 33621, 3877, 670, 16579, 13, 823, 767, 11], 'temperature': 0.0, 'avg_logprob': -0.08442115783691406, 'compression_ratio': 1.6944444444444444, 'no_speech_prob': 1.3200951798353344e-05}, {'id': 113, 'seek': 85584, 'start': 864.1600000000001, 'end': 870.1600000000001, 'text': \" it's not that these two ways of looking at grammar have nothing to do with each other. I mean,\", 'tokens': [309, 311, 406, 300, 613, 732, 2098, 295, 1237, 412, 22317, 362, 1825, 281, 360, 365, 1184, 661, 13, 286, 914, 11], 'temperature': 0.0, 'avg_logprob': -0.08442115783691406, 'compression_ratio': 1.6944444444444444, 'no_speech_prob': 1.3200951798353344e-05}, {'id': 114, 'seek': 85584, 'start': 870.1600000000001, 'end': 876.5600000000001, 'text': \" there's a whole formal theory about the relationships between different kinds of grammars,\", 'tokens': [456, 311, 257, 1379, 9860, 5261, 466, 264, 6159, 1296, 819, 3685, 295, 17570, 685, 11], 'temperature': 0.0, 'avg_logprob': -0.08442115783691406, 'compression_ratio': 1.6944444444444444, 'no_speech_prob': 1.3200951798353344e-05}, {'id': 115, 'seek': 85584, 'start': 876.5600000000001, 'end': 884.24, 'text': ' and you can very precisely state relationships and isomorphisms between different grammars of', 'tokens': [293, 291, 393, 588, 13402, 1785, 6159, 293, 307, 32702, 13539, 1296, 819, 17570, 685, 295], 'temperature': 0.0, 'avg_logprob': -0.08442115783691406, 'compression_ratio': 1.6944444444444444, 'no_speech_prob': 1.3200951798353344e-05}, {'id': 116, 'seek': 88424, 'start': 884.24, 'end': 890.96, 'text': ' different kinds. But on the surface, these two kinds of grammars look sort of different and', 'tokens': [819, 3685, 13, 583, 322, 264, 3753, 11, 613, 732, 3685, 295, 17570, 685, 574, 1333, 295, 819, 293], 'temperature': 0.0, 'avg_logprob': -0.09196715970193187, 'compression_ratio': 1.6193181818181819, 'no_speech_prob': 7.883138096076436e-06}, {'id': 117, 'seek': 88424, 'start': 890.96, 'end': 900.8, 'text': ' emphasize different things. And for reasons of their sort of closeness to picking out relationships', 'tokens': [16078, 819, 721, 13, 400, 337, 4112, 295, 641, 1333, 295, 2611, 15264, 281, 8867, 484, 6159], 'temperature': 0.0, 'avg_logprob': -0.09196715970193187, 'compression_ratio': 1.6193181818181819, 'no_speech_prob': 7.883138096076436e-06}, {'id': 118, 'seek': 88424, 'start': 900.8, 'end': 909.2, 'text': ' and sentences and their ease of use, it turns out that in modern natural language processing,', 'tokens': [293, 16579, 293, 641, 12708, 295, 764, 11, 309, 4523, 484, 300, 294, 4363, 3303, 2856, 9007, 11], 'temperature': 0.0, 'avg_logprob': -0.09196715970193187, 'compression_ratio': 1.6193181818181819, 'no_speech_prob': 7.883138096076436e-06}, {'id': 119, 'seek': 90920, 'start': 909.2, 'end': 917.36, 'text': ' starting, I guess, around 2000, sort of really in the last 20 years, NLP people have really swung', 'tokens': [2891, 11, 286, 2041, 11, 926, 8132, 11, 1333, 295, 534, 294, 264, 1036, 945, 924, 11, 426, 45196, 561, 362, 534, 1693, 1063], 'temperature': 0.0, 'avg_logprob': -0.09878004590670268, 'compression_ratio': 1.6506550218340612, 'no_speech_prob': 9.881844744086266e-05}, {'id': 120, 'seek': 90920, 'start': 917.36, 'end': 923.84, 'text': ' behind dependency grammars. So if you look around now where people are using grammars in NLP,', 'tokens': [2261, 33621, 17570, 685, 13, 407, 498, 291, 574, 926, 586, 689, 561, 366, 1228, 17570, 685, 294, 426, 45196, 11], 'temperature': 0.0, 'avg_logprob': -0.09878004590670268, 'compression_ratio': 1.6506550218340612, 'no_speech_prob': 9.881844744086266e-05}, {'id': 121, 'seek': 90920, 'start': 923.84, 'end': 929.76, 'text': \" by far the most common thing that's being used is dependency grammars. So I'm going to teach us\", 'tokens': [538, 1400, 264, 881, 2689, 551, 300, 311, 885, 1143, 307, 33621, 17570, 685, 13, 407, 286, 478, 516, 281, 2924, 505], 'temperature': 0.0, 'avg_logprob': -0.09878004590670268, 'compression_ratio': 1.6506550218340612, 'no_speech_prob': 9.881844744086266e-05}, {'id': 122, 'seek': 90920, 'start': 929.76, 'end': 936.24, 'text': \" today a bit about those. And for what we're going to build in assignment three is building\", 'tokens': [965, 257, 857, 466, 729, 13, 400, 337, 437, 321, 434, 516, 281, 1322, 294, 15187, 1045, 307, 2390], 'temperature': 0.0, 'avg_logprob': -0.09878004590670268, 'compression_ratio': 1.6506550218340612, 'no_speech_prob': 9.881844744086266e-05}, {'id': 123, 'seek': 93624, 'start': 936.24, 'end': 944.4, 'text': ' using supervised learning and neural dependency parser. So the idea of dependency grammar is that', 'tokens': [1228, 46533, 2539, 293, 18161, 33621, 21156, 260, 13, 407, 264, 1558, 295, 33621, 22317, 307, 300], 'temperature': 0.0, 'avg_logprob': -0.08364326245075948, 'compression_ratio': 1.8113207547169812, 'no_speech_prob': 7.710591307841241e-05}, {'id': 124, 'seek': 93624, 'start': 944.4, 'end': 952.64, 'text': \" when we have a sentence, what we're going to do is we're going to say for each word, what other\", 'tokens': [562, 321, 362, 257, 8174, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 584, 337, 1184, 1349, 11, 437, 661], 'temperature': 0.0, 'avg_logprob': -0.08364326245075948, 'compression_ratio': 1.8113207547169812, 'no_speech_prob': 7.710591307841241e-05}, {'id': 125, 'seek': 93624, 'start': 952.64, 'end': 960.5600000000001, 'text': \" words modify it. So what we're going to do is when we say the large crate, we're going to say,\", 'tokens': [2283, 16927, 309, 13, 407, 437, 321, 434, 516, 281, 360, 307, 562, 321, 584, 264, 2416, 42426, 11, 321, 434, 516, 281, 584, 11], 'temperature': 0.0, 'avg_logprob': -0.08364326245075948, 'compression_ratio': 1.8113207547169812, 'no_speech_prob': 7.710591307841241e-05}, {'id': 126, 'seek': 96056, 'start': 960.56, 'end': 969.04, 'text': ' okay, well, large is modifying crate and that is modifying crate in the kitchen. That is modifying', 'tokens': [1392, 11, 731, 11, 2416, 307, 42626, 42426, 293, 300, 307, 42626, 42426, 294, 264, 6525, 13, 663, 307, 42626], 'temperature': 0.0, 'avg_logprob': -0.1550093454027933, 'compression_ratio': 1.729559748427673, 'no_speech_prob': 5.512423376785591e-05}, {'id': 127, 'seek': 96056, 'start': 969.04, 'end': 978.0799999999999, 'text': \" kitchen by the door. That is modifying door. And so I'm showing modification, a dependency or\", 'tokens': [6525, 538, 264, 2853, 13, 663, 307, 42626, 2853, 13, 400, 370, 286, 478, 4099, 26747, 11, 257, 33621, 420], 'temperature': 0.0, 'avg_logprob': -0.1550093454027933, 'compression_ratio': 1.729559748427673, 'no_speech_prob': 5.512423376785591e-05}, {'id': 128, 'seek': 96056, 'start': 978.0799999999999, 'end': 985.3599999999999, 'text': \" an attachment relationship by drawing an arrow from the head to what's referred to\", 'tokens': [364, 19431, 2480, 538, 6316, 364, 11610, 490, 264, 1378, 281, 437, 311, 10839, 281], 'temperature': 0.0, 'avg_logprob': -0.1550093454027933, 'compression_ratio': 1.729559748427673, 'no_speech_prob': 5.512423376785591e-05}, {'id': 129, 'seek': 98536, 'start': 985.36, 'end': 993.44, 'text': ' in dependency grammar as the dependent. The thing that modifies further specifies or attaches', 'tokens': [294, 33621, 22317, 382, 264, 12334, 13, 440, 551, 300, 1072, 11221, 3052, 1608, 11221, 420, 49404], 'temperature': 0.0, 'avg_logprob': -0.1363479990354726, 'compression_ratio': 1.6987951807228916, 'no_speech_prob': 1.8018861737800762e-05}, {'id': 130, 'seek': 98536, 'start': 994.24, 'end': 1004.16, 'text': \" to the head. Okay, so that's the start of this. Well, another dependency that is that, well,\", 'tokens': [281, 264, 1378, 13, 1033, 11, 370, 300, 311, 264, 722, 295, 341, 13, 1042, 11, 1071, 33621, 300, 307, 300, 11, 731, 11], 'temperature': 0.0, 'avg_logprob': -0.1363479990354726, 'compression_ratio': 1.6987951807228916, 'no_speech_prob': 1.8018861737800762e-05}, {'id': 131, 'seek': 98536, 'start': 1006.16, 'end': 1012.4, 'text': \" looking in the large crate, that where you're looking is in the large crate. So you're going to\", 'tokens': [1237, 294, 264, 2416, 42426, 11, 300, 689, 291, 434, 1237, 307, 294, 264, 2416, 42426, 13, 407, 291, 434, 516, 281], 'temperature': 0.0, 'avg_logprob': -0.1363479990354726, 'compression_ratio': 1.6987951807228916, 'no_speech_prob': 1.8018861737800762e-05}, {'id': 132, 'seek': 101240, 'start': 1012.4, 'end': 1020.24, 'text': \" want to have the large in the large crate as being a dependent of look. And so that's also going\", 'tokens': [528, 281, 362, 264, 2416, 294, 264, 2416, 42426, 382, 885, 257, 12334, 295, 574, 13, 400, 370, 300, 311, 611, 516], 'temperature': 0.0, 'avg_logprob': -0.06978921487297811, 'compression_ratio': 1.6590909090909092, 'no_speech_prob': 2.3149734261096455e-05}, {'id': 133, 'seek': 101240, 'start': 1020.24, 'end': 1028.96, 'text': \" to be a dependency relationship here. And then there's one final bit that might seem a little bit\", 'tokens': [281, 312, 257, 33621, 2480, 510, 13, 400, 550, 456, 311, 472, 2572, 857, 300, 1062, 1643, 257, 707, 857], 'temperature': 0.0, 'avg_logprob': -0.06978921487297811, 'compression_ratio': 1.6590909090909092, 'no_speech_prob': 2.3149734261096455e-05}, {'id': 134, 'seek': 101240, 'start': 1028.96, 'end': 1035.12, 'text': \" confusing to people. And that's actually when we have these prepositions, there are two ways that\", 'tokens': [13181, 281, 561, 13, 400, 300, 311, 767, 562, 321, 362, 613, 2666, 329, 2451, 11, 456, 366, 732, 2098, 300], 'temperature': 0.0, 'avg_logprob': -0.06978921487297811, 'compression_ratio': 1.6590909090909092, 'no_speech_prob': 2.3149734261096455e-05}, {'id': 135, 'seek': 103512, 'start': 1035.12, 'end': 1043.12, 'text': ' you can think that this might work. So if it was something like look in the crate,', 'tokens': [291, 393, 519, 300, 341, 1062, 589, 13, 407, 498, 309, 390, 746, 411, 574, 294, 264, 42426, 11], 'temperature': 0.0, 'avg_logprob': -0.11460665922898512, 'compression_ratio': 1.6729559748427674, 'no_speech_prob': 1.8898303096648306e-05}, {'id': 136, 'seek': 103512, 'start': 1045.76, 'end': 1053.12, 'text': ' that seems like that is a dependent of crate, but you could think that you want to say look in', 'tokens': [300, 2544, 411, 300, 307, 257, 12334, 295, 42426, 11, 457, 291, 727, 519, 300, 291, 528, 281, 584, 574, 294], 'temperature': 0.0, 'avg_logprob': -0.11460665922898512, 'compression_ratio': 1.6729559748427674, 'no_speech_prob': 1.8898303096648306e-05}, {'id': 137, 'seek': 103512, 'start': 1053.76, 'end': 1059.6, 'text': \" and it's in the crate and give this dependency relationship with the sort of preposition\", 'tokens': [293, 309, 311, 294, 264, 42426, 293, 976, 341, 33621, 2480, 365, 264, 1333, 295, 2666, 5830], 'temperature': 0.0, 'avg_logprob': -0.11460665922898512, 'compression_ratio': 1.6729559748427674, 'no_speech_prob': 1.8898303096648306e-05}, {'id': 138, 'seek': 105960, 'start': 1059.6, 'end': 1066.8799999999999, 'text': \" as sort of thinking of it as the head of what was before our prepositional phrase. And that's\", 'tokens': [382, 1333, 295, 1953, 295, 309, 382, 264, 1378, 295, 437, 390, 949, 527, 2666, 329, 2628, 9535, 13, 400, 300, 311], 'temperature': 0.0, 'avg_logprob': -0.10436746933880975, 'compression_ratio': 1.7300884955752212, 'no_speech_prob': 5.296344170346856e-05}, {'id': 139, 'seek': 105960, 'start': 1066.8799999999999, 'end': 1075.6799999999998, 'text': \" a possible strategy in the dependency grammar. But what I'm going to show you today and what you're\", 'tokens': [257, 1944, 5206, 294, 264, 33621, 22317, 13, 583, 437, 286, 478, 516, 281, 855, 291, 965, 293, 437, 291, 434], 'temperature': 0.0, 'avg_logprob': -0.10436746933880975, 'compression_ratio': 1.7300884955752212, 'no_speech_prob': 5.296344170346856e-05}, {'id': 140, 'seek': 105960, 'start': 1075.6799999999998, 'end': 1082.8, 'text': ' going to use in this assignment is dependency grammars that follow the representation of universal', 'tokens': [516, 281, 764, 294, 341, 15187, 307, 33621, 17570, 685, 300, 1524, 264, 10290, 295, 11455], 'temperature': 0.0, 'avg_logprob': -0.10436746933880975, 'compression_ratio': 1.7300884955752212, 'no_speech_prob': 5.296344170346856e-05}, {'id': 141, 'seek': 105960, 'start': 1082.8, 'end': 1089.04, 'text': ' dependencies. And universal dependencies is a framework which actually I was involved in creating,', 'tokens': [36606, 13, 400, 11455, 36606, 307, 257, 8388, 597, 767, 286, 390, 3288, 294, 4084, 11], 'temperature': 0.0, 'avg_logprob': -0.10436746933880975, 'compression_ratio': 1.7300884955752212, 'no_speech_prob': 5.296344170346856e-05}, {'id': 142, 'seek': 108904, 'start': 1089.04, 'end': 1095.84, 'text': ' which was set up to try and give a common dependency grammar over many different human languages.', 'tokens': [597, 390, 992, 493, 281, 853, 293, 976, 257, 2689, 33621, 22317, 670, 867, 819, 1952, 8650, 13], 'temperature': 0.0, 'avg_logprob': -0.08312667784143667, 'compression_ratio': 1.6, 'no_speech_prob': 4.388752131490037e-05}, {'id': 143, 'seek': 108904, 'start': 1095.84, 'end': 1103.12, 'text': ' And in the design decisions that were made in the context of designing universal dependencies,', 'tokens': [400, 294, 264, 1715, 5327, 300, 645, 1027, 294, 264, 4319, 295, 14685, 11455, 36606, 11], 'temperature': 0.0, 'avg_logprob': -0.08312667784143667, 'compression_ratio': 1.6, 'no_speech_prob': 4.388752131490037e-05}, {'id': 144, 'seek': 108904, 'start': 1104.48, 'end': 1112.96, 'text': ' what we decided was that for what in some languages you use prepositions, lots of other', 'tokens': [437, 321, 3047, 390, 300, 337, 437, 294, 512, 8650, 291, 764, 2666, 329, 2451, 11, 3195, 295, 661], 'temperature': 0.0, 'avg_logprob': -0.08312667784143667, 'compression_ratio': 1.6, 'no_speech_prob': 4.388752131490037e-05}, {'id': 145, 'seek': 111296, 'start': 1112.96, 'end': 1119.92, 'text': \" languages make much more use of case marking. So if you've seen something like German, you've seen\", 'tokens': [8650, 652, 709, 544, 764, 295, 1389, 25482, 13, 407, 498, 291, 600, 1612, 746, 411, 6521, 11, 291, 600, 1612], 'temperature': 0.0, 'avg_logprob': -0.13412540260402636, 'compression_ratio': 1.7168141592920354, 'no_speech_prob': 2.5050854674191214e-05}, {'id': 146, 'seek': 111296, 'start': 1119.92, 'end': 1128.48, 'text': ' more case markings like genitive and date of cases. And in other languages like Latin or Finnish,', 'tokens': [544, 1389, 39087, 411, 1049, 2187, 293, 4002, 295, 3331, 13, 400, 294, 661, 8650, 411, 10803, 420, 38429, 11], 'temperature': 0.0, 'avg_logprob': -0.13412540260402636, 'compression_ratio': 1.7168141592920354, 'no_speech_prob': 2.5050854674191214e-05}, {'id': 147, 'seek': 111296, 'start': 1129.76, 'end': 1135.28, 'text': ' lots of Native American languages, you have many more case markings again, which cover most of', 'tokens': [3195, 295, 15093, 2665, 8650, 11, 291, 362, 867, 544, 1389, 39087, 797, 11, 597, 2060, 881, 295], 'temperature': 0.0, 'avg_logprob': -0.13412540260402636, 'compression_ratio': 1.7168141592920354, 'no_speech_prob': 2.5050854674191214e-05}, {'id': 148, 'seek': 113528, 'start': 1135.28, 'end': 1143.68, 'text': ' the role of prepositions. So in universal dependencies, essentially in the crate is treated like a', 'tokens': [264, 3090, 295, 2666, 329, 2451, 13, 407, 294, 11455, 36606, 11, 4476, 294, 264, 42426, 307, 8668, 411, 257], 'temperature': 0.0, 'avg_logprob': -0.10655800501505534, 'compression_ratio': 1.6764705882352942, 'no_speech_prob': 2.7399037207942456e-05}, {'id': 149, 'seek': 113528, 'start': 1143.68, 'end': 1152.56, 'text': \" case marked noun. And so what we say is that the in is also a dependent of crate and then you're\", 'tokens': [1389, 12658, 23307, 13, 400, 370, 437, 321, 584, 307, 300, 264, 294, 307, 611, 257, 12334, 295, 42426, 293, 550, 291, 434], 'temperature': 0.0, 'avg_logprob': -0.10655800501505534, 'compression_ratio': 1.6764705882352942, 'no_speech_prob': 2.7399037207942456e-05}, {'id': 150, 'seek': 113528, 'start': 1152.56, 'end': 1161.44, 'text': ' looking in the crate. So in the structure we adopt in as dependent of crate, this in as a', 'tokens': [1237, 294, 264, 42426, 13, 407, 294, 264, 3877, 321, 6878, 294, 382, 12334, 295, 42426, 11, 341, 294, 382, 257], 'temperature': 0.0, 'avg_logprob': -0.10655800501505534, 'compression_ratio': 1.6764705882352942, 'no_speech_prob': 2.7399037207942456e-05}, {'id': 151, 'seek': 116144, 'start': 1161.44, 'end': 1170.16, 'text': ' dependent of kitchen, this by as a dependent of door. And then we have these prepositional phrases', 'tokens': [12334, 295, 6525, 11, 341, 538, 382, 257, 12334, 295, 2853, 13, 400, 550, 321, 362, 613, 2666, 329, 2628, 20312], 'temperature': 0.0, 'avg_logprob': -0.11297445827060276, 'compression_ratio': 1.653179190751445, 'no_speech_prob': 2.743662662396673e-05}, {'id': 152, 'seek': 116144, 'start': 1170.16, 'end': 1177.2, 'text': ' in the kitchen by the door and we want to work out well what they modify. Well in the kitchen', 'tokens': [294, 264, 6525, 538, 264, 2853, 293, 321, 528, 281, 589, 484, 731, 437, 436, 16927, 13, 1042, 294, 264, 6525], 'temperature': 0.0, 'avg_logprob': -0.11297445827060276, 'compression_ratio': 1.653179190751445, 'no_speech_prob': 2.743662662396673e-05}, {'id': 153, 'seek': 116144, 'start': 1178.3200000000002, 'end': 1183.8400000000001, 'text': \" is modifying crate right because it's a crate in the kitchen. So we're going to say that it's\", 'tokens': [307, 42626, 42426, 558, 570, 309, 311, 257, 42426, 294, 264, 6525, 13, 407, 321, 434, 516, 281, 584, 300, 309, 311], 'temperature': 0.0, 'avg_logprob': -0.11297445827060276, 'compression_ratio': 1.653179190751445, 'no_speech_prob': 2.743662662396673e-05}, {'id': 154, 'seek': 118384, 'start': 1183.84, 'end': 1192.56, 'text': \" this piece is a dependent of crate. And then well what about by the door? Well it's not really\", 'tokens': [341, 2522, 307, 257, 12334, 295, 42426, 13, 400, 550, 731, 437, 466, 538, 264, 2853, 30, 1042, 309, 311, 406, 534], 'temperature': 0.0, 'avg_logprob': -0.1333595178066156, 'compression_ratio': 1.8280254777070064, 'no_speech_prob': 1.145769601862412e-05}, {'id': 155, 'seek': 118384, 'start': 1192.56, 'end': 1199.6799999999998, 'text': \" meaning that's a kitchen by the door and it's not meaning to look by the door. Again it's a crate\", 'tokens': [3620, 300, 311, 257, 6525, 538, 264, 2853, 293, 309, 311, 406, 3620, 281, 574, 538, 264, 2853, 13, 3764, 309, 311, 257, 42426], 'temperature': 0.0, 'avg_logprob': -0.1333595178066156, 'compression_ratio': 1.8280254777070064, 'no_speech_prob': 1.145769601862412e-05}, {'id': 156, 'seek': 118384, 'start': 1199.6799999999998, 'end': 1206.32, 'text': \" by the door. And so what we're going to have is the crate also has door as a dependent. And so\", 'tokens': [538, 264, 2853, 13, 400, 370, 437, 321, 434, 516, 281, 362, 307, 264, 42426, 611, 575, 2853, 382, 257, 12334, 13, 400, 370], 'temperature': 0.0, 'avg_logprob': -0.1333595178066156, 'compression_ratio': 1.8280254777070064, 'no_speech_prob': 1.145769601862412e-05}, {'id': 157, 'seek': 120632, 'start': 1206.32, 'end': 1220.96, 'text': \" that gives us our full dependency structure of this sentence. Okay. And so that's a teeny introduction\", 'tokens': [300, 2709, 505, 527, 1577, 33621, 3877, 295, 341, 8174, 13, 1033, 13, 400, 370, 300, 311, 257, 48232, 9339], 'temperature': 0.0, 'avg_logprob': -0.11304283142089844, 'compression_ratio': 1.5483870967741935, 'no_speech_prob': 8.3877475844929e-06}, {'id': 158, 'seek': 120632, 'start': 1220.96, 'end': 1227.04, 'text': \" to syntactic structure. I'm going to say a bit more about it and give a few more examples.\", 'tokens': [281, 23980, 19892, 3877, 13, 286, 478, 516, 281, 584, 257, 857, 544, 466, 309, 293, 976, 257, 1326, 544, 5110, 13], 'temperature': 0.0, 'avg_logprob': -0.11304283142089844, 'compression_ratio': 1.5483870967741935, 'no_speech_prob': 8.3877475844929e-06}, {'id': 159, 'seek': 120632, 'start': 1227.6, 'end': 1233.76, 'text': ' But let me just for a moment sort of say a little bit about why are we interested in syntactic', 'tokens': [583, 718, 385, 445, 337, 257, 1623, 1333, 295, 584, 257, 707, 857, 466, 983, 366, 321, 3102, 294, 23980, 19892], 'temperature': 0.0, 'avg_logprob': -0.11304283142089844, 'compression_ratio': 1.5483870967741935, 'no_speech_prob': 8.3877475844929e-06}, {'id': 160, 'seek': 123376, 'start': 1233.76, 'end': 1241.28, 'text': ' structure? Why do we need to know the structure of sentences? And this gets into how does human', 'tokens': [3877, 30, 1545, 360, 321, 643, 281, 458, 264, 3877, 295, 16579, 30, 400, 341, 2170, 666, 577, 775, 1952], 'temperature': 0.0, 'avg_logprob': -0.09723100354594569, 'compression_ratio': 1.6067415730337078, 'no_speech_prob': 2.962195321742911e-05}, {'id': 161, 'seek': 123376, 'start': 1241.28, 'end': 1250.64, 'text': ' languages work? So human languages can can communicate very complex ideas. I mean in fact you know', 'tokens': [8650, 589, 30, 407, 1952, 8650, 393, 393, 7890, 588, 3997, 3487, 13, 286, 914, 294, 1186, 291, 458], 'temperature': 0.0, 'avg_logprob': -0.09723100354594569, 'compression_ratio': 1.6067415730337078, 'no_speech_prob': 2.962195321742911e-05}, {'id': 162, 'seek': 123376, 'start': 1250.64, 'end': 1257.36, 'text': ' anything that humans know how to communicate to one another they communicate pretty much by', 'tokens': [1340, 300, 6255, 458, 577, 281, 7890, 281, 472, 1071, 436, 7890, 1238, 709, 538], 'temperature': 0.0, 'avg_logprob': -0.09723100354594569, 'compression_ratio': 1.6067415730337078, 'no_speech_prob': 2.962195321742911e-05}, {'id': 163, 'seek': 125736, 'start': 1257.36, 'end': 1265.76, 'text': \" using words. So we can structure and communicate very complex ideas. But we can't communicate a\", 'tokens': [1228, 2283, 13, 407, 321, 393, 3877, 293, 7890, 588, 3997, 3487, 13, 583, 321, 393, 380, 7890, 257], 'temperature': 0.0, 'avg_logprob': -0.09666695378043434, 'compression_ratio': 1.792626728110599, 'no_speech_prob': 4.119554068893194e-05}, {'id': 164, 'seek': 125736, 'start': 1265.76, 'end': 1274.9599999999998, 'text': \" really complex idea by one word. We can't just you know choose a word like you know empathy and say\", 'tokens': [534, 3997, 1558, 538, 472, 1349, 13, 492, 393, 380, 445, 291, 458, 2826, 257, 1349, 411, 291, 458, 18701, 293, 584], 'temperature': 0.0, 'avg_logprob': -0.09666695378043434, 'compression_ratio': 1.792626728110599, 'no_speech_prob': 4.119554068893194e-05}, {'id': 165, 'seek': 125736, 'start': 1274.9599999999998, 'end': 1279.6, 'text': \" it with a lot of meaning and say empathy and the other person's meant to understand everything\", 'tokens': [309, 365, 257, 688, 295, 3620, 293, 584, 18701, 293, 264, 661, 954, 311, 4140, 281, 1223, 1203], 'temperature': 0.0, 'avg_logprob': -0.09666695378043434, 'compression_ratio': 1.792626728110599, 'no_speech_prob': 4.119554068893194e-05}, {'id': 166, 'seek': 125736, 'start': 1279.6, 'end': 1286.56, 'text': ' about what that means. Right. We have to compose a complex meaning that explains things by putting', 'tokens': [466, 437, 300, 1355, 13, 1779, 13, 492, 362, 281, 35925, 257, 3997, 3620, 300, 13948, 721, 538, 3372], 'temperature': 0.0, 'avg_logprob': -0.09666695378043434, 'compression_ratio': 1.792626728110599, 'no_speech_prob': 4.119554068893194e-05}, {'id': 167, 'seek': 128656, 'start': 1286.56, 'end': 1294.0, 'text': ' words together into bigger units. And the syntax of a language allows us to put words together', 'tokens': [2283, 1214, 666, 3801, 6815, 13, 400, 264, 28431, 295, 257, 2856, 4045, 505, 281, 829, 2283, 1214], 'temperature': 0.0, 'avg_logprob': -0.0832855505101821, 'compression_ratio': 1.8018867924528301, 'no_speech_prob': 8.599326247349381e-05}, {'id': 168, 'seek': 128656, 'start': 1294.0, 'end': 1302.08, 'text': ' into bigger units where we can build up and convey to other people a complex meaning. And so', 'tokens': [666, 3801, 6815, 689, 321, 393, 1322, 493, 293, 16965, 281, 661, 561, 257, 3997, 3620, 13, 400, 370], 'temperature': 0.0, 'avg_logprob': -0.0832855505101821, 'compression_ratio': 1.8018867924528301, 'no_speech_prob': 8.599326247349381e-05}, {'id': 169, 'seek': 128656, 'start': 1302.08, 'end': 1308.6399999999999, 'text': \" then the listener doesn't get this syntactic structure. Right. The syntactic structure of the\", 'tokens': [550, 264, 31569, 1177, 380, 483, 341, 23980, 19892, 3877, 13, 1779, 13, 440, 23980, 19892, 3877, 295, 264], 'temperature': 0.0, 'avg_logprob': -0.0832855505101821, 'compression_ratio': 1.8018867924528301, 'no_speech_prob': 8.599326247349381e-05}, {'id': 170, 'seek': 128656, 'start': 1308.6399999999999, 'end': 1316.0, 'text': ' sentence is hidden from the listener. All the listener gets is a sequence of words one after another', 'tokens': [8174, 307, 7633, 490, 264, 31569, 13, 1057, 264, 31569, 2170, 307, 257, 8310, 295, 2283, 472, 934, 1071], 'temperature': 0.0, 'avg_logprob': -0.0832855505101821, 'compression_ratio': 1.8018867924528301, 'no_speech_prob': 8.599326247349381e-05}, {'id': 171, 'seek': 131600, 'start': 1316.0, 'end': 1323.6, 'text': ' bang bang bang. So the listener has to be able to do what I was just trying to do in this example', 'tokens': [8550, 8550, 8550, 13, 407, 264, 31569, 575, 281, 312, 1075, 281, 360, 437, 286, 390, 445, 1382, 281, 360, 294, 341, 1365], 'temperature': 0.0, 'avg_logprob': -0.08079368151151217, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 9.717762441141531e-05}, {'id': 172, 'seek': 131600, 'start': 1323.6, 'end': 1331.52, 'text': ' that as the sequence of words comes in that the listener works out which words modify which', 'tokens': [300, 382, 264, 8310, 295, 2283, 1487, 294, 300, 264, 31569, 1985, 484, 597, 2283, 16927, 597], 'temperature': 0.0, 'avg_logprob': -0.08079368151151217, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 9.717762441141531e-05}, {'id': 173, 'seek': 131600, 'start': 1331.52, 'end': 1338.72, 'text': ' are the words and therefore can construct the structure of the sentence and hence the meaning of', 'tokens': [366, 264, 2283, 293, 4412, 393, 7690, 264, 3877, 295, 264, 8174, 293, 16678, 264, 3620, 295], 'temperature': 0.0, 'avg_logprob': -0.08079368151151217, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 9.717762441141531e-05}, {'id': 174, 'seek': 133872, 'start': 1338.72, 'end': 1347.2, 'text': ' the sentence. And so in the same way if we want to build clever neural net models that can understand', 'tokens': [264, 8174, 13, 400, 370, 294, 264, 912, 636, 498, 321, 528, 281, 1322, 13494, 18161, 2533, 5245, 300, 393, 1223], 'temperature': 0.0, 'avg_logprob': -0.07410798516384391, 'compression_ratio': 1.7354260089686098, 'no_speech_prob': 2.317749022040516e-05}, {'id': 175, 'seek': 133872, 'start': 1347.2, 'end': 1354.24, 'text': ' the meaning of sentences those clever neural net models also have to understand what is this', 'tokens': [264, 3620, 295, 16579, 729, 13494, 18161, 2533, 5245, 611, 362, 281, 1223, 437, 307, 341], 'temperature': 0.0, 'avg_logprob': -0.07410798516384391, 'compression_ratio': 1.7354260089686098, 'no_speech_prob': 2.317749022040516e-05}, {'id': 176, 'seek': 133872, 'start': 1354.24, 'end': 1359.68, 'text': \" structure of the sentence so that they can interpret the language correctly. And we'll go through\", 'tokens': [3877, 295, 264, 8174, 370, 300, 436, 393, 7302, 264, 2856, 8944, 13, 400, 321, 603, 352, 807], 'temperature': 0.0, 'avg_logprob': -0.07410798516384391, 'compression_ratio': 1.7354260089686098, 'no_speech_prob': 2.317749022040516e-05}, {'id': 177, 'seek': 133872, 'start': 1359.68, 'end': 1366.64, 'text': \" some examples and see more of that. Okay. So the fundamental point that we're going to sort of\", 'tokens': [512, 5110, 293, 536, 544, 295, 300, 13, 1033, 13, 407, 264, 8088, 935, 300, 321, 434, 516, 281, 1333, 295], 'temperature': 0.0, 'avg_logprob': -0.07410798516384391, 'compression_ratio': 1.7354260089686098, 'no_speech_prob': 2.317749022040516e-05}, {'id': 178, 'seek': 136664, 'start': 1366.64, 'end': 1375.3600000000001, 'text': ' spend a bit more time on is that these choices of how you build up the structure of a language', 'tokens': [3496, 257, 857, 544, 565, 322, 307, 300, 613, 7994, 295, 577, 291, 1322, 493, 264, 3877, 295, 257, 2856], 'temperature': 0.0, 'avg_logprob': -0.07657560507456461, 'compression_ratio': 1.6395348837209303, 'no_speech_prob': 9.007241169456393e-05}, {'id': 179, 'seek': 136664, 'start': 1376.3200000000002, 'end': 1384.48, 'text': ' change the interpretation of the language and a human listener or equally a natural language', 'tokens': [1319, 264, 14174, 295, 264, 2856, 293, 257, 1952, 31569, 420, 12309, 257, 3303, 2856], 'temperature': 0.0, 'avg_logprob': -0.07657560507456461, 'compression_ratio': 1.6395348837209303, 'no_speech_prob': 9.007241169456393e-05}, {'id': 180, 'seek': 136664, 'start': 1384.48, 'end': 1393.2800000000002, 'text': ' understanding program has to make in a sort of probabilistic fashion choices as to which words', 'tokens': [3701, 1461, 575, 281, 652, 294, 257, 1333, 295, 31959, 3142, 6700, 7994, 382, 281, 597, 2283], 'temperature': 0.0, 'avg_logprob': -0.07657560507456461, 'compression_ratio': 1.6395348837209303, 'no_speech_prob': 9.007241169456393e-05}, {'id': 181, 'seek': 139328, 'start': 1393.28, 'end': 1399.84, 'text': \" modify I depend upon which other words so that they're coming up with the interpretation of the\", 'tokens': [16927, 286, 5672, 3564, 597, 661, 2283, 370, 300, 436, 434, 1348, 493, 365, 264, 14174, 295, 264], 'temperature': 0.0, 'avg_logprob': -0.08170332330645937, 'compression_ratio': 1.5714285714285714, 'no_speech_prob': 8.329062984557822e-05}, {'id': 182, 'seek': 139328, 'start': 1399.84, 'end': 1408.96, 'text': ' sentence that they think was intended by the person who said it. Okay. So to get a sense of this', 'tokens': [8174, 300, 436, 519, 390, 10226, 538, 264, 954, 567, 848, 309, 13, 1033, 13, 407, 281, 483, 257, 2020, 295, 341], 'temperature': 0.0, 'avg_logprob': -0.08170332330645937, 'compression_ratio': 1.5714285714285714, 'no_speech_prob': 8.329062984557822e-05}, {'id': 183, 'seek': 139328, 'start': 1408.96, 'end': 1416.6399999999999, 'text': \" and how sentence structure is interesting and difficult what I'm going to go through now is a\", 'tokens': [293, 577, 8174, 3877, 307, 1880, 293, 2252, 437, 286, 478, 516, 281, 352, 807, 586, 307, 257], 'temperature': 0.0, 'avg_logprob': -0.08170332330645937, 'compression_ratio': 1.5714285714285714, 'no_speech_prob': 8.329062984557822e-05}, {'id': 184, 'seek': 141664, 'start': 1416.64, 'end': 1424.0800000000002, 'text': \" few examples of different ambiguities that you find in natural language and I've got some funny\", 'tokens': [1326, 5110, 295, 819, 40390, 1088, 300, 291, 915, 294, 3303, 2856, 293, 286, 600, 658, 512, 4074], 'temperature': 0.0, 'avg_logprob': -0.08635699162717725, 'compression_ratio': 1.695906432748538, 'no_speech_prob': 4.277142943465151e-05}, {'id': 185, 'seek': 141664, 'start': 1424.0800000000002, 'end': 1432.88, 'text': ' examples from newspaper headlines but these are all real natural language ambiguities that you find', 'tokens': [5110, 490, 13669, 23867, 457, 613, 366, 439, 957, 3303, 2856, 40390, 1088, 300, 291, 915], 'temperature': 0.0, 'avg_logprob': -0.08635699162717725, 'compression_ratio': 1.695906432748538, 'no_speech_prob': 4.277142943465151e-05}, {'id': 186, 'seek': 141664, 'start': 1432.88, 'end': 1439.76, 'text': \" throughout natural language. Well at this point I should say this is where I'm being guilty of\", 'tokens': [3710, 3303, 2856, 13, 1042, 412, 341, 935, 286, 820, 584, 341, 307, 689, 286, 478, 885, 12341, 295], 'temperature': 0.0, 'avg_logprob': -0.08635699162717725, 'compression_ratio': 1.695906432748538, 'no_speech_prob': 4.277142943465151e-05}, {'id': 187, 'seek': 143976, 'start': 1439.76, 'end': 1447.12, 'text': \" saying natural language but I'm meaning in English. Some of these ambiguities you find in lots of\", 'tokens': [1566, 3303, 2856, 457, 286, 478, 3620, 294, 3669, 13, 2188, 295, 613, 40390, 1088, 291, 915, 294, 3195, 295], 'temperature': 0.0, 'avg_logprob': -0.15134533440194478, 'compression_ratio': 1.8156682027649769, 'no_speech_prob': 2.821725320245605e-05}, {'id': 188, 'seek': 143976, 'start': 1447.12, 'end': 1454.72, 'text': ' other languages as well but which ambiguities that are for syntactic structure partly depend on the', 'tokens': [661, 8650, 382, 731, 457, 597, 40390, 1088, 300, 366, 337, 23980, 19892, 3877, 17031, 5672, 322, 264], 'temperature': 0.0, 'avg_logprob': -0.15134533440194478, 'compression_ratio': 1.8156682027649769, 'no_speech_prob': 2.821725320245605e-05}, {'id': 189, 'seek': 143976, 'start': 1454.72, 'end': 1461.12, 'text': ' details of the language. So different languages have different syntactic constructions, different', 'tokens': [4365, 295, 264, 2856, 13, 407, 819, 8650, 362, 819, 23980, 19892, 7690, 626, 11, 819], 'temperature': 0.0, 'avg_logprob': -0.15134533440194478, 'compression_ratio': 1.8156682027649769, 'no_speech_prob': 2.821725320245605e-05}, {'id': 190, 'seek': 143976, 'start': 1461.12, 'end': 1469.28, 'text': ' word orders, different amounts of words having different forms of words like case markings. And so', 'tokens': [1349, 9470, 11, 819, 11663, 295, 2283, 1419, 819, 6422, 295, 2283, 411, 1389, 39087, 13, 400, 370], 'temperature': 0.0, 'avg_logprob': -0.15134533440194478, 'compression_ratio': 1.8156682027649769, 'no_speech_prob': 2.821725320245605e-05}, {'id': 191, 'seek': 146928, 'start': 1469.28, 'end': 1477.12, 'text': \" depending on those details there might be different ambiguities. So here's one ambiguity which is\", 'tokens': [5413, 322, 729, 4365, 456, 1062, 312, 819, 40390, 1088, 13, 407, 510, 311, 472, 46519, 597, 307], 'temperature': 0.0, 'avg_logprob': -0.10913032034169072, 'compression_ratio': 1.69364161849711, 'no_speech_prob': 3.068830483243801e-05}, {'id': 192, 'seek': 146928, 'start': 1477.12, 'end': 1485.76, 'text': ' one of the commentest ambiguities in English. So San Jose cops kill man with knife. So this sentence', 'tokens': [472, 295, 264, 2871, 377, 40390, 1088, 294, 3669, 13, 407, 5271, 8635, 19012, 1961, 587, 365, 7976, 13, 407, 341, 8174], 'temperature': 0.0, 'avg_logprob': -0.10913032034169072, 'compression_ratio': 1.69364161849711, 'no_speech_prob': 3.068830483243801e-05}, {'id': 193, 'seek': 146928, 'start': 1485.76, 'end': 1495.6, 'text': \" has two meanings either it's the San Jose cops who are killing a man and they're killing a man\", 'tokens': [575, 732, 28138, 2139, 309, 311, 264, 5271, 8635, 19012, 567, 366, 8011, 257, 587, 293, 436, 434, 8011, 257, 587], 'temperature': 0.0, 'avg_logprob': -0.10913032034169072, 'compression_ratio': 1.69364161849711, 'no_speech_prob': 3.068830483243801e-05}, {'id': 194, 'seek': 149560, 'start': 1495.6, 'end': 1503.76, 'text': ' with a knife. And so that corresponds to a dependency structure where the San Jose cops', 'tokens': [365, 257, 7976, 13, 400, 370, 300, 23249, 281, 257, 33621, 3877, 689, 264, 5271, 8635, 19012], 'temperature': 0.0, 'avg_logprob': -0.08493451029062271, 'compression_ratio': 1.7228915662650603, 'no_speech_prob': 8.336912287632003e-05}, {'id': 195, 'seek': 149560, 'start': 1503.76, 'end': 1513.6799999999998, 'text': ' are the subject of killing the man is the object of killing and then the knife is then the instrument', 'tokens': [366, 264, 3983, 295, 8011, 264, 587, 307, 264, 2657, 295, 8011, 293, 550, 264, 7976, 307, 550, 264, 7198], 'temperature': 0.0, 'avg_logprob': -0.08493451029062271, 'compression_ratio': 1.7228915662650603, 'no_speech_prob': 8.336912287632003e-05}, {'id': 196, 'seek': 149560, 'start': 1513.6799999999998, 'end': 1521.04, 'text': \" with which they're doing the killing so that the knife is an oblique modifier for the instrument\", 'tokens': [365, 597, 436, 434, 884, 264, 8011, 370, 300, 264, 7976, 307, 364, 23740, 1925, 38011, 337, 264, 7198], 'temperature': 0.0, 'avg_logprob': -0.08493451029062271, 'compression_ratio': 1.7228915662650603, 'no_speech_prob': 8.336912287632003e-05}, {'id': 197, 'seek': 152104, 'start': 1521.04, 'end': 1528.48, 'text': \" of killing. And so that's one possible structure for this sentence but it's probably not the right one.\", 'tokens': [295, 8011, 13, 400, 370, 300, 311, 472, 1944, 3877, 337, 341, 8174, 457, 309, 311, 1391, 406, 264, 558, 472, 13], 'temperature': 0.0, 'avg_logprob': -0.07742296809881506, 'compression_ratio': 1.6243093922651934, 'no_speech_prob': 3.070757520617917e-05}, {'id': 198, 'seek': 152104, 'start': 1529.36, 'end': 1537.52, 'text': ' So what it actually probably was was that it was a man with a knife and the San Jose cops killed', 'tokens': [407, 437, 309, 767, 1391, 390, 390, 300, 309, 390, 257, 587, 365, 257, 7976, 293, 264, 5271, 8635, 19012, 4652], 'temperature': 0.0, 'avg_logprob': -0.07742296809881506, 'compression_ratio': 1.6243093922651934, 'no_speech_prob': 3.070757520617917e-05}, {'id': 199, 'seek': 152104, 'start': 1537.52, 'end': 1549.12, 'text': ' the man. So that corresponds to the knife then being a noun modifier of the man and then kill', 'tokens': [264, 587, 13, 407, 300, 23249, 281, 264, 7976, 550, 885, 257, 23307, 38011, 295, 264, 587, 293, 550, 1961], 'temperature': 0.0, 'avg_logprob': -0.07742296809881506, 'compression_ratio': 1.6243093922651934, 'no_speech_prob': 3.070757520617917e-05}, {'id': 200, 'seek': 154912, 'start': 1549.12, 'end': 1555.6799999999998, 'text': ' is still killing the man. So the man is the object of killing and the cops are still the subject.', 'tokens': [307, 920, 8011, 264, 587, 13, 407, 264, 587, 307, 264, 2657, 295, 8011, 293, 264, 19012, 366, 920, 264, 3983, 13], 'temperature': 0.0, 'avg_logprob': -0.07021261989206508, 'compression_ratio': 1.5888888888888888, 'no_speech_prob': 9.16078279260546e-05}, {'id': 201, 'seek': 154912, 'start': 1557.1999999999998, 'end': 1565.12, 'text': \" And so whenever you have a prepositional phrase like this that's coming further on in a sentence\", 'tokens': [400, 370, 5699, 291, 362, 257, 2666, 329, 2628, 9535, 411, 341, 300, 311, 1348, 3052, 322, 294, 257, 8174], 'temperature': 0.0, 'avg_logprob': -0.07021261989206508, 'compression_ratio': 1.5888888888888888, 'no_speech_prob': 9.16078279260546e-05}, {'id': 202, 'seek': 154912, 'start': 1566.0, 'end': 1573.36, 'text': \" there's a choice of how to interpret it. It could be either interpreted as modifying a noun\", 'tokens': [456, 311, 257, 3922, 295, 577, 281, 7302, 309, 13, 467, 727, 312, 2139, 26749, 382, 42626, 257, 23307], 'temperature': 0.0, 'avg_logprob': -0.07021261989206508, 'compression_ratio': 1.5888888888888888, 'no_speech_prob': 9.16078279260546e-05}, {'id': 203, 'seek': 157336, 'start': 1573.36, 'end': 1580.24, 'text': ' phrase that comes before it or it can be interpreted as modifying a verb that comes before it.', 'tokens': [9535, 300, 1487, 949, 309, 420, 309, 393, 312, 26749, 382, 42626, 257, 9595, 300, 1487, 949, 309, 13], 'temperature': 0.0, 'avg_logprob': -0.10524536823404246, 'compression_ratio': 1.5593220338983051, 'no_speech_prob': 4.752725726575591e-05}, {'id': 204, 'seek': 157336, 'start': 1580.24, 'end': 1586.56, 'text': ' So systematically in English you get these prepositional phrase attachment ambiguities', 'tokens': [407, 39531, 294, 3669, 291, 483, 613, 2666, 329, 2628, 9535, 19431, 40390, 1088], 'temperature': 0.0, 'avg_logprob': -0.10524536823404246, 'compression_ratio': 1.5593220338983051, 'no_speech_prob': 4.752725726575591e-05}, {'id': 205, 'seek': 157336, 'start': 1586.56, 'end': 1594.0, 'text': ' throughout all of our sentences but you know to give two further observations on that you know', 'tokens': [3710, 439, 295, 527, 16579, 457, 291, 458, 281, 976, 732, 3052, 18163, 322, 300, 291, 458], 'temperature': 0.0, 'avg_logprob': -0.10524536823404246, 'compression_ratio': 1.5593220338983051, 'no_speech_prob': 4.752725726575591e-05}, {'id': 206, 'seek': 159400, 'start': 1594.0, 'end': 1603.28, 'text': ' the first observation is you know you encounter sentences with prepositional phrase attachment', 'tokens': [264, 700, 14816, 307, 291, 458, 291, 8593, 16579, 365, 2666, 329, 2628, 9535, 19431], 'temperature': 0.0, 'avg_logprob': -0.07780357134544243, 'compression_ratio': 1.6043956043956045, 'no_speech_prob': 1.5929806977510452e-05}, {'id': 207, 'seek': 159400, 'start': 1604.56, 'end': 1611.2, 'text': ' ambiguities every time you read a newspaper article every time you talk to somebody but most of', 'tokens': [40390, 1088, 633, 565, 291, 1401, 257, 13669, 7222, 633, 565, 291, 751, 281, 2618, 457, 881, 295], 'temperature': 0.0, 'avg_logprob': -0.07780357134544243, 'compression_ratio': 1.6043956043956045, 'no_speech_prob': 1.5929806977510452e-05}, {'id': 208, 'seek': 159400, 'start': 1611.2, 'end': 1618.96, 'text': \" the time you never notice them and that's because our human brains are incredibly good at considering\", 'tokens': [264, 565, 291, 1128, 3449, 552, 293, 300, 311, 570, 527, 1952, 15442, 366, 6252, 665, 412, 8079], 'temperature': 0.0, 'avg_logprob': -0.07780357134544243, 'compression_ratio': 1.6043956043956045, 'no_speech_prob': 1.5929806977510452e-05}, {'id': 209, 'seek': 161896, 'start': 1618.96, 'end': 1625.28, 'text': ' the possible interpretations and going with the one that makes sense according to context.', 'tokens': [264, 1944, 37547, 293, 516, 365, 264, 472, 300, 1669, 2020, 4650, 281, 4319, 13], 'temperature': 0.0, 'avg_logprob': -0.10808445311881401, 'compression_ratio': 1.6334841628959276, 'no_speech_prob': 2.880938700400293e-05}, {'id': 210, 'seek': 161896, 'start': 1627.44, 'end': 1634.96, 'text': ' The second comment as I said different human languages expose different ambiguities. So for', 'tokens': [440, 1150, 2871, 382, 286, 848, 819, 1952, 8650, 19219, 819, 40390, 1088, 13, 407, 337], 'temperature': 0.0, 'avg_logprob': -0.10808445311881401, 'compression_ratio': 1.6334841628959276, 'no_speech_prob': 2.880938700400293e-05}, {'id': 211, 'seek': 161896, 'start': 1634.96, 'end': 1640.88, 'text': \" example this is an ambiguity that you normally don't get in Chinese because in Chinese\", 'tokens': [1365, 341, 307, 364, 46519, 300, 291, 5646, 500, 380, 483, 294, 4649, 570, 294, 4649], 'temperature': 0.0, 'avg_logprob': -0.10808445311881401, 'compression_ratio': 1.6334841628959276, 'no_speech_prob': 2.880938700400293e-05}, {'id': 212, 'seek': 161896, 'start': 1641.76, 'end': 1648.48, 'text': ' prepositional phrases modifying a verb are normally placed before the verb and so there you', 'tokens': [2666, 329, 2628, 20312, 42626, 257, 9595, 366, 5646, 7074, 949, 264, 9595, 293, 370, 456, 291], 'temperature': 0.0, 'avg_logprob': -0.10808445311881401, 'compression_ratio': 1.6334841628959276, 'no_speech_prob': 2.880938700400293e-05}, {'id': 213, 'seek': 164848, 'start': 1648.48, 'end': 1655.76, 'text': \" don't standedly get this ambiguity but you know there are different other ambiguities that you find\", 'tokens': [500, 380, 1463, 13516, 483, 341, 46519, 457, 291, 458, 456, 366, 819, 661, 40390, 1088, 300, 291, 915], 'temperature': 0.0, 'avg_logprob': -0.11099897286830804, 'compression_ratio': 1.7276785714285714, 'no_speech_prob': 5.048827006248757e-05}, {'id': 214, 'seek': 164848, 'start': 1655.76, 'end': 1663.2, 'text': ' commonly in Chinese sentences. Okay so this ambiguity you find everywhere because prepositional', 'tokens': [12719, 294, 4649, 16579, 13, 1033, 370, 341, 46519, 291, 915, 5315, 570, 2666, 329, 2628], 'temperature': 0.0, 'avg_logprob': -0.11099897286830804, 'compression_ratio': 1.7276785714285714, 'no_speech_prob': 5.048827006248757e-05}, {'id': 215, 'seek': 164848, 'start': 1663.2, 'end': 1669.52, 'text': \" phrases are really common at the right ends of sentences so here's another one scientist count\", 'tokens': [20312, 366, 534, 2689, 412, 264, 558, 5314, 295, 16579, 370, 510, 311, 1071, 472, 12662, 1207], 'temperature': 0.0, 'avg_logprob': -0.11099897286830804, 'compression_ratio': 1.7276785714285714, 'no_speech_prob': 5.048827006248757e-05}, {'id': 216, 'seek': 164848, 'start': 1669.52, 'end': 1676.32, 'text': ' whales from space so that gives us these two possible interpretations that there are whales from', 'tokens': [32403, 490, 1901, 370, 300, 2709, 505, 613, 732, 1944, 37547, 300, 456, 366, 32403, 490], 'temperature': 0.0, 'avg_logprob': -0.11099897286830804, 'compression_ratio': 1.7276785714285714, 'no_speech_prob': 5.048827006248757e-05}, {'id': 217, 'seek': 167632, 'start': 1676.32, 'end': 1684.72, 'text': ' space and scientists accounting them and then the other one is how the scientists accounting the', 'tokens': [1901, 293, 7708, 19163, 552, 293, 550, 264, 661, 472, 307, 577, 264, 7708, 19163, 264], 'temperature': 0.0, 'avg_logprob': -0.10574553694043841, 'compression_ratio': 1.8513513513513513, 'no_speech_prob': 5.0573464250192046e-05}, {'id': 218, 'seek': 167632, 'start': 1684.72, 'end': 1691.04, 'text': \" whales is that they're counting them from space and they're using satellites to count the\", 'tokens': [32403, 307, 300, 436, 434, 13251, 552, 490, 1901, 293, 436, 434, 1228, 24960, 281, 1207, 264], 'temperature': 0.0, 'avg_logprob': -0.10574553694043841, 'compression_ratio': 1.8513513513513513, 'no_speech_prob': 5.0573464250192046e-05}, {'id': 219, 'seek': 167632, 'start': 1691.04, 'end': 1698.1599999999999, 'text': \" sales which is the correct interpretation that the newspaper hopes that you're getting.\", 'tokens': [5763, 597, 307, 264, 3006, 14174, 300, 264, 13669, 13681, 300, 291, 434, 1242, 13], 'temperature': 0.0, 'avg_logprob': -0.10574553694043841, 'compression_ratio': 1.8513513513513513, 'no_speech_prob': 5.0573464250192046e-05}, {'id': 220, 'seek': 169816, 'start': 1698.16, 'end': 1710.72, 'text': ' And this problem gets much much more complex because many sentences in English have prepositional', 'tokens': [400, 341, 1154, 2170, 709, 709, 544, 3997, 570, 867, 16579, 294, 3669, 362, 2666, 329, 2628], 'temperature': 0.0, 'avg_logprob': -0.1237863540649414, 'compression_ratio': 1.4522613065326633, 'no_speech_prob': 0.00010859352914849296}, {'id': 221, 'seek': 169816, 'start': 1710.72, 'end': 1717.76, 'text': \" phrases all over the place so here's the kind of boring sentence that you find in the financial\", 'tokens': [20312, 439, 670, 264, 1081, 370, 510, 311, 264, 733, 295, 9989, 8174, 300, 291, 915, 294, 264, 4669], 'temperature': 0.0, 'avg_logprob': -0.1237863540649414, 'compression_ratio': 1.4522613065326633, 'no_speech_prob': 0.00010859352914849296}, {'id': 222, 'seek': 169816, 'start': 1717.76, 'end': 1725.76, 'text': ' news the board approved its acquisition by Royal Trust Co Ltd of Toronto for $27 a share at its', 'tokens': [2583, 264, 3150, 10826, 1080, 21668, 538, 12717, 11580, 3066, 44451, 67, 295, 14140, 337, 1848, 10076, 257, 2073, 412, 1080], 'temperature': 0.0, 'avg_logprob': -0.1237863540649414, 'compression_ratio': 1.4522613065326633, 'no_speech_prob': 0.00010859352914849296}, {'id': 223, 'seek': 172576, 'start': 1725.76, 'end': 1731.6, 'text': ' monthly meeting and while if you look at the structure of this sentence what we find is you know', 'tokens': [12878, 3440, 293, 1339, 498, 291, 574, 412, 264, 3877, 295, 341, 8174, 437, 321, 915, 307, 291, 458], 'temperature': 0.0, 'avg_logprob': -0.08157821761237251, 'compression_ratio': 2.161111111111111, 'no_speech_prob': 5.219029480940662e-05}, {'id': 224, 'seek': 172576, 'start': 1731.6, 'end': 1741.6, 'text': \" here's a verb then here's the object noun phrase so we've got the object noun phrase here and then\", 'tokens': [510, 311, 257, 9595, 550, 510, 311, 264, 2657, 23307, 9535, 370, 321, 600, 658, 264, 2657, 23307, 9535, 510, 293, 550], 'temperature': 0.0, 'avg_logprob': -0.08157821761237251, 'compression_ratio': 2.161111111111111, 'no_speech_prob': 5.219029480940662e-05}, {'id': 225, 'seek': 172576, 'start': 1741.6, 'end': 1748.32, 'text': ' after that what do we find well we find a prepositional phrase another prepositional phrase another', 'tokens': [934, 300, 437, 360, 321, 915, 731, 321, 915, 257, 2666, 329, 2628, 9535, 1071, 2666, 329, 2628, 9535, 1071], 'temperature': 0.0, 'avg_logprob': -0.08157821761237251, 'compression_ratio': 2.161111111111111, 'no_speech_prob': 5.219029480940662e-05}, {'id': 226, 'seek': 172576, 'start': 1748.32, 'end': 1755.2, 'text': ' prepositional phrase and another prepositional phrase and how to attach each of these is then', 'tokens': [2666, 329, 2628, 9535, 293, 1071, 2666, 329, 2628, 9535, 293, 577, 281, 5085, 1184, 295, 613, 307, 550], 'temperature': 0.0, 'avg_logprob': -0.08157821761237251, 'compression_ratio': 2.161111111111111, 'no_speech_prob': 5.219029480940662e-05}, {'id': 227, 'seek': 175520, 'start': 1755.2, 'end': 1762.4, 'text': ' ambiguous so the basic rule of how you can attach them is you can attach them to things to the left', 'tokens': [39465, 370, 264, 3875, 4978, 295, 577, 291, 393, 5085, 552, 307, 291, 393, 5085, 552, 281, 721, 281, 264, 1411], 'temperature': 0.0, 'avg_logprob': -0.10166592231163611, 'compression_ratio': 1.7195121951219512, 'no_speech_prob': 2.2087777324486524e-05}, {'id': 228, 'seek': 175520, 'start': 1763.1200000000001, 'end': 1771.2, 'text': \" providing you don't create crossing attachments so in principle by Royal Trust Co Ltd\", 'tokens': [6530, 291, 500, 380, 1884, 14712, 37987, 370, 294, 8665, 538, 12717, 11580, 3066, 44451, 67], 'temperature': 0.0, 'avg_logprob': -0.10166592231163611, 'compression_ratio': 1.7195121951219512, 'no_speech_prob': 2.2087777324486524e-05}, {'id': 229, 'seek': 175520, 'start': 1771.2, 'end': 1779.1200000000001, 'text': ' could be attached to either approved or acquisition but in this case by Royal Trust Co Ltd it is', 'tokens': [727, 312, 8570, 281, 2139, 10826, 420, 21668, 457, 294, 341, 1389, 538, 12717, 11580, 3066, 44451, 67, 309, 307], 'temperature': 0.0, 'avg_logprob': -0.10166592231163611, 'compression_ratio': 1.7195121951219512, 'no_speech_prob': 2.2087777324486524e-05}, {'id': 230, 'seek': 177912, 'start': 1779.12, 'end': 1792.3999999999999, 'text': \" the acquirer so it's a modifier of the acquisition okay so then we have of Toronto so of Toronto\", 'tokens': [264, 6667, 347, 260, 370, 309, 311, 257, 38011, 295, 264, 21668, 1392, 370, 550, 321, 362, 295, 14140, 370, 295, 14140], 'temperature': 0.0, 'avg_logprob': -0.0843669976761092, 'compression_ratio': 1.8152866242038217, 'no_speech_prob': 1.3376469723880291e-05}, {'id': 231, 'seek': 177912, 'start': 1792.3999999999999, 'end': 1799.1999999999998, 'text': ' could be modifying Royal Trust Co Ltd it could be modifying the acquisition or it can be modifying', 'tokens': [727, 312, 42626, 12717, 11580, 3066, 44451, 67, 309, 727, 312, 42626, 264, 21668, 420, 309, 393, 312, 42626], 'temperature': 0.0, 'avg_logprob': -0.0843669976761092, 'compression_ratio': 1.8152866242038217, 'no_speech_prob': 1.3376469723880291e-05}, {'id': 232, 'seek': 177912, 'start': 1799.1999999999998, 'end': 1806.1599999999999, 'text': ' the approved and in this case the of Toronto is telling you more about the company and so', 'tokens': [264, 10826, 293, 294, 341, 1389, 264, 295, 14140, 307, 3585, 291, 544, 466, 264, 2237, 293, 370], 'temperature': 0.0, 'avg_logprob': -0.0843669976761092, 'compression_ratio': 1.8152866242038217, 'no_speech_prob': 1.3376469723880291e-05}, {'id': 233, 'seek': 180616, 'start': 1806.16, 'end': 1815.28, 'text': \" it's a modifier of Royal Trust Co Ltd okay so then the next one is for $27 a share and that could\", 'tokens': [309, 311, 257, 38011, 295, 12717, 11580, 3066, 44451, 67, 1392, 370, 550, 264, 958, 472, 307, 337, 1848, 10076, 257, 2073, 293, 300, 727], 'temperature': 0.0, 'avg_logprob': -0.1010148623218275, 'compression_ratio': 1.598901098901099, 'no_speech_prob': 1.8906885088654235e-05}, {'id': 234, 'seek': 180616, 'start': 1815.28, 'end': 1823.8400000000001, 'text': ' be modifying Toronto Royal Trust Co Ltd the acquisition or the approving and well in this case', 'tokens': [312, 42626, 14140, 12717, 11580, 3066, 44451, 67, 264, 21668, 420, 264, 2075, 798, 293, 731, 294, 341, 1389], 'temperature': 0.0, 'avg_logprob': -0.1010148623218275, 'compression_ratio': 1.598901098901099, 'no_speech_prob': 1.8906885088654235e-05}, {'id': 235, 'seek': 180616, 'start': 1825.1200000000001, 'end': 1832.24, 'text': \" that's talking about the price of the acquisition so this one is mod go jumps back and this is now\", 'tokens': [300, 311, 1417, 466, 264, 3218, 295, 264, 21668, 370, 341, 472, 307, 1072, 352, 16704, 646, 293, 341, 307, 586], 'temperature': 0.0, 'avg_logprob': -0.1010148623218275, 'compression_ratio': 1.598901098901099, 'no_speech_prob': 1.8906885088654235e-05}, {'id': 236, 'seek': 183224, 'start': 1832.24, 'end': 1841.52, 'text': \" prepositional phrase that's modifying the acquisition and then at the end at its monthly meeting\", 'tokens': [2666, 329, 2628, 9535, 300, 311, 42626, 264, 21668, 293, 550, 412, 264, 917, 412, 1080, 12878, 3440], 'temperature': 0.0, 'avg_logprob': -0.05785158423126721, 'compression_ratio': 1.8581081081081081, 'no_speech_prob': 5.605971091426909e-05}, {'id': 237, 'seek': 183224, 'start': 1842.48, 'end': 1849.6, 'text': \" well that's where the approval is happening by the by the board so rather than any of these\", 'tokens': [731, 300, 311, 689, 264, 13317, 307, 2737, 538, 264, 538, 264, 3150, 370, 2831, 813, 604, 295, 613], 'temperature': 0.0, 'avg_logprob': -0.05785158423126721, 'compression_ratio': 1.8581081081081081, 'no_speech_prob': 5.605971091426909e-05}, {'id': 238, 'seek': 183224, 'start': 1849.6, 'end': 1858.72, 'text': ' preceding four noun phrases at its monthly meeting is modifying the approval and so it', 'tokens': [16969, 278, 1451, 23307, 20312, 412, 1080, 12878, 3440, 307, 42626, 264, 13317, 293, 370, 309], 'temperature': 0.0, 'avg_logprob': -0.05785158423126721, 'compression_ratio': 1.8581081081081081, 'no_speech_prob': 5.605971091426909e-05}, {'id': 239, 'seek': 185872, 'start': 1858.72, 'end': 1865.76, 'text': \" attaches right back there and this example is kind of too big and so I couldn't fit it in one line\", 'tokens': [49404, 558, 646, 456, 293, 341, 1365, 307, 733, 295, 886, 955, 293, 370, 286, 2809, 380, 3318, 309, 294, 472, 1622], 'temperature': 0.0, 'avg_logprob': -0.09070166047797146, 'compression_ratio': 1.6885964912280702, 'no_speech_prob': 7.463000656571239e-05}, {'id': 240, 'seek': 185872, 'start': 1865.76, 'end': 1871.92, 'text': ' but as I think maybe you can see that you know none of these dependencies cross each other', 'tokens': [457, 382, 286, 519, 1310, 291, 393, 536, 300, 291, 458, 6022, 295, 613, 36606, 3278, 1184, 661], 'temperature': 0.0, 'avg_logprob': -0.09070166047797146, 'compression_ratio': 1.6885964912280702, 'no_speech_prob': 7.463000656571239e-05}, {'id': 241, 'seek': 185872, 'start': 1871.92, 'end': 1879.28, 'text': ' and they connect at different places ambiguously so because we can chain these prepositions like this', 'tokens': [293, 436, 1745, 412, 819, 3190, 40390, 5098, 370, 570, 321, 393, 5021, 613, 2666, 329, 2451, 411, 341], 'temperature': 0.0, 'avg_logprob': -0.09070166047797146, 'compression_ratio': 1.6885964912280702, 'no_speech_prob': 7.463000656571239e-05}, {'id': 242, 'seek': 185872, 'start': 1879.28, 'end': 1886.72, 'text': ' and attach them at different places like this human language sentences are actually extremely', 'tokens': [293, 5085, 552, 412, 819, 3190, 411, 341, 1952, 2856, 16579, 366, 767, 4664], 'temperature': 0.0, 'avg_logprob': -0.09070166047797146, 'compression_ratio': 1.6885964912280702, 'no_speech_prob': 7.463000656571239e-05}, {'id': 243, 'seek': 188672, 'start': 1886.72, 'end': 1897.92, 'text': ' ambiguous so the number if you have a sentence with K prepositional phrases at the end of', 'tokens': [39465, 370, 264, 1230, 498, 291, 362, 257, 8174, 365, 591, 2666, 329, 2628, 20312, 412, 264, 917, 295], 'temperature': 0.0, 'avg_logprob': -0.1384585698445638, 'compression_ratio': 1.7423312883435582, 'no_speech_prob': 2.3141910787671804e-05}, {'id': 244, 'seek': 188672, 'start': 1897.92, 'end': 1904.96, 'text': ' earth where here we have K equals four the number of parses this sentence has the number of different', 'tokens': [4120, 689, 510, 321, 362, 591, 6915, 1451, 264, 1230, 295, 21156, 279, 341, 8174, 575, 264, 1230, 295, 819], 'temperature': 0.0, 'avg_logprob': -0.1384585698445638, 'compression_ratio': 1.7423312883435582, 'no_speech_prob': 2.3141910787671804e-05}, {'id': 245, 'seek': 188672, 'start': 1904.96, 'end': 1910.96, 'text': ' ways you can make these attachments is given by the cutler numbers so the cutler numbers are', 'tokens': [2098, 291, 393, 652, 613, 37987, 307, 2212, 538, 264, 1723, 1918, 3547, 370, 264, 1723, 1918, 3547, 366], 'temperature': 0.0, 'avg_logprob': -0.1384585698445638, 'compression_ratio': 1.7423312883435582, 'no_speech_prob': 2.3141910787671804e-05}, {'id': 246, 'seek': 191096, 'start': 1910.96, 'end': 1918.0, 'text': \" an exponentially growing series which arises in many tree like context so if you're doing something\", 'tokens': [364, 37330, 4194, 2638, 597, 27388, 294, 867, 4230, 411, 4319, 370, 498, 291, 434, 884, 746], 'temperature': 0.0, 'avg_logprob': -0.08949273891663284, 'compression_ratio': 1.8108108108108107, 'no_speech_prob': 2.1003805159125477e-05}, {'id': 247, 'seek': 191096, 'start': 1918.0, 'end': 1924.8, 'text': \" like triangulations of a polygon you get cutler numbers if you're doing triangulation and graphical\", 'tokens': [411, 19335, 4136, 295, 257, 48242, 291, 483, 1723, 1918, 3547, 498, 291, 434, 884, 19335, 2776, 293, 35942], 'temperature': 0.0, 'avg_logprob': -0.08949273891663284, 'compression_ratio': 1.8108108108108107, 'no_speech_prob': 2.1003805159125477e-05}, {'id': 248, 'seek': 191096, 'start': 1924.8, 'end': 1931.28, 'text': \" models in CS228 you get cutler numbers but we don't need to worry about the details here the central\", 'tokens': [5245, 294, 9460, 7490, 23, 291, 483, 1723, 1918, 3547, 457, 321, 500, 380, 643, 281, 3292, 466, 264, 4365, 510, 264, 5777], 'temperature': 0.0, 'avg_logprob': -0.08949273891663284, 'compression_ratio': 1.8108108108108107, 'no_speech_prob': 2.1003805159125477e-05}, {'id': 249, 'seek': 191096, 'start': 1931.28, 'end': 1938.0, 'text': \" point is this is an exponential series and so you're getting an exponential number of parses in terms\", 'tokens': [935, 307, 341, 307, 364, 21510, 2638, 293, 370, 291, 434, 1242, 364, 21510, 1230, 295, 21156, 279, 294, 2115], 'temperature': 0.0, 'avg_logprob': -0.08949273891663284, 'compression_ratio': 1.8108108108108107, 'no_speech_prob': 2.1003805159125477e-05}, {'id': 250, 'seek': 193800, 'start': 1938.0, 'end': 1944.64, 'text': ' of the number of prepositional phrases and so in general you know the number of parses human', 'tokens': [295, 264, 1230, 295, 2666, 329, 2628, 20312, 293, 370, 294, 2674, 291, 458, 264, 1230, 295, 21156, 279, 1952], 'temperature': 0.0, 'avg_logprob': -0.0567961401409573, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 2.390066765656229e-05}, {'id': 251, 'seek': 193800, 'start': 1944.64, 'end': 1951.84, 'text': \" languages have is exponential in their length which is kind of bad news because if you're then trying\", 'tokens': [8650, 362, 307, 21510, 294, 641, 4641, 597, 307, 733, 295, 1578, 2583, 570, 498, 291, 434, 550, 1382], 'temperature': 0.0, 'avg_logprob': -0.0567961401409573, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 2.390066765656229e-05}, {'id': 252, 'seek': 193800, 'start': 1951.84, 'end': 1959.28, 'text': ' to enumerate all the parses it you might fear that you really have to do a ton of work the thing to', 'tokens': [281, 465, 15583, 473, 439, 264, 21156, 279, 309, 291, 1062, 4240, 300, 291, 534, 362, 281, 360, 257, 2952, 295, 589, 264, 551, 281], 'temperature': 0.0, 'avg_logprob': -0.0567961401409573, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 2.390066765656229e-05}, {'id': 253, 'seek': 195928, 'start': 1959.28, 'end': 1968.24, 'text': \" notice about structures like these prepositional phrase attachment ambiguities is that there's nothing\", 'tokens': [3449, 466, 9227, 411, 613, 2666, 329, 2628, 9535, 19431, 40390, 1088, 307, 300, 456, 311, 1825], 'temperature': 0.0, 'avg_logprob': -0.05496533711751302, 'compression_ratio': 1.8650306748466257, 'no_speech_prob': 1.6672878700774163e-05}, {'id': 254, 'seek': 195928, 'start': 1968.24, 'end': 1977.04, 'text': \" that resolves these ambiguities in terms of the structure of the sentence so if you've done something\", 'tokens': [300, 7923, 977, 613, 40390, 1088, 294, 2115, 295, 264, 3877, 295, 264, 8174, 370, 498, 291, 600, 1096, 746], 'temperature': 0.0, 'avg_logprob': -0.05496533711751302, 'compression_ratio': 1.8650306748466257, 'no_speech_prob': 1.6672878700774163e-05}, {'id': 255, 'seek': 195928, 'start': 1977.04, 'end': 1983.84, 'text': ' like looked at the kind of grammars that are used in compilers that the grammars used in compilings', 'tokens': [411, 2956, 412, 264, 733, 295, 17570, 685, 300, 366, 1143, 294, 715, 388, 433, 300, 264, 17570, 685, 1143, 294, 715, 388, 1109], 'temperature': 0.0, 'avg_logprob': -0.05496533711751302, 'compression_ratio': 1.8650306748466257, 'no_speech_prob': 1.6672878700774163e-05}, {'id': 256, 'seek': 198384, 'start': 1983.84, 'end': 1991.04, 'text': ' and compilers for programming languages are mainly made to be unambiguous and to the extent that', 'tokens': [293, 715, 388, 433, 337, 9410, 8650, 366, 8704, 1027, 281, 312, 517, 2173, 30525, 293, 281, 264, 8396, 300], 'temperature': 0.0, 'avg_logprob': -0.09329340193006727, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 2.176308407797478e-05}, {'id': 257, 'seek': 198384, 'start': 1991.04, 'end': 1998.56, 'text': ' there are any ambiguities there are default rules that are used to say choose this one particular', 'tokens': [456, 366, 604, 40390, 1088, 456, 366, 7576, 4474, 300, 366, 1143, 281, 584, 2826, 341, 472, 1729], 'temperature': 0.0, 'avg_logprob': -0.09329340193006727, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 2.176308407797478e-05}, {'id': 258, 'seek': 198384, 'start': 1999.4399999999998, 'end': 2007.04, 'text': \" parse tree for your piece of a programming language and human languages just aren't like that\", 'tokens': [48377, 4230, 337, 428, 2522, 295, 257, 9410, 2856, 293, 1952, 8650, 445, 3212, 380, 411, 300], 'temperature': 0.0, 'avg_logprob': -0.09329340193006727, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 2.176308407797478e-05}, {'id': 259, 'seek': 200704, 'start': 2007.04, 'end': 2013.6, 'text': \" they're globally ambiguous and the listening human is just meant to be smart enough to figure out\", 'tokens': [436, 434, 18958, 39465, 293, 264, 4764, 1952, 307, 445, 4140, 281, 312, 4069, 1547, 281, 2573, 484], 'temperature': 0.0, 'avg_logprob': -0.061534684151411057, 'compression_ratio': 1.60989010989011, 'no_speech_prob': 4.8912945203483105e-05}, {'id': 260, 'seek': 200704, 'start': 2013.6, 'end': 2024.24, 'text': \" what was intended so the analogy would be that you know in programming languages when you're working\", 'tokens': [437, 390, 10226, 370, 264, 21663, 576, 312, 300, 291, 458, 294, 9410, 8650, 562, 291, 434, 1364], 'temperature': 0.0, 'avg_logprob': -0.061534684151411057, 'compression_ratio': 1.60989010989011, 'no_speech_prob': 4.8912945203483105e-05}, {'id': 261, 'seek': 200704, 'start': 2024.24, 'end': 2034.3999999999999, 'text': \" out what does an else clause modify well you've got the answer that you can either look at the\", 'tokens': [484, 437, 775, 364, 1646, 25925, 16927, 731, 291, 600, 658, 264, 1867, 300, 291, 393, 2139, 574, 412, 264], 'temperature': 0.0, 'avg_logprob': -0.061534684151411057, 'compression_ratio': 1.60989010989011, 'no_speech_prob': 4.8912945203483105e-05}, {'id': 262, 'seek': 203440, 'start': 2034.4, 'end': 2040.3200000000002, 'text': \" curly braces to work out what the else clause modifies or if you're using Python you look at the\", 'tokens': [32066, 41537, 281, 589, 484, 437, 264, 1646, 25925, 1072, 11221, 420, 498, 291, 434, 1228, 15329, 291, 574, 412, 264], 'temperature': 0.0, 'avg_logprob': -0.06974684397379557, 'compression_ratio': 1.8803827751196172, 'no_speech_prob': 4.588200317812152e-05}, {'id': 263, 'seek': 203440, 'start': 2040.3200000000002, 'end': 2047.68, 'text': ' indentation and it tells you what the else clause modifies where by contrast for human languages', 'tokens': [44494, 399, 293, 309, 5112, 291, 437, 264, 1646, 25925, 1072, 11221, 689, 538, 8712, 337, 1952, 8650], 'temperature': 0.0, 'avg_logprob': -0.06974684397379557, 'compression_ratio': 1.8803827751196172, 'no_speech_prob': 4.588200317812152e-05}, {'id': 264, 'seek': 203440, 'start': 2048.8, 'end': 2056.32, 'text': \" the it would be just write down else something doesn't matter how you do it you don't need parentheses\", 'tokens': [264, 309, 576, 312, 445, 2464, 760, 1646, 746, 1177, 380, 1871, 577, 291, 360, 309, 291, 500, 380, 643, 34153], 'temperature': 0.0, 'avg_logprob': -0.06974684397379557, 'compression_ratio': 1.8803827751196172, 'no_speech_prob': 4.588200317812152e-05}, {'id': 265, 'seek': 203440, 'start': 2056.32, 'end': 2061.6800000000003, 'text': \" you don't need indentation the human being will just figure out what the else clause is meant to\", 'tokens': [291, 500, 380, 643, 44494, 399, 264, 1952, 885, 486, 445, 2573, 484, 437, 264, 1646, 25925, 307, 4140, 281], 'temperature': 0.0, 'avg_logprob': -0.06974684397379557, 'compression_ratio': 1.8803827751196172, 'no_speech_prob': 4.588200317812152e-05}, {'id': 266, 'seek': 206168, 'start': 2061.68, 'end': 2071.6, 'text': \" pair up with okay lots of other forms of ambiguities in human languages so let's look at a few others\", 'tokens': [6119, 493, 365, 1392, 3195, 295, 661, 6422, 295, 40390, 1088, 294, 1952, 8650, 370, 718, 311, 574, 412, 257, 1326, 2357], 'temperature': 0.0, 'avg_logprob': -0.15919474949912418, 'compression_ratio': 1.6077348066298343, 'no_speech_prob': 9.122304618358612e-05}, {'id': 267, 'seek': 206168, 'start': 2071.6, 'end': 2078.0, 'text': ' another one that is very common over all sorts of languages is coordination scope ambiguities', 'tokens': [1071, 472, 300, 307, 588, 2689, 670, 439, 7527, 295, 8650, 307, 21252, 11923, 40390, 1088], 'temperature': 0.0, 'avg_logprob': -0.15919474949912418, 'compression_ratio': 1.6077348066298343, 'no_speech_prob': 9.122304618358612e-05}, {'id': 268, 'seek': 206168, 'start': 2079.04, 'end': 2084.72, 'text': \" so here's a sentence shuttle veteran and long time that's your executive Fred Gregory appointed\", 'tokens': [370, 510, 311, 257, 8174, 26728, 18324, 293, 938, 565, 300, 311, 428, 10140, 10112, 37915, 17653], 'temperature': 0.0, 'avg_logprob': -0.15919474949912418, 'compression_ratio': 1.6077348066298343, 'no_speech_prob': 9.122304618358612e-05}, {'id': 269, 'seek': 208472, 'start': 2084.72, 'end': 2093.7599999999998, 'text': ' to board well this is an ambiguous sentence there are two possible readings of this one reading', 'tokens': [281, 3150, 731, 341, 307, 364, 39465, 8174, 456, 366, 732, 1944, 27319, 295, 341, 472, 3760], 'temperature': 0.0, 'avg_logprob': -0.048430542792043375, 'compression_ratio': 1.7560975609756098, 'no_speech_prob': 4.0464095945935696e-05}, {'id': 270, 'seek': 208472, 'start': 2093.7599999999998, 'end': 2099.12, 'text': \" is that there are two people there's a shuttle veteran and there's a long time that's your\", 'tokens': [307, 300, 456, 366, 732, 561, 456, 311, 257, 26728, 18324, 293, 456, 311, 257, 938, 565, 300, 311, 428], 'temperature': 0.0, 'avg_logprob': -0.048430542792043375, 'compression_ratio': 1.7560975609756098, 'no_speech_prob': 4.0464095945935696e-05}, {'id': 271, 'seek': 208472, 'start': 2099.12, 'end': 2108.3999999999996, 'text': ' executive Fred Gregory and they were both appointed to the board two people and the other possibility', 'tokens': [10140, 10112, 37915, 293, 436, 645, 1293, 17653, 281, 264, 3150, 732, 561, 293, 264, 661, 7959], 'temperature': 0.0, 'avg_logprob': -0.048430542792043375, 'compression_ratio': 1.7560975609756098, 'no_speech_prob': 4.0464095945935696e-05}, {'id': 272, 'seek': 210840, 'start': 2108.4, 'end': 2117.04, 'text': \" is there's someone named Fred Gregory who's a shuttle veteran and long time that's your executive\", 'tokens': [307, 456, 311, 1580, 4926, 10112, 37915, 567, 311, 257, 26728, 18324, 293, 938, 565, 300, 311, 428, 10140], 'temperature': 0.0, 'avg_logprob': -0.08084781964619954, 'compression_ratio': 1.6065573770491803, 'no_speech_prob': 2.5362851374666207e-05}, {'id': 273, 'seek': 210840, 'start': 2117.84, 'end': 2126.48, 'text': \" and they're appointed to the verb one person and these two interpretations again correspond to having\", 'tokens': [293, 436, 434, 17653, 281, 264, 9595, 472, 954, 293, 613, 732, 37547, 797, 6805, 281, 1419], 'temperature': 0.0, 'avg_logprob': -0.08084781964619954, 'compression_ratio': 1.6065573770491803, 'no_speech_prob': 2.5362851374666207e-05}, {'id': 274, 'seek': 210840, 'start': 2126.48, 'end': 2136.08, 'text': \" different paths structures so in one structure we've got a coordination of the shuttle veteran\", 'tokens': [819, 14518, 9227, 370, 294, 472, 3877, 321, 600, 658, 257, 21252, 295, 264, 26728, 18324], 'temperature': 0.0, 'avg_logprob': -0.08084781964619954, 'compression_ratio': 1.6065573770491803, 'no_speech_prob': 2.5362851374666207e-05}, {'id': 275, 'seek': 213608, 'start': 2136.08, 'end': 2143.7599999999998, 'text': \" and the long time that's your executive Fred Gregory coordinated together in one case these\", 'tokens': [293, 264, 938, 565, 300, 311, 428, 10140, 10112, 37915, 29591, 1214, 294, 472, 1389, 613], 'temperature': 0.0, 'avg_logprob': -0.11922464673481291, 'compression_ratio': 1.775, 'no_speech_prob': 3.8765534554841e-05}, {'id': 276, 'seek': 213608, 'start': 2143.7599999999998, 'end': 2153.2799999999997, 'text': \" are coordinated and then Fred Gregory specifies the name of the Nassar executive so it's then\", 'tokens': [366, 29591, 293, 550, 10112, 37915, 1608, 11221, 264, 1315, 295, 264, 426, 640, 289, 10140, 370, 309, 311, 550], 'temperature': 0.0, 'avg_logprob': -0.11922464673481291, 'compression_ratio': 1.775, 'no_speech_prob': 3.8765534554841e-05}, {'id': 277, 'seek': 213608, 'start': 2154.64, 'end': 2161.04, 'text': ' specifying who that executive is where the what in the other one the shuttle veteran and long time', 'tokens': [1608, 5489, 567, 300, 10140, 307, 689, 264, 437, 294, 264, 661, 472, 264, 26728, 18324, 293, 938, 565], 'temperature': 0.0, 'avg_logprob': -0.11922464673481291, 'compression_ratio': 1.775, 'no_speech_prob': 3.8765534554841e-05}, {'id': 278, 'seek': 216104, 'start': 2161.04, 'end': 2169.7599999999998, 'text': ' Nassar executive all together is then something that is a modifier of Fred Gregory', 'tokens': [426, 640, 289, 10140, 439, 1214, 307, 550, 746, 300, 307, 257, 38011, 295, 10112, 37915], 'temperature': 0.0, 'avg_logprob': -0.09352480081411509, 'compression_ratio': 1.7672955974842768, 'no_speech_prob': 1.890679595817346e-05}, {'id': 279, 'seek': 216104, 'start': 2172.88, 'end': 2180.48, 'text': ' okay so one time this is the unit that modifies Fred Gregory in the other one up here just long time', 'tokens': [1392, 370, 472, 565, 341, 307, 264, 4985, 300, 1072, 11221, 10112, 37915, 294, 264, 661, 472, 493, 510, 445, 938, 565], 'temperature': 0.0, 'avg_logprob': -0.09352480081411509, 'compression_ratio': 1.7672955974842768, 'no_speech_prob': 1.890679595817346e-05}, {'id': 280, 'seek': 216104, 'start': 2180.48, 'end': 2187.12, 'text': \" Nassar executive modifies Fred Gregory and then that's can join together with the shuttle veteran\", 'tokens': [426, 640, 289, 10140, 1072, 11221, 10112, 37915, 293, 550, 300, 311, 393, 3917, 1214, 365, 264, 26728, 18324], 'temperature': 0.0, 'avg_logprob': -0.09352480081411509, 'compression_ratio': 1.7672955974842768, 'no_speech_prob': 1.890679595817346e-05}, {'id': 281, 'seek': 218712, 'start': 2187.12, 'end': 2195.04, 'text': ' and so that also gives different interpretations so this is a slightly reduced example of the', 'tokens': [293, 370, 300, 611, 2709, 819, 37547, 370, 341, 307, 257, 4748, 9212, 1365, 295, 264], 'temperature': 0.0, 'avg_logprob': -0.12028608322143555, 'compression_ratio': 1.546448087431694, 'no_speech_prob': 0.00013303979358170182}, {'id': 282, 'seek': 218712, 'start': 2195.04, 'end': 2204.0, 'text': ' I mean in newspaper headlines tend to be more ambiguous than many other pieces of text because', 'tokens': [286, 914, 294, 13669, 23867, 3928, 281, 312, 544, 39465, 813, 867, 661, 3755, 295, 2487, 570], 'temperature': 0.0, 'avg_logprob': -0.12028608322143555, 'compression_ratio': 1.546448087431694, 'no_speech_prob': 0.00013303979358170182}, {'id': 283, 'seek': 218712, 'start': 2204.0, 'end': 2210.08, 'text': \" they're written in this short and formed get things to fit and this isn't especially short and\", 'tokens': [436, 434, 3720, 294, 341, 2099, 293, 8693, 483, 721, 281, 3318, 293, 341, 1943, 380, 2318, 2099, 293], 'temperature': 0.0, 'avg_logprob': -0.12028608322143555, 'compression_ratio': 1.546448087431694, 'no_speech_prob': 0.00013303979358170182}, {'id': 284, 'seek': 221008, 'start': 2210.08, 'end': 2219.7599999999998, 'text': ' form whereas actually left out in explicit conjunction but this headline says doctor no heart cognitive', 'tokens': [1254, 9735, 767, 1411, 484, 294, 13691, 27482, 457, 341, 28380, 1619, 4631, 572, 1917, 15605], 'temperature': 0.0, 'avg_logprob': -0.1331457942724228, 'compression_ratio': 1.6236559139784945, 'no_speech_prob': 7.785662455717102e-05}, {'id': 285, 'seek': 221008, 'start': 2219.7599999999998, 'end': 2226.72, 'text': \" issues and this was after I guess one of Trump it was after Trump's first physical and while this\", 'tokens': [2663, 293, 341, 390, 934, 286, 2041, 472, 295, 3899, 309, 390, 934, 3899, 311, 700, 4001, 293, 1339, 341], 'temperature': 0.0, 'avg_logprob': -0.1331457942724228, 'compression_ratio': 1.6236559139784945, 'no_speech_prob': 7.785662455717102e-05}, {'id': 286, 'seek': 221008, 'start': 2226.72, 'end': 2232.4, 'text': ' is an ambiguity because there are two ways that you can read this you can either read this as saying', 'tokens': [307, 364, 46519, 570, 456, 366, 732, 2098, 300, 291, 393, 1401, 341, 291, 393, 2139, 1401, 341, 382, 1566], 'temperature': 0.0, 'avg_logprob': -0.1331457942724228, 'compression_ratio': 1.6236559139784945, 'no_speech_prob': 7.785662455717102e-05}, {'id': 287, 'seek': 223240, 'start': 2232.4, 'end': 2242.08, 'text': ' doctor no heart and cognitive issues which gives you one interpretation instead of that the way we', 'tokens': [4631, 572, 1917, 293, 15605, 2663, 597, 2709, 291, 472, 14174, 2602, 295, 300, 264, 636, 321], 'temperature': 0.0, 'avg_logprob': -0.07449343896681263, 'compression_ratio': 1.81875, 'no_speech_prob': 1.199515099870041e-05}, {'id': 288, 'seek': 223240, 'start': 2242.08, 'end': 2252.4, 'text': \" should read it is that it's heart or cognitive and so it's then saying no heart or cognitive issues\", 'tokens': [820, 1401, 309, 307, 300, 309, 311, 1917, 420, 15605, 293, 370, 309, 311, 550, 1566, 572, 1917, 420, 15605, 2663], 'temperature': 0.0, 'avg_logprob': -0.07449343896681263, 'compression_ratio': 1.81875, 'no_speech_prob': 1.199515099870041e-05}, {'id': 289, 'seek': 225240, 'start': 2252.4, 'end': 2261.76, 'text': ' and we have a different narrower scope of the coordination and then we get a different reading.', 'tokens': [293, 321, 362, 257, 819, 46751, 11923, 295, 264, 21252, 293, 550, 321, 483, 257, 819, 3760, 13], 'temperature': 0.0, 'avg_logprob': -0.12604933518629807, 'compression_ratio': 1.6457142857142857, 'no_speech_prob': 5.4679308959748596e-05}, {'id': 290, 'seek': 225240, 'start': 2263.84, 'end': 2271.28, 'text': ' Okay I want to give a couple more examples of different kinds of ambiguities another one you see', 'tokens': [1033, 286, 528, 281, 976, 257, 1916, 544, 5110, 295, 819, 3685, 295, 40390, 1088, 1071, 472, 291, 536], 'temperature': 0.0, 'avg_logprob': -0.12604933518629807, 'compression_ratio': 1.6457142857142857, 'no_speech_prob': 5.4679308959748596e-05}, {'id': 291, 'seek': 225240, 'start': 2271.28, 'end': 2277.76, 'text': ' quite a bit is when you have modifiers that are adjectives and adverbs that there are different', 'tokens': [1596, 257, 857, 307, 562, 291, 362, 1072, 23463, 300, 366, 29378, 1539, 293, 614, 43348, 300, 456, 366, 819], 'temperature': 0.0, 'avg_logprob': -0.12604933518629807, 'compression_ratio': 1.6457142857142857, 'no_speech_prob': 5.4679308959748596e-05}, {'id': 292, 'seek': 227776, 'start': 2277.76, 'end': 2284.2400000000002, 'text': \" ways that you don't have things modifying other things this example is a little bit not safe for\", 'tokens': [2098, 300, 291, 500, 380, 362, 721, 42626, 661, 721, 341, 1365, 307, 257, 707, 857, 406, 3273, 337], 'temperature': 0.0, 'avg_logprob': -0.10117769241333008, 'compression_ratio': 1.6685714285714286, 'no_speech_prob': 4.1214927477994934e-05}, {'id': 293, 'seek': 227776, 'start': 2284.2400000000002, 'end': 2294.6400000000003, 'text': ' work but here goes students get first hand job experience so this is an ambiguous sentence and again', 'tokens': [589, 457, 510, 1709, 1731, 483, 700, 1011, 1691, 1752, 370, 341, 307, 364, 39465, 8174, 293, 797], 'temperature': 0.0, 'avg_logprob': -0.10117769241333008, 'compression_ratio': 1.6685714285714286, 'no_speech_prob': 4.1214927477994934e-05}, {'id': 294, 'seek': 227776, 'start': 2294.6400000000003, 'end': 2302.96, 'text': ' we can think of it as a syntactic ambiguity in terms of which things modify which other things', 'tokens': [321, 393, 519, 295, 309, 382, 257, 23980, 19892, 46519, 294, 2115, 295, 597, 721, 16927, 597, 661, 721], 'temperature': 0.0, 'avg_logprob': -0.10117769241333008, 'compression_ratio': 1.6685714285714286, 'no_speech_prob': 4.1214927477994934e-05}, {'id': 295, 'seek': 230296, 'start': 2302.96, 'end': 2314.96, 'text': \" so the nice polite way to render this sentence is that first is modifying hand so we've got first hand\", 'tokens': [370, 264, 1481, 25171, 636, 281, 15529, 341, 8174, 307, 300, 700, 307, 42626, 1011, 370, 321, 600, 658, 700, 1011], 'temperature': 0.0, 'avg_logprob': -0.1568804211086697, 'compression_ratio': 1.7008547008547008, 'no_speech_prob': 0.0001209194160765037}, {'id': 296, 'seek': 230296, 'start': 2314.96, 'end': 2323.12, 'text': \" it's job experience so job is a compound now modifying experience and it's first hand experience\", 'tokens': [309, 311, 1691, 1752, 370, 1691, 307, 257, 14154, 586, 42626, 1752, 293, 309, 311, 700, 1011, 1752], 'temperature': 0.0, 'avg_logprob': -0.1568804211086697, 'compression_ratio': 1.7008547008547008, 'no_speech_prob': 0.0001209194160765037}, {'id': 297, 'seek': 232312, 'start': 2323.12, 'end': 2333.6, 'text': ' so first hand is then modifying experience and then get is the object of our first hand job', 'tokens': [370, 700, 1011, 307, 550, 42626, 1752, 293, 550, 483, 307, 264, 2657, 295, 527, 700, 1011, 1691], 'temperature': 0.0, 'avg_logprob': -0.17263999515109593, 'compression_ratio': 1.724770642201835, 'no_speech_prob': 8.836038614390418e-05}, {'id': 298, 'seek': 232312, 'start': 2333.6, 'end': 2342.64, 'text': ' experience is the object of get and the students are the subject of get but if you have a smarty', 'tokens': [1752, 307, 264, 2657, 295, 483, 293, 264, 1731, 366, 264, 3983, 295, 483, 457, 498, 291, 362, 257, 4069, 88], 'temperature': 0.0, 'avg_logprob': -0.17263999515109593, 'compression_ratio': 1.724770642201835, 'no_speech_prob': 8.836038614390418e-05}, {'id': 299, 'seek': 234264, 'start': 2342.64, 'end': 2353.2799999999997, 'text': ' a mind you can interpret this a different way and in the alternative interpretation you then have hand', 'tokens': [257, 1575, 291, 393, 7302, 341, 257, 819, 636, 293, 294, 264, 8535, 14174, 291, 550, 362, 1011], 'temperature': 0.0, 'avg_logprob': -0.10654320959317481, 'compression_ratio': 1.9281045751633987, 'no_speech_prob': 5.878324373043142e-05}, {'id': 300, 'seek': 234264, 'start': 2353.2799999999997, 'end': 2364.64, 'text': ' going together with job and the first is then a modifier of experience and job is still a', 'tokens': [516, 1214, 365, 1691, 293, 264, 700, 307, 550, 257, 38011, 295, 1752, 293, 1691, 307, 920, 257], 'temperature': 0.0, 'avg_logprob': -0.10654320959317481, 'compression_ratio': 1.9281045751633987, 'no_speech_prob': 5.878324373043142e-05}, {'id': 301, 'seek': 234264, 'start': 2364.64, 'end': 2370.8799999999997, 'text': ' modifier of experience and so then you get this different power structure and different interpretation', 'tokens': [38011, 295, 1752, 293, 370, 550, 291, 483, 341, 819, 1347, 3877, 293, 819, 14174], 'temperature': 0.0, 'avg_logprob': -0.10654320959317481, 'compression_ratio': 1.9281045751633987, 'no_speech_prob': 5.878324373043142e-05}, {'id': 302, 'seek': 237088, 'start': 2370.88, 'end': 2380.08, 'text': \" there okay one more example in a way this example similar to the previous one it's sort of having\", 'tokens': [456, 1392, 472, 544, 1365, 294, 257, 636, 341, 1365, 2531, 281, 264, 3894, 472, 309, 311, 1333, 295, 1419], 'temperature': 0.0, 'avg_logprob': -0.09665364906436107, 'compression_ratio': 1.6032608695652173, 'no_speech_prob': 3.6986180930398405e-05}, {'id': 303, 'seek': 237088, 'start': 2381.36, 'end': 2387.76, 'text': ' modifier pieces that can modify different things but rather than just being with individual adjectives', 'tokens': [38011, 3755, 300, 393, 16927, 819, 721, 457, 2831, 813, 445, 885, 365, 2609, 29378, 1539], 'temperature': 0.0, 'avg_logprob': -0.09665364906436107, 'compression_ratio': 1.6032608695652173, 'no_speech_prob': 3.6986180930398405e-05}, {'id': 304, 'seek': 237088, 'start': 2387.76, 'end': 2396.7200000000003, 'text': ' or individual adverbs is then much larger units such as verb phrases can often have attachment', 'tokens': [420, 2609, 614, 43348, 307, 550, 709, 4833, 6815, 1270, 382, 9595, 20312, 393, 2049, 362, 19431], 'temperature': 0.0, 'avg_logprob': -0.09665364906436107, 'compression_ratio': 1.6032608695652173, 'no_speech_prob': 3.6986180930398405e-05}, {'id': 305, 'seek': 239672, 'start': 2396.72, 'end': 2404.3199999999997, 'text': ' ambiguities so this sentence headline is mutilated body washes up on Rio Beach to be used for', 'tokens': [40390, 1088, 370, 341, 8174, 28380, 307, 5839, 45678, 1772, 48616, 493, 322, 18719, 14866, 281, 312, 1143, 337], 'temperature': 0.0, 'avg_logprob': -0.109309407531238, 'compression_ratio': 1.7469135802469136, 'no_speech_prob': 5.382926974561997e-05}, {'id': 306, 'seek': 239672, 'start': 2404.3199999999997, 'end': 2411.6, 'text': ' Olympics Beach volleyball so we have this big verb phrase here of to be used for Olympics Beach', 'tokens': [19854, 14866, 35887, 370, 321, 362, 341, 955, 9595, 9535, 510, 295, 281, 312, 1143, 337, 19854, 14866], 'temperature': 0.0, 'avg_logprob': -0.109309407531238, 'compression_ratio': 1.7469135802469136, 'no_speech_prob': 5.382926974561997e-05}, {'id': 307, 'seek': 239672, 'start': 2411.6, 'end': 2421.2, 'text': ' volleyball and then again we have this attachment decision that we could either say that that', 'tokens': [35887, 293, 550, 797, 321, 362, 341, 19431, 3537, 300, 321, 727, 2139, 584, 300, 300], 'temperature': 0.0, 'avg_logprob': -0.109309407531238, 'compression_ratio': 1.7469135802469136, 'no_speech_prob': 5.382926974561997e-05}, {'id': 308, 'seek': 242120, 'start': 2421.2, 'end': 2433.4399999999996, 'text': ' big verb phrase is modifying i is attached to the Rio Beach or we could say no no the to be used', 'tokens': [955, 9595, 9535, 307, 42626, 741, 307, 8570, 281, 264, 18719, 14866, 420, 321, 727, 584, 572, 572, 264, 281, 312, 1143], 'temperature': 0.0, 'avg_logprob': -0.08635251662310432, 'compression_ratio': 1.7245508982035929, 'no_speech_prob': 4.4595846702577546e-05}, {'id': 309, 'seek': 242120, 'start': 2433.4399999999996, 'end': 2443.7599999999998, 'text': \" for Olympics Beach volleyball that that is modifying the mutilated body and it's a body that's\", 'tokens': [337, 19854, 14866, 35887, 300, 300, 307, 42626, 264, 5839, 45678, 1772, 293, 309, 311, 257, 1772, 300, 311], 'temperature': 0.0, 'avg_logprob': -0.08635251662310432, 'compression_ratio': 1.7245508982035929, 'no_speech_prob': 4.4595846702577546e-05}, {'id': 310, 'seek': 242120, 'start': 2443.7599999999998, 'end': 2450.7999999999997, 'text': \" to be used for the Olympics Beach volleyball which gives the funny reading yeah so I hope that's\", 'tokens': [281, 312, 1143, 337, 264, 19854, 14866, 35887, 597, 2709, 264, 4074, 3760, 1338, 370, 286, 1454, 300, 311], 'temperature': 0.0, 'avg_logprob': -0.08635251662310432, 'compression_ratio': 1.7245508982035929, 'no_speech_prob': 4.4595846702577546e-05}, {'id': 311, 'seek': 245080, 'start': 2450.8, 'end': 2458.8, 'text': ' giving you at least a little bit of a sense of how human language syntactic structure is complex', 'tokens': [2902, 291, 412, 1935, 257, 707, 857, 295, 257, 2020, 295, 577, 1952, 2856, 23980, 19892, 3877, 307, 3997], 'temperature': 0.0, 'avg_logprob': -0.13416714018041437, 'compression_ratio': 1.7844036697247707, 'no_speech_prob': 9.594601579010487e-05}, {'id': 312, 'seek': 245080, 'start': 2458.8, 'end': 2466.1600000000003, 'text': ' and big u.s and to work out the intended interpretations you need to know something about that structure', 'tokens': [293, 955, 344, 13, 82, 293, 281, 589, 484, 264, 10226, 37547, 291, 643, 281, 458, 746, 466, 300, 3877], 'temperature': 0.0, 'avg_logprob': -0.13416714018041437, 'compression_ratio': 1.7844036697247707, 'no_speech_prob': 9.594601579010487e-05}, {'id': 313, 'seek': 245080, 'start': 2467.44, 'end': 2474.4, 'text': ' in terms of how much you need to understand i mean you know this is under linguistics class if', 'tokens': [294, 2115, 295, 577, 709, 291, 643, 281, 1223, 741, 914, 291, 458, 341, 307, 833, 21766, 6006, 1508, 498], 'temperature': 0.0, 'avg_logprob': -0.13416714018041437, 'compression_ratio': 1.7844036697247707, 'no_speech_prob': 9.594601579010487e-05}, {'id': 314, 'seek': 245080, 'start': 2474.4, 'end': 2479.6000000000004, 'text': \" you'd like to learn more about human language structure you can go off and do a syntax class\", 'tokens': [291, 1116, 411, 281, 1466, 544, 466, 1952, 2856, 3877, 291, 393, 352, 766, 293, 360, 257, 28431, 1508], 'temperature': 0.0, 'avg_logprob': -0.13416714018041437, 'compression_ratio': 1.7844036697247707, 'no_speech_prob': 9.594601579010487e-05}, {'id': 315, 'seek': 247960, 'start': 2479.6, 'end': 2486.08, 'text': \" but you know we're not really going to spend a lot of time working through language structure\", 'tokens': [457, 291, 458, 321, 434, 406, 534, 516, 281, 3496, 257, 688, 295, 565, 1364, 807, 2856, 3877], 'temperature': 0.0, 'avg_logprob': -0.05455183688505196, 'compression_ratio': 1.7534883720930232, 'no_speech_prob': 0.0002466135483700782}, {'id': 316, 'seek': 247960, 'start': 2486.08, 'end': 2491.8399999999997, 'text': \" but there will be some questions on this in the assignment and so we're expecting that you can\", 'tokens': [457, 456, 486, 312, 512, 1651, 322, 341, 294, 264, 15187, 293, 370, 321, 434, 9650, 300, 291, 393], 'temperature': 0.0, 'avg_logprob': -0.05455183688505196, 'compression_ratio': 1.7534883720930232, 'no_speech_prob': 0.0002466135483700782}, {'id': 317, 'seek': 247960, 'start': 2491.8399999999997, 'end': 2498.08, 'text': ' be at the level that you can have sort of some intuitions as to which words and phrases are', 'tokens': [312, 412, 264, 1496, 300, 291, 393, 362, 1333, 295, 512, 16224, 626, 382, 281, 597, 2283, 293, 20312, 366], 'temperature': 0.0, 'avg_logprob': -0.05455183688505196, 'compression_ratio': 1.7534883720930232, 'no_speech_prob': 0.0002466135483700782}, {'id': 318, 'seek': 247960, 'start': 2498.08, 'end': 2504.0, 'text': ' modifying other words and phrases and therefore you could choose between two dependency analyses', 'tokens': [42626, 661, 2283, 293, 20312, 293, 4412, 291, 727, 2826, 1296, 732, 33621, 37560], 'temperature': 0.0, 'avg_logprob': -0.05455183688505196, 'compression_ratio': 1.7534883720930232, 'no_speech_prob': 0.0002466135483700782}, {'id': 319, 'seek': 250400, 'start': 2504.0, 'end': 2513.52, 'text': \" which ones correct okay i've spent quite a bit of time on that so better keep going okay so\", 'tokens': [597, 2306, 3006, 1392, 741, 600, 4418, 1596, 257, 857, 295, 565, 322, 300, 370, 1101, 1066, 516, 1392, 370], 'temperature': 0.0, 'avg_logprob': -0.08485187458086617, 'compression_ratio': 1.6576576576576576, 'no_speech_prob': 2.4232558644143865e-05}, {'id': 320, 'seek': 250400, 'start': 2514.4, 'end': 2521.6, 'text': ' the general idea is that knowing this sort of syntactic structure of a sentence can help us', 'tokens': [264, 2674, 1558, 307, 300, 5276, 341, 1333, 295, 23980, 19892, 3877, 295, 257, 8174, 393, 854, 505], 'temperature': 0.0, 'avg_logprob': -0.08485187458086617, 'compression_ratio': 1.6576576576576576, 'no_speech_prob': 2.4232558644143865e-05}, {'id': 321, 'seek': 250400, 'start': 2521.6, 'end': 2527.84, 'text': ' with semantic interpretation i mean as well as just generally saying we can understand language', 'tokens': [365, 47982, 14174, 741, 914, 382, 731, 382, 445, 5101, 1566, 321, 393, 1223, 2856], 'temperature': 0.0, 'avg_logprob': -0.08485187458086617, 'compression_ratio': 1.6576576576576576, 'no_speech_prob': 2.4232558644143865e-05}, {'id': 322, 'seek': 250400, 'start': 2527.84, 'end': 2533.68, 'text': \" it's also used in many cases for simple practical forms of semantic extraction so people\", 'tokens': [309, 311, 611, 1143, 294, 867, 3331, 337, 2199, 8496, 6422, 295, 47982, 30197, 370, 561], 'temperature': 0.0, 'avg_logprob': -0.08485187458086617, 'compression_ratio': 1.6576576576576576, 'no_speech_prob': 2.4232558644143865e-05}, {'id': 323, 'seek': 253368, 'start': 2533.68, 'end': 2539.2799999999997, 'text': ' such as in biomedical informatics often want to get out particular relations such as protein', 'tokens': [1270, 382, 294, 49775, 1356, 30292, 2049, 528, 281, 483, 484, 1729, 2299, 1270, 382, 7944], 'temperature': 0.0, 'avg_logprob': -0.14503486247002323, 'compression_ratio': 1.8028169014084507, 'no_speech_prob': 2.7074909667135216e-05}, {'id': 324, 'seek': 253368, 'start': 2539.2799999999997, 'end': 2545.12, 'text': \" protein interactions and while here's a sentence the results demonstrated that kai c interacts\", 'tokens': [7944, 13280, 293, 1339, 510, 311, 257, 8174, 264, 3542, 18772, 300, 350, 1301, 269, 43582], 'temperature': 0.0, 'avg_logprob': -0.14503486247002323, 'compression_ratio': 1.8028169014084507, 'no_speech_prob': 2.7074909667135216e-05}, {'id': 325, 'seek': 253368, 'start': 2545.12, 'end': 2555.04, 'text': ' rhythmically with sasa kai and kai b and commonly that people can get out those kind of relationships', 'tokens': [11801, 984, 365, 262, 9994, 350, 1301, 293, 350, 1301, 272, 293, 12719, 300, 561, 393, 483, 484, 729, 733, 295, 6159], 'temperature': 0.0, 'avg_logprob': -0.14503486247002323, 'compression_ratio': 1.8028169014084507, 'no_speech_prob': 2.7074909667135216e-05}, {'id': 326, 'seek': 253368, 'start': 2555.04, 'end': 2562.3999999999996, 'text': ' by looking at patterns of dependency relations with particular verbs so for the interacts verb', 'tokens': [538, 1237, 412, 8294, 295, 33621, 2299, 365, 1729, 30051, 370, 337, 264, 43582, 9595], 'temperature': 0.0, 'avg_logprob': -0.14503486247002323, 'compression_ratio': 1.8028169014084507, 'no_speech_prob': 2.7074909667135216e-05}, {'id': 327, 'seek': 256240, 'start': 2562.4, 'end': 2568.4, 'text': ' if you have a pattern of something being the subject and something else being the noun modifier', 'tokens': [498, 291, 362, 257, 5102, 295, 746, 885, 264, 3983, 293, 746, 1646, 885, 264, 23307, 38011], 'temperature': 0.0, 'avg_logprob': -0.0708314832051595, 'compression_ratio': 1.9214659685863875, 'no_speech_prob': 6.671422306681052e-05}, {'id': 328, 'seek': 256240, 'start': 2568.4, 'end': 2573.84, 'text': \" of interacts well that's an interaction relationship but it gets a bit more complicated than that\", 'tokens': [295, 43582, 731, 300, 311, 364, 9285, 2480, 457, 309, 2170, 257, 857, 544, 6179, 813, 300], 'temperature': 0.0, 'avg_logprob': -0.0708314832051595, 'compression_ratio': 1.9214659685863875, 'no_speech_prob': 6.671422306681052e-05}, {'id': 329, 'seek': 256240, 'start': 2573.84, 'end': 2579.52, 'text': ' as in this example because often there are conjunctions so you also have another pattern', 'tokens': [382, 294, 341, 1365, 570, 2049, 456, 366, 18244, 3916, 370, 291, 611, 362, 1071, 5102], 'temperature': 0.0, 'avg_logprob': -0.0708314832051595, 'compression_ratio': 1.9214659685863875, 'no_speech_prob': 6.671422306681052e-05}, {'id': 330, 'seek': 256240, 'start': 2579.52, 'end': 2587.2000000000003, 'text': ' where you have also interactions between the subject and the noun modifiers conjunct', 'tokens': [689, 291, 362, 611, 13280, 1296, 264, 3983, 293, 264, 23307, 1072, 23463, 18244, 349], 'temperature': 0.0, 'avg_logprob': -0.0708314832051595, 'compression_ratio': 1.9214659685863875, 'no_speech_prob': 6.671422306681052e-05}, {'id': 331, 'seek': 258720, 'start': 2587.2, 'end': 2597.12, 'text': \" which will allow us to also find the kai and kai b examples okay um so i've sort of given an informal\", 'tokens': [597, 486, 2089, 505, 281, 611, 915, 264, 350, 1301, 293, 350, 1301, 272, 5110, 1392, 1105, 370, 741, 600, 1333, 295, 2212, 364, 24342], 'temperature': 0.0, 'avg_logprob': -0.08648674038873203, 'compression_ratio': 1.6878612716763006, 'no_speech_prob': 1.775791861291509e-05}, {'id': 332, 'seek': 258720, 'start': 2597.12, 'end': 2604.7999999999997, 'text': ' tour of dependency grammar to just try and uh quickly um say a little bit more about formally', 'tokens': [3512, 295, 33621, 22317, 281, 445, 853, 293, 2232, 2661, 1105, 584, 257, 707, 857, 544, 466, 25983], 'temperature': 0.0, 'avg_logprob': -0.08648674038873203, 'compression_ratio': 1.6878612716763006, 'no_speech_prob': 1.775791861291509e-05}, {'id': 333, 'seek': 258720, 'start': 2604.7999999999997, 'end': 2613.7599999999998, 'text': ' what a dependency grammar is so in dependency syntax what we say is that the syntactic structure', 'tokens': [437, 257, 33621, 22317, 307, 370, 294, 33621, 28431, 437, 321, 584, 307, 300, 264, 23980, 19892, 3877], 'temperature': 0.0, 'avg_logprob': -0.08648674038873203, 'compression_ratio': 1.6878612716763006, 'no_speech_prob': 1.775791861291509e-05}, {'id': 334, 'seek': 261376, 'start': 2613.76, 'end': 2622.7200000000003, 'text': \" of a sentence consists of relations between pairs of words um and it's a binary asymmetric relation\", 'tokens': [295, 257, 8174, 14689, 295, 2299, 1296, 15494, 295, 2283, 1105, 293, 309, 311, 257, 17434, 37277, 17475, 9721], 'temperature': 0.0, 'avg_logprob': -0.07314611062770937, 'compression_ratio': 1.963350785340314, 'no_speech_prob': 2.6258914658683352e-05}, {'id': 335, 'seek': 261376, 'start': 2622.7200000000003, 'end': 2630.7200000000003, 'text': ' i we draw arrows between pairs of words which we call dependencies now normally dependency', 'tokens': [741, 321, 2642, 19669, 1296, 15494, 295, 2283, 597, 321, 818, 36606, 586, 5646, 33621], 'temperature': 0.0, 'avg_logprob': -0.07314611062770937, 'compression_ratio': 1.963350785340314, 'no_speech_prob': 2.6258914658683352e-05}, {'id': 336, 'seek': 261376, 'start': 2630.7200000000003, 'end': 2636.88, 'text': ' grammars then type those grammatical relation type those arrows to express what kind of', 'tokens': [17570, 685, 550, 2010, 729, 17570, 267, 804, 9721, 2010, 729, 19669, 281, 5109, 437, 733, 295], 'temperature': 0.0, 'avg_logprob': -0.07314611062770937, 'compression_ratio': 1.963350785340314, 'no_speech_prob': 2.6258914658683352e-05}, {'id': 337, 'seek': 261376, 'start': 2636.88, 'end': 2642.8, 'text': ' relation that there is and so that they have some kind of taxonomy of grammatical relation so we', 'tokens': [9721, 300, 456, 307, 293, 370, 300, 436, 362, 512, 733, 295, 3366, 23423, 295, 17570, 267, 804, 9721, 370, 321], 'temperature': 0.0, 'avg_logprob': -0.07314611062770937, 'compression_ratio': 1.963350785340314, 'no_speech_prob': 2.6258914658683352e-05}, {'id': 338, 'seek': 264280, 'start': 2642.8, 'end': 2648.4, 'text': ' might have a subject grammatical relation of verbal auxiliary grammatical relation and a bleak', 'tokens': [1062, 362, 257, 3983, 17570, 267, 804, 9721, 295, 24781, 43741, 17570, 267, 804, 9721, 293, 257, 5408, 514], 'temperature': 0.0, 'avg_logprob': -0.07671817380990555, 'compression_ratio': 1.864516129032258, 'no_speech_prob': 7.641156116733328e-06}, {'id': 339, 'seek': 264280, 'start': 2648.4, 'end': 2656.88, 'text': ' modifier grammatical relation we have some kind of typology of grammatical relations um so and we', 'tokens': [38011, 17570, 267, 804, 9721, 321, 362, 512, 733, 295, 2125, 1793, 295, 17570, 267, 804, 2299, 1105, 370, 293, 321], 'temperature': 0.0, 'avg_logprob': -0.07671817380990555, 'compression_ratio': 1.864516129032258, 'no_speech_prob': 7.641156116733328e-06}, {'id': 340, 'seek': 264280, 'start': 2656.88, 'end': 2665.76, 'text': ' refer to the arrows going between the head is the head here and something that is a dependent of', 'tokens': [2864, 281, 264, 19669, 516, 1296, 264, 1378, 307, 264, 1378, 510, 293, 746, 300, 307, 257, 12334, 295], 'temperature': 0.0, 'avg_logprob': -0.07671817380990555, 'compression_ratio': 1.864516129032258, 'no_speech_prob': 7.641156116733328e-06}, {'id': 341, 'seek': 266576, 'start': 2665.76, 'end': 2674.0800000000004, 'text': ' it so the subject of a verb is the dependent of the verb or when you have a noun modifier like', 'tokens': [309, 370, 264, 3983, 295, 257, 9595, 307, 264, 12334, 295, 264, 9595, 420, 562, 291, 362, 257, 23307, 38011, 411], 'temperature': 0.0, 'avg_logprob': -0.07094893957439222, 'compression_ratio': 1.8050314465408805, 'no_speech_prob': 2.3491320462198928e-05}, {'id': 342, 'seek': 266576, 'start': 2674.0800000000004, 'end': 2685.36, 'text': ' our sort of cuddly cat we say that um cuddly is a dependent of cat and so cat is the head of cuddly', 'tokens': [527, 1333, 295, 269, 26656, 356, 3857, 321, 584, 300, 1105, 269, 26656, 356, 307, 257, 12334, 295, 3857, 293, 370, 3857, 307, 264, 1378, 295, 269, 26656, 356], 'temperature': 0.0, 'avg_logprob': -0.07094893957439222, 'compression_ratio': 1.8050314465408805, 'no_speech_prob': 2.3491320462198928e-05}, {'id': 343, 'seek': 266576, 'start': 2685.36, 'end': 2695.5200000000004, 'text': ' cat and so normally um dependencies like in these examples form a tree which is formal it so', 'tokens': [3857, 293, 370, 5646, 1105, 36606, 411, 294, 613, 5110, 1254, 257, 4230, 597, 307, 9860, 309, 370], 'temperature': 0.0, 'avg_logprob': -0.07094893957439222, 'compression_ratio': 1.8050314465408805, 'no_speech_prob': 2.3491320462198928e-05}, {'id': 344, 'seek': 269552, 'start': 2695.52, 'end': 2704.16, 'text': \" it's not just any graph with arrows we have an graph which is connected a cyclic and has a single\", 'tokens': [309, 311, 406, 445, 604, 4295, 365, 19669, 321, 362, 364, 4295, 597, 307, 4582, 257, 38154, 1050, 293, 575, 257, 2167], 'temperature': 0.0, 'avg_logprob': -0.12170510821872288, 'compression_ratio': 1.6761363636363635, 'no_speech_prob': 1.2564606549858581e-05}, {'id': 345, 'seek': 269552, 'start': 2704.16, 'end': 2713.68, 'text': \" root so here's the root of the graph um and so that gives us a dependency tree analysis um dependency\", 'tokens': [5593, 370, 510, 311, 264, 5593, 295, 264, 4295, 1105, 293, 370, 300, 2709, 505, 257, 33621, 4230, 5215, 1105, 33621], 'temperature': 0.0, 'avg_logprob': -0.12170510821872288, 'compression_ratio': 1.6761363636363635, 'no_speech_prob': 1.2564606549858581e-05}, {'id': 346, 'seek': 269552, 'start': 2713.68, 'end': 2723.12, 'text': ' grammars have a really really long history um so the famous first linguist um was panini um who', 'tokens': [17570, 685, 362, 257, 534, 534, 938, 2503, 1105, 370, 264, 4618, 700, 21766, 468, 1105, 390, 2462, 3812, 1105, 567], 'temperature': 0.0, 'avg_logprob': -0.12170510821872288, 'compression_ratio': 1.6761363636363635, 'no_speech_prob': 1.2564606549858581e-05}, {'id': 347, 'seek': 272312, 'start': 2723.12, 'end': 2730.88, 'text': ' wrote about the structure of Sanskrit um and mainly he worked on the sound system of Sanskrit', 'tokens': [4114, 466, 264, 3877, 295, 44392, 1105, 293, 8704, 415, 2732, 322, 264, 1626, 1185, 295, 44392], 'temperature': 0.0, 'avg_logprob': -0.12087983260920018, 'compression_ratio': 1.8357487922705313, 'no_speech_prob': 6.586160452570766e-05}, {'id': 348, 'seek': 272312, 'start': 2730.88, 'end': 2736.24, 'text': ' and how sounds change in various contexts which what linguists call phonology and the different', 'tokens': [293, 577, 3263, 1319, 294, 3683, 30628, 597, 437, 21766, 1751, 818, 30754, 1793, 293, 264, 819], 'temperature': 0.0, 'avg_logprob': -0.12087983260920018, 'compression_ratio': 1.8357487922705313, 'no_speech_prob': 6.586160452570766e-05}, {'id': 349, 'seek': 272312, 'start': 2736.24, 'end': 2743.04, 'text': ' forms of Sanskrit words Sanskrit has rich morphology of inflecting nouns and verbs for different', 'tokens': [6422, 295, 44392, 2283, 44392, 575, 4593, 25778, 1793, 295, 1536, 1809, 278, 48184, 293, 30051, 337, 819], 'temperature': 0.0, 'avg_logprob': -0.12087983260920018, 'compression_ratio': 1.8357487922705313, 'no_speech_prob': 6.586160452570766e-05}, {'id': 350, 'seek': 272312, 'start': 2743.04, 'end': 2750.88, 'text': ' cases and forms um but he also worked a little on the syntactic structure of Sanskrit censors', 'tokens': [3331, 293, 6422, 1105, 457, 415, 611, 2732, 257, 707, 322, 264, 23980, 19892, 3877, 295, 44392, 19019, 830], 'temperature': 0.0, 'avg_logprob': -0.12087983260920018, 'compression_ratio': 1.8357487922705313, 'no_speech_prob': 6.586160452570766e-05}, {'id': 351, 'seek': 275088, 'start': 2750.88, 'end': 2758.32, 'text': ' and essentially what he proposed was the dependency grammar over Sanskrit sentences and it turns out', 'tokens': [293, 4476, 437, 415, 10348, 390, 264, 33621, 22317, 670, 44392, 16579, 293, 309, 4523, 484], 'temperature': 0.0, 'avg_logprob': -0.14134968028349035, 'compression_ratio': 1.7828054298642535, 'no_speech_prob': 1.1290570910205133e-05}, {'id': 352, 'seek': 275088, 'start': 2758.32, 'end': 2765.44, 'text': ' that sort of from most of recorded history when then when people have then um gone on and tried to', 'tokens': [300, 1333, 295, 490, 881, 295, 8287, 2503, 562, 550, 562, 561, 362, 550, 1105, 2780, 322, 293, 3031, 281], 'temperature': 0.0, 'avg_logprob': -0.14134968028349035, 'compression_ratio': 1.7828054298642535, 'no_speech_prob': 1.1290570910205133e-05}, {'id': 353, 'seek': 275088, 'start': 2765.44, 'end': 2773.36, 'text': ' put structures over human sentences um what they have used is dependency grammars um so there was a', 'tokens': [829, 9227, 670, 1952, 16579, 1105, 437, 436, 362, 1143, 307, 33621, 17570, 685, 1105, 370, 456, 390, 257], 'temperature': 0.0, 'avg_logprob': -0.14134968028349035, 'compression_ratio': 1.7828054298642535, 'no_speech_prob': 1.1290570910205133e-05}, {'id': 354, 'seek': 275088, 'start': 2773.36, 'end': 2780.4, 'text': ' lot of work in the first millennium by Arabic grammarians of trying to work out the grammar um', 'tokens': [688, 295, 589, 294, 264, 700, 21362, 2197, 538, 19938, 22317, 2567, 295, 1382, 281, 589, 484, 264, 22317, 1105], 'temperature': 0.0, 'avg_logprob': -0.14134968028349035, 'compression_ratio': 1.7828054298642535, 'no_speech_prob': 1.1290570910205133e-05}, {'id': 355, 'seek': 278040, 'start': 2780.4, 'end': 2786.8, 'text': \" structure of sentences and effectively what they used was but you know kind what I've just presented\", 'tokens': [3877, 295, 16579, 293, 8659, 437, 436, 1143, 390, 457, 291, 458, 733, 437, 286, 600, 445, 8212], 'temperature': 0.0, 'avg_logprob': -0.12230527257344809, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 4.053033990203403e-05}, {'id': 356, 'seek': 278040, 'start': 2786.8, 'end': 2795.92, 'text': ' as a dependency grammar so compared to you know 2500 years of history the ideas of having context', 'tokens': [382, 257, 33621, 22317, 370, 5347, 281, 291, 458, 41171, 924, 295, 2503, 264, 3487, 295, 1419, 4319], 'temperature': 0.0, 'avg_logprob': -0.12230527257344809, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 4.053033990203403e-05}, {'id': 357, 'seek': 278040, 'start': 2795.92, 'end': 2801.6800000000003, 'text': ' free grammars and having constituency grammars is actually a really really recent invention so it', 'tokens': [1737, 17570, 685, 293, 1419, 46146, 17570, 685, 307, 767, 257, 534, 534, 5162, 22265, 370, 309], 'temperature': 0.0, 'avg_logprob': -0.12230527257344809, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 4.053033990203403e-05}, {'id': 358, 'seek': 278040, 'start': 2801.6800000000003, 'end': 2808.1600000000003, 'text': ' was really sort of in the middle of the 20th century that the ideas of um constituency grammar and', 'tokens': [390, 534, 1333, 295, 294, 264, 2808, 295, 264, 945, 392, 4901, 300, 264, 3487, 295, 1105, 46146, 22317, 293], 'temperature': 0.0, 'avg_logprob': -0.12230527257344809, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 4.053033990203403e-05}, {'id': 359, 'seek': 280816, 'start': 2808.16, 'end': 2814.0, 'text': ' context free grammars would develop first by wells in the forties and then by known chomsky in the', 'tokens': [4319, 1737, 17570, 685, 576, 1499, 700, 538, 30984, 294, 264, 5009, 530, 293, 550, 538, 2570, 417, 4785, 4133, 294, 264], 'temperature': 0.0, 'avg_logprob': -0.20330441935678548, 'compression_ratio': 1.536, 'no_speech_prob': 3.360509072081186e-05}, {'id': 360, 'seek': 280816, 'start': 2814.0, 'end': 2821.12, 'text': ' early 50s leading to things like the chomsky hierarchy that you might see um CS 103 or formal', 'tokens': [2440, 2625, 82, 5775, 281, 721, 411, 264, 417, 4785, 4133, 22333, 300, 291, 1062, 536, 1105, 9460, 48784, 420, 9860], 'temperature': 0.0, 'avg_logprob': -0.20330441935678548, 'compression_ratio': 1.536, 'no_speech_prob': 3.360509072081186e-05}, {'id': 361, 'seek': 280816, 'start': 2821.12, 'end': 2830.3199999999997, 'text': ' languages class um so for modern work on dependency grammar using kind of the terminology and um', 'tokens': [8650, 1508, 1105, 370, 337, 4363, 589, 322, 33621, 22317, 1228, 733, 295, 264, 27575, 293, 1105], 'temperature': 0.0, 'avg_logprob': -0.20330441935678548, 'compression_ratio': 1.536, 'no_speech_prob': 3.360509072081186e-05}, {'id': 362, 'seek': 280816, 'start': 2830.3199999999997, 'end': 2836.3199999999997, 'text': \" notation that I've just introduced that's normally attributed to Lucian Tania who was a French\", 'tokens': [24657, 300, 286, 600, 445, 7268, 300, 311, 5646, 30976, 281, 37309, 282, 314, 5609, 567, 390, 257, 5522], 'temperature': 0.0, 'avg_logprob': -0.20330441935678548, 'compression_ratio': 1.536, 'no_speech_prob': 3.360509072081186e-05}, {'id': 363, 'seek': 283632, 'start': 2836.32, 'end': 2844.0, 'text': ' linguist um in around the sort of middle of the 20th century as well um dependency grammar was', 'tokens': [21766, 468, 1105, 294, 926, 264, 1333, 295, 2808, 295, 264, 945, 392, 4901, 382, 731, 1105, 33621, 22317, 390], 'temperature': 0.0, 'avg_logprob': -0.09354991262609308, 'compression_ratio': 1.669603524229075, 'no_speech_prob': 5.135123137733899e-05}, {'id': 364, 'seek': 283632, 'start': 2844.0, 'end': 2851.1200000000003, 'text': ' widely used in the 20th century um in a number of places I mean in particular it tends to be', 'tokens': [13371, 1143, 294, 264, 945, 392, 4901, 1105, 294, 257, 1230, 295, 3190, 286, 914, 294, 1729, 309, 12258, 281, 312], 'temperature': 0.0, 'avg_logprob': -0.09354991262609308, 'compression_ratio': 1.669603524229075, 'no_speech_prob': 5.135123137733899e-05}, {'id': 365, 'seek': 283632, 'start': 2851.1200000000003, 'end': 2857.52, 'text': ' sort of much more natural and easier to think about for languages that have a lot of different', 'tokens': [1333, 295, 709, 544, 3303, 293, 3571, 281, 519, 466, 337, 8650, 300, 362, 257, 688, 295, 819], 'temperature': 0.0, 'avg_logprob': -0.09354991262609308, 'compression_ratio': 1.669603524229075, 'no_speech_prob': 5.135123137733899e-05}, {'id': 366, 'seek': 283632, 'start': 2857.52, 'end': 2864.0, 'text': ' case markings on nouns like nomad of accused of genitive data of instrumental kind of cases like', 'tokens': [1389, 39087, 322, 48184, 411, 5369, 345, 295, 17085, 295, 1049, 2187, 1412, 295, 17388, 733, 295, 3331, 411], 'temperature': 0.0, 'avg_logprob': -0.09354991262609308, 'compression_ratio': 1.669603524229075, 'no_speech_prob': 5.135123137733899e-05}, {'id': 367, 'seek': 286400, 'start': 2864.0, 'end': 2869.28, 'text': ' you get in the language like Latin or Russian and a lot of those languages have much', 'tokens': [291, 483, 294, 264, 2856, 411, 10803, 420, 7220, 293, 257, 688, 295, 729, 8650, 362, 709], 'temperature': 0.0, 'avg_logprob': -0.07929898225344144, 'compression_ratio': 1.9462809917355373, 'no_speech_prob': 6.099312668084167e-05}, {'id': 368, 'seek': 286400, 'start': 2869.28, 'end': 2875.44, 'text': ' free word order than English so the subject or objective you know in English the subject has to', 'tokens': [1737, 1349, 1668, 813, 3669, 370, 264, 3983, 420, 10024, 291, 458, 294, 3669, 264, 3983, 575, 281], 'temperature': 0.0, 'avg_logprob': -0.07929898225344144, 'compression_ratio': 1.9462809917355373, 'no_speech_prob': 6.099312668084167e-05}, {'id': 369, 'seek': 286400, 'start': 2875.44, 'end': 2880.4, 'text': ' be before the verb and the object has to be after the verb but lots of other languages have much', 'tokens': [312, 949, 264, 9595, 293, 264, 2657, 575, 281, 312, 934, 264, 9595, 457, 3195, 295, 661, 8650, 362, 709], 'temperature': 0.0, 'avg_logprob': -0.07929898225344144, 'compression_ratio': 1.9462809917355373, 'no_speech_prob': 6.099312668084167e-05}, {'id': 370, 'seek': 286400, 'start': 2880.4, 'end': 2887.12, 'text': \" free word order and instead use different forms of nouns to show you what's the subject or the\", 'tokens': [1737, 1349, 1668, 293, 2602, 764, 819, 6422, 295, 48184, 281, 855, 291, 437, 311, 264, 3983, 420, 264], 'temperature': 0.0, 'avg_logprob': -0.07929898225344144, 'compression_ratio': 1.9462809917355373, 'no_speech_prob': 6.099312668084167e-05}, {'id': 371, 'seek': 286400, 'start': 2887.12, 'end': 2893.52, 'text': ' object of the sentence and dependency grammars can often seem much more natural for those kinds of', 'tokens': [2657, 295, 264, 8174, 293, 33621, 17570, 685, 393, 2049, 1643, 709, 544, 3303, 337, 729, 3685, 295], 'temperature': 0.0, 'avg_logprob': -0.07929898225344144, 'compression_ratio': 1.9462809917355373, 'no_speech_prob': 6.099312668084167e-05}, {'id': 372, 'seek': 289352, 'start': 2893.52, 'end': 2899.52, 'text': ' languages dependency grammars were also prominent at the very beginnings of computational linguistics so', 'tokens': [8650, 33621, 17570, 685, 645, 611, 17034, 412, 264, 588, 37281, 295, 28270, 21766, 6006, 370], 'temperature': 0.0, 'avg_logprob': -0.10560643183041925, 'compression_ratio': 2.1366120218579234, 'no_speech_prob': 3.454527541180141e-05}, {'id': 373, 'seek': 289352, 'start': 2900.32, 'end': 2907.12, 'text': ' one of the first people working computational linguistics in the US was David Hayes so the', 'tokens': [472, 295, 264, 700, 561, 1364, 28270, 21766, 6006, 294, 264, 2546, 390, 4389, 8721, 279, 370, 264], 'temperature': 0.0, 'avg_logprob': -0.10560643183041925, 'compression_ratio': 2.1366120218579234, 'no_speech_prob': 3.454527541180141e-05}, {'id': 374, 'seek': 289352, 'start': 2907.12, 'end': 2912.48, 'text': ' professional society for computational linguistics is called the association for computational linguistics', 'tokens': [4843, 4086, 337, 28270, 21766, 6006, 307, 1219, 264, 14598, 337, 28270, 21766, 6006], 'temperature': 0.0, 'avg_logprob': -0.10560643183041925, 'compression_ratio': 2.1366120218579234, 'no_speech_prob': 3.454527541180141e-05}, {'id': 375, 'seek': 289352, 'start': 2912.48, 'end': 2917.12, 'text': ' and he was actually one of the founders of the association for computational linguistics', 'tokens': [293, 415, 390, 767, 472, 295, 264, 25608, 295, 264, 14598, 337, 28270, 21766, 6006], 'temperature': 0.0, 'avg_logprob': -0.10560643183041925, 'compression_ratio': 2.1366120218579234, 'no_speech_prob': 3.454527541180141e-05}, {'id': 376, 'seek': 291712, 'start': 2917.12, 'end': 2924.64, 'text': ' and he published in the early 1960s and early perhaps the first dependency grammar past how', 'tokens': [293, 415, 6572, 294, 264, 2440, 16157, 82, 293, 2440, 4317, 264, 700, 33621, 22317, 1791, 577], 'temperature': 0.0, 'avg_logprob': -0.13727658987045288, 'compression_ratio': 1.7605633802816902, 'no_speech_prob': 1.3397908332990482e-05}, {'id': 377, 'seek': 291712, 'start': 2924.64, 'end': 2934.0, 'text': ' you dependency parser okay yeah a little teeny note just in case you see other things when', 'tokens': [291, 33621, 21156, 260, 1392, 1338, 257, 707, 48232, 3637, 445, 294, 1389, 291, 536, 661, 721, 562], 'temperature': 0.0, 'avg_logprob': -0.13727658987045288, 'compression_ratio': 1.7605633802816902, 'no_speech_prob': 1.3397908332990482e-05}, {'id': 378, 'seek': 291712, 'start': 2934.0, 'end': 2940.64, 'text': ' when you have these arrows you can draw them in either direction you either draw arrows from their', 'tokens': [562, 291, 362, 613, 19669, 291, 393, 2642, 552, 294, 2139, 3513, 291, 2139, 2642, 19669, 490, 641], 'temperature': 0.0, 'avg_logprob': -0.13727658987045288, 'compression_ratio': 1.7605633802816902, 'no_speech_prob': 1.3397908332990482e-05}, {'id': 379, 'seek': 291712, 'start': 2940.64, 'end': 2946.88, 'text': ' head or to the dependent or from the dependent to the head and actually different people have', 'tokens': [1378, 420, 281, 264, 12334, 420, 490, 264, 12334, 281, 264, 1378, 293, 767, 819, 561, 362], 'temperature': 0.0, 'avg_logprob': -0.13727658987045288, 'compression_ratio': 1.7605633802816902, 'no_speech_prob': 1.3397908332990482e-05}, {'id': 380, 'seek': 294688, 'start': 2946.88, 'end': 2953.44, 'text': ' done one and the other right so the way ten year drew them was to draw them from the head to the', 'tokens': [1096, 472, 293, 264, 661, 558, 370, 264, 636, 2064, 1064, 12804, 552, 390, 281, 2642, 552, 490, 264, 1378, 281, 264], 'temperature': 0.0, 'avg_logprob': -0.11065068738213901, 'compression_ratio': 1.8428571428571427, 'no_speech_prob': 2.3106669686967507e-05}, {'id': 381, 'seek': 294688, 'start': 2953.44, 'end': 2958.08, 'text': \" the dependent and we're following that convention but you know if you're looking at something that\", 'tokens': [264, 12334, 293, 321, 434, 3480, 300, 10286, 457, 291, 458, 498, 291, 434, 1237, 412, 746, 300], 'temperature': 0.0, 'avg_logprob': -0.11065068738213901, 'compression_ratio': 1.8428571428571427, 'no_speech_prob': 2.3106669686967507e-05}, {'id': 382, 'seek': 294688, 'start': 2958.08, 'end': 2964.32, 'text': ' somebody else has written with dependency arrows the first thing you have to work out is are they', 'tokens': [2618, 1646, 575, 3720, 365, 33621, 19669, 264, 700, 551, 291, 362, 281, 589, 484, 307, 366, 436], 'temperature': 0.0, 'avg_logprob': -0.11065068738213901, 'compression_ratio': 1.8428571428571427, 'no_speech_prob': 2.3106669686967507e-05}, {'id': 383, 'seek': 294688, 'start': 2964.32, 'end': 2971.76, 'text': ' using the arrow heads at the heads or the dependence now and not one other thing here is that', 'tokens': [1228, 264, 11610, 8050, 412, 264, 8050, 420, 264, 31704, 586, 293, 406, 472, 661, 551, 510, 307, 300], 'temperature': 0.0, 'avg_logprob': -0.11065068738213901, 'compression_ratio': 1.8428571428571427, 'no_speech_prob': 2.3106669686967507e-05}, {'id': 384, 'seek': 297176, 'start': 2971.76, 'end': 2979.6800000000003, 'text': ' we a sentence is seen as having the overall head word of the sentence which every other word of', 'tokens': [321, 257, 8174, 307, 1612, 382, 1419, 264, 4787, 1378, 1349, 295, 264, 8174, 597, 633, 661, 1349, 295], 'temperature': 0.0, 'avg_logprob': -0.06459934938521612, 'compression_ratio': 1.93, 'no_speech_prob': 5.139250060892664e-05}, {'id': 385, 'seek': 297176, 'start': 2979.6800000000003, 'end': 2986.48, 'text': \" the sentence hangs off it's a common convention to add this sort of fake route to every sentence\", 'tokens': [264, 8174, 35947, 766, 309, 311, 257, 2689, 10286, 281, 909, 341, 1333, 295, 7592, 7955, 281, 633, 8174], 'temperature': 0.0, 'avg_logprob': -0.06459934938521612, 'compression_ratio': 1.93, 'no_speech_prob': 5.139250060892664e-05}, {'id': 386, 'seek': 297176, 'start': 2986.48, 'end': 2994.48, 'text': ' that then points to the head word of the whole sentence here completed that just tends to make', 'tokens': [300, 550, 2793, 281, 264, 1378, 1349, 295, 264, 1379, 8174, 510, 7365, 300, 445, 12258, 281, 652], 'temperature': 0.0, 'avg_logprob': -0.06459934938521612, 'compression_ratio': 1.93, 'no_speech_prob': 5.139250060892664e-05}, {'id': 387, 'seek': 297176, 'start': 2994.48, 'end': 3001.44, 'text': ' the algorithmic stuff easier because then you can say that every word of the sentence is dependent', 'tokens': [264, 9284, 299, 1507, 3571, 570, 550, 291, 393, 584, 300, 633, 1349, 295, 264, 8174, 307, 12334], 'temperature': 0.0, 'avg_logprob': -0.06459934938521612, 'compression_ratio': 1.93, 'no_speech_prob': 5.139250060892664e-05}, {'id': 388, 'seek': 300144, 'start': 3001.44, 'end': 3008.0, 'text': ' on precisely one other node where what you can be dependent on is either another word on the', 'tokens': [322, 13402, 472, 661, 9984, 689, 437, 291, 393, 312, 12334, 322, 307, 2139, 1071, 1349, 322, 264], 'temperature': 0.0, 'avg_logprob': -0.08344392170981756, 'compression_ratio': 1.7030303030303031, 'no_speech_prob': 2.1409952751128003e-05}, {'id': 389, 'seek': 300144, 'start': 3008.0, 'end': 3014.48, 'text': ' sentence or the fake route of the sentence and when we build our parsers we will introduce that', 'tokens': [8174, 420, 264, 7592, 7955, 295, 264, 8174, 293, 562, 321, 1322, 527, 21156, 433, 321, 486, 5366, 300], 'temperature': 0.0, 'avg_logprob': -0.08344392170981756, 'compression_ratio': 1.7030303030303031, 'no_speech_prob': 2.1409952751128003e-05}, {'id': 390, 'seek': 300144, 'start': 3014.48, 'end': 3027.04, 'text': \" fake route okay so that's sort of dependency grammars and dependency structure I now want to\", 'tokens': [7592, 7955, 1392, 370, 300, 311, 1333, 295, 33621, 17570, 685, 293, 33621, 3877, 286, 586, 528, 281], 'temperature': 0.0, 'avg_logprob': -0.08344392170981756, 'compression_ratio': 1.7030303030303031, 'no_speech_prob': 2.1409952751128003e-05}, {'id': 391, 'seek': 302704, 'start': 3027.04, 'end': 3036.96, 'text': ' get us back to natural language processing and starting to build parsers for dependency grammars', 'tokens': [483, 505, 646, 281, 3303, 2856, 9007, 293, 2891, 281, 1322, 21156, 433, 337, 33621, 17570, 685], 'temperature': 0.0, 'avg_logprob': -0.05134370177984238, 'compression_ratio': 1.5714285714285714, 'no_speech_prob': 3.1170504371402785e-05}, {'id': 392, 'seek': 302704, 'start': 3036.96, 'end': 3045.36, 'text': \" but before doing that I just want to say yeah where do we get our data from and that's actually\", 'tokens': [457, 949, 884, 300, 286, 445, 528, 281, 584, 1338, 689, 360, 321, 483, 527, 1412, 490, 293, 300, 311, 767], 'temperature': 0.0, 'avg_logprob': -0.05134370177984238, 'compression_ratio': 1.5714285714285714, 'no_speech_prob': 3.1170504371402785e-05}, {'id': 393, 'seek': 302704, 'start': 3045.36, 'end': 3056.08, 'text': ' an interesting story in some sense so the answer to that is well what we do is get', 'tokens': [364, 1880, 1657, 294, 512, 2020, 370, 264, 1867, 281, 300, 307, 731, 437, 321, 360, 307, 483], 'temperature': 0.0, 'avg_logprob': -0.05134370177984238, 'compression_ratio': 1.5714285714285714, 'no_speech_prob': 3.1170504371402785e-05}, {'id': 394, 'seek': 305608, 'start': 3056.08, 'end': 3063.2, 'text': ' human beings commonly linguists or other people who are actually interested in the structure', 'tokens': [1952, 8958, 12719, 21766, 1751, 420, 661, 561, 567, 366, 767, 3102, 294, 264, 3877], 'temperature': 0.0, 'avg_logprob': -0.07233225914739794, 'compression_ratio': 1.7423312883435582, 'no_speech_prob': 6.289477460086346e-05}, {'id': 395, 'seek': 305608, 'start': 3063.2, 'end': 3071.7599999999998, 'text': ' of human sentences and we get them to sit around and hand parse sentences and give them dependency', 'tokens': [295, 1952, 16579, 293, 321, 483, 552, 281, 1394, 926, 293, 1011, 48377, 16579, 293, 976, 552, 33621], 'temperature': 0.0, 'avg_logprob': -0.07233225914739794, 'compression_ratio': 1.7423312883435582, 'no_speech_prob': 6.289477460086346e-05}, {'id': 396, 'seek': 305608, 'start': 3071.7599999999998, 'end': 3082.64, 'text': ' structures and we collect a lot of those parsers and we call that a tree bank and so this is', 'tokens': [9227, 293, 321, 2500, 257, 688, 295, 729, 21156, 433, 293, 321, 818, 300, 257, 4230, 3765, 293, 370, 341, 307], 'temperature': 0.0, 'avg_logprob': -0.07233225914739794, 'compression_ratio': 1.7423312883435582, 'no_speech_prob': 6.289477460086346e-05}, {'id': 397, 'seek': 308264, 'start': 3082.64, 'end': 3090.3199999999997, 'text': ' something that really only started happening in the late 80s and took off in a big away in the 90s', 'tokens': [746, 300, 534, 787, 1409, 2737, 294, 264, 3469, 4688, 82, 293, 1890, 766, 294, 257, 955, 1314, 294, 264, 4289, 82], 'temperature': 0.0, 'avg_logprob': -0.07358786400328292, 'compression_ratio': 1.867298578199052, 'no_speech_prob': 6.599734479095787e-05}, {'id': 398, 'seek': 308264, 'start': 3090.3199999999997, 'end': 3096.7999999999997, 'text': ' until then no one had attempted to build tree banks lots of people had attempted to build parsers', 'tokens': [1826, 550, 572, 472, 632, 18997, 281, 1322, 4230, 10237, 3195, 295, 561, 632, 18997, 281, 1322, 21156, 433], 'temperature': 0.0, 'avg_logprob': -0.07358786400328292, 'compression_ratio': 1.867298578199052, 'no_speech_prob': 6.599734479095787e-05}, {'id': 399, 'seek': 308264, 'start': 3096.7999999999997, 'end': 3104.24, 'text': ' and it seemed like well if you want to build a parser the efficient way to do it is to start writing', 'tokens': [293, 309, 6576, 411, 731, 498, 291, 528, 281, 1322, 257, 21156, 260, 264, 7148, 636, 281, 360, 309, 307, 281, 722, 3579], 'temperature': 0.0, 'avg_logprob': -0.07358786400328292, 'compression_ratio': 1.867298578199052, 'no_speech_prob': 6.599734479095787e-05}, {'id': 400, 'seek': 308264, 'start': 3104.24, 'end': 3110.08, 'text': ' a grammar so you start writing some grammar rules and you start writing a lexicon with words and', 'tokens': [257, 22317, 370, 291, 722, 3579, 512, 22317, 4474, 293, 291, 722, 3579, 257, 476, 87, 11911, 365, 2283, 293], 'temperature': 0.0, 'avg_logprob': -0.07358786400328292, 'compression_ratio': 1.867298578199052, 'no_speech_prob': 6.599734479095787e-05}, {'id': 401, 'seek': 311008, 'start': 3110.08, 'end': 3117.2799999999997, 'text': ' parts of speech and you sit around working on your grammar when I was a PhD student one of my first', 'tokens': [3166, 295, 6218, 293, 291, 1394, 926, 1364, 322, 428, 22317, 562, 286, 390, 257, 14476, 3107, 472, 295, 452, 700], 'temperature': 0.0, 'avg_logprob': -0.06528280442019543, 'compression_ratio': 1.6956521739130435, 'no_speech_prob': 2.6612549845594913e-05}, {'id': 402, 'seek': 311008, 'start': 3117.2799999999997, 'end': 3124.56, 'text': ' summer jobs was spending the summer handwriting a grammar and it sort of seems like writing a', 'tokens': [4266, 4782, 390, 6434, 264, 4266, 39179, 257, 22317, 293, 309, 1333, 295, 2544, 411, 3579, 257], 'temperature': 0.0, 'avg_logprob': -0.06528280442019543, 'compression_ratio': 1.6956521739130435, 'no_speech_prob': 2.6612549845594913e-05}, {'id': 403, 'seek': 311008, 'start': 3124.56, 'end': 3129.6, 'text': \" grammar is more efficient because you're writing this one general thing that tells you the structure\", 'tokens': [22317, 307, 544, 7148, 570, 291, 434, 3579, 341, 472, 2674, 551, 300, 5112, 291, 264, 3877], 'temperature': 0.0, 'avg_logprob': -0.06528280442019543, 'compression_ratio': 1.6956521739130435, 'no_speech_prob': 2.6612549845594913e-05}, {'id': 404, 'seek': 311008, 'start': 3129.6, 'end': 3135.7599999999998, 'text': \" of a human language but there's just been this massive sea change partly driven by the adoption\", 'tokens': [295, 257, 1952, 2856, 457, 456, 311, 445, 668, 341, 5994, 4158, 1319, 17031, 9555, 538, 264, 19215], 'temperature': 0.0, 'avg_logprob': -0.06528280442019543, 'compression_ratio': 1.6956521739130435, 'no_speech_prob': 2.6612549845594913e-05}, {'id': 405, 'seek': 313576, 'start': 3135.76, 'end': 3142.6400000000003, 'text': \" of machine learning techniques where it's now seen as axiomatic that the way to make progress\", 'tokens': [295, 3479, 2539, 7512, 689, 309, 311, 586, 1612, 382, 6360, 72, 13143, 300, 264, 636, 281, 652, 4205], 'temperature': 0.0, 'avg_logprob': -0.07152489493874943, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 9.291252354159951e-05}, {'id': 406, 'seek': 313576, 'start': 3142.6400000000003, 'end': 3151.44, 'text': ' is to have annotated data namely here a tree bank that shows you the structure of sentences', 'tokens': [307, 281, 362, 25339, 770, 1412, 20926, 510, 257, 4230, 3765, 300, 3110, 291, 264, 3877, 295, 16579], 'temperature': 0.0, 'avg_logprob': -0.07152489493874943, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 9.291252354159951e-05}, {'id': 407, 'seek': 313576, 'start': 3151.44, 'end': 3158.48, 'text': \" and so what I'm showing here is a teeny extract from a universal dependencies tree bank and so that's\", 'tokens': [293, 370, 437, 286, 478, 4099, 510, 307, 257, 48232, 8947, 490, 257, 11455, 36606, 4230, 3765, 293, 370, 300, 311], 'temperature': 0.0, 'avg_logprob': -0.07152489493874943, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 9.291252354159951e-05}, {'id': 408, 'seek': 313576, 'start': 3158.48, 'end': 3164.2400000000002, 'text': ' what I mentioned earlier that this has been this effort to try and have a common dependency', 'tokens': [437, 286, 2835, 3071, 300, 341, 575, 668, 341, 4630, 281, 853, 293, 362, 257, 2689, 33621], 'temperature': 0.0, 'avg_logprob': -0.07152489493874943, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 9.291252354159951e-05}, {'id': 409, 'seek': 316424, 'start': 3164.24, 'end': 3169.4399999999996, 'text': ' grammar representation that you can apply to lots of different human languages and so you can go', 'tokens': [22317, 10290, 300, 291, 393, 3079, 281, 3195, 295, 819, 1952, 8650, 293, 370, 291, 393, 352], 'temperature': 0.0, 'avg_logprob': -0.14077448041251536, 'compression_ratio': 1.6485355648535565, 'no_speech_prob': 3.701816967804916e-05}, {'id': 410, 'seek': 316424, 'start': 3169.4399999999996, 'end': 3175.2799999999997, 'text': \" over to this URL and see that there's about 60 different languages at the moment which have universal\", 'tokens': [670, 281, 341, 12905, 293, 536, 300, 456, 311, 466, 4060, 819, 8650, 412, 264, 1623, 597, 362, 11455], 'temperature': 0.0, 'avg_logprob': -0.14077448041251536, 'compression_ratio': 1.6485355648535565, 'no_speech_prob': 3.701816967804916e-05}, {'id': 411, 'seek': 316424, 'start': 3175.2799999999997, 'end': 3185.2799999999997, 'text': \" dependencies tree banks. So why are tree banks good? I mean it sort of seems like it's bad news if\", 'tokens': [36606, 4230, 10237, 13, 407, 983, 366, 4230, 10237, 665, 30, 286, 914, 309, 1333, 295, 2544, 411, 309, 311, 1578, 2583, 498], 'temperature': 0.0, 'avg_logprob': -0.14077448041251536, 'compression_ratio': 1.6485355648535565, 'no_speech_prob': 3.701816967804916e-05}, {'id': 412, 'seek': 316424, 'start': 3185.2799999999997, 'end': 3192.08, 'text': ' you have to have people sitting around for weeks and months hand-posing sentences it seems a lot', 'tokens': [291, 362, 281, 362, 561, 3798, 926, 337, 3259, 293, 2493, 1011, 12, 79, 6110, 16579, 309, 2544, 257, 688], 'temperature': 0.0, 'avg_logprob': -0.14077448041251536, 'compression_ratio': 1.6485355648535565, 'no_speech_prob': 3.701816967804916e-05}, {'id': 413, 'seek': 319208, 'start': 3192.08, 'end': 3200.3199999999997, 'text': ' slower and actually a lot less useful than having somebody writing a grammar which just has', 'tokens': [14009, 293, 767, 257, 688, 1570, 4420, 813, 1419, 2618, 3579, 257, 22317, 597, 445, 575], 'temperature': 0.0, 'avg_logprob': -0.10639429092407227, 'compression_ratio': 1.53475935828877, 'no_speech_prob': 6.39546342426911e-05}, {'id': 414, 'seek': 319208, 'start': 3201.04, 'end': 3210.4, 'text': ' you know a much bigger multiply factor in the utility of their effort. It turns out that although', 'tokens': [291, 458, 257, 709, 3801, 12972, 5952, 294, 264, 14877, 295, 641, 4630, 13, 467, 4523, 484, 300, 4878], 'temperature': 0.0, 'avg_logprob': -0.10639429092407227, 'compression_ratio': 1.53475935828877, 'no_speech_prob': 6.39546342426911e-05}, {'id': 415, 'seek': 319208, 'start': 3210.4, 'end': 3216.88, 'text': \" that initial feeling seems sort of valid that in practice there's just a lot more you can do with\", 'tokens': [300, 5883, 2633, 2544, 1333, 295, 7363, 300, 294, 3124, 456, 311, 445, 257, 688, 544, 291, 393, 360, 365], 'temperature': 0.0, 'avg_logprob': -0.10639429092407227, 'compression_ratio': 1.53475935828877, 'no_speech_prob': 6.39546342426911e-05}, {'id': 416, 'seek': 321688, 'start': 3216.88, 'end': 3226.4, 'text': ' the tree bank. So why are tree banks great? You know one reason is the tree banks are highly reusable', 'tokens': [264, 4230, 3765, 13, 407, 983, 366, 4230, 10237, 869, 30, 509, 458, 472, 1778, 307, 264, 4230, 10237, 366, 5405, 41807], 'temperature': 0.0, 'avg_logprob': -0.11623066493443081, 'compression_ratio': 1.7633136094674555, 'no_speech_prob': 1.6171739844139665e-05}, {'id': 417, 'seek': 321688, 'start': 3226.4, 'end': 3233.04, 'text': \" so typically when people have written grammars they've written grammars for you know one particular\", 'tokens': [370, 5850, 562, 561, 362, 3720, 17570, 685, 436, 600, 3720, 17570, 685, 337, 291, 458, 472, 1729], 'temperature': 0.0, 'avg_logprob': -0.11623066493443081, 'compression_ratio': 1.7633136094674555, 'no_speech_prob': 1.6171739844139665e-05}, {'id': 418, 'seek': 321688, 'start': 3233.84, 'end': 3240.1600000000003, 'text': ' parser and the only thing it was ever used in is that one particular parser but when you build a', 'tokens': [21156, 260, 293, 264, 787, 551, 309, 390, 1562, 1143, 294, 307, 300, 472, 1729, 21156, 260, 457, 562, 291, 1322, 257], 'temperature': 0.0, 'avg_logprob': -0.11623066493443081, 'compression_ratio': 1.7633136094674555, 'no_speech_prob': 1.6171739844139665e-05}, {'id': 419, 'seek': 324016, 'start': 3240.16, 'end': 3249.3599999999997, 'text': \" tree bank that's just a useful data resource and people use it for all kinds of things. So the\", 'tokens': [4230, 3765, 300, 311, 445, 257, 4420, 1412, 7684, 293, 561, 764, 309, 337, 439, 3685, 295, 721, 13, 407, 264], 'temperature': 0.0, 'avg_logprob': -0.06266373679751441, 'compression_ratio': 1.7149321266968325, 'no_speech_prob': 1.4227804967958946e-05}, {'id': 420, 'seek': 324016, 'start': 3249.3599999999997, 'end': 3256.48, 'text': ' well-known tree banks have been used by hundreds and hundreds of people and although all tree banks', 'tokens': [731, 12, 6861, 4230, 10237, 362, 668, 1143, 538, 6779, 293, 6779, 295, 561, 293, 4878, 439, 4230, 10237], 'temperature': 0.0, 'avg_logprob': -0.06266373679751441, 'compression_ratio': 1.7149321266968325, 'no_speech_prob': 1.4227804967958946e-05}, {'id': 421, 'seek': 324016, 'start': 3256.48, 'end': 3262.72, 'text': \" were initially built for the purposes of hey let's help natural language processing systems\", 'tokens': [645, 9105, 3094, 337, 264, 9932, 295, 4177, 718, 311, 854, 3303, 2856, 9007, 3652], 'temperature': 0.0, 'avg_logprob': -0.06266373679751441, 'compression_ratio': 1.7149321266968325, 'no_speech_prob': 1.4227804967958946e-05}, {'id': 422, 'seek': 324016, 'start': 3262.72, 'end': 3268.0, 'text': ' it turns out that people have actually been able to do lots of other things with tree banks.', 'tokens': [309, 4523, 484, 300, 561, 362, 767, 668, 1075, 281, 360, 3195, 295, 661, 721, 365, 4230, 10237, 13], 'temperature': 0.0, 'avg_logprob': -0.06266373679751441, 'compression_ratio': 1.7149321266968325, 'no_speech_prob': 1.4227804967958946e-05}, {'id': 423, 'seek': 326800, 'start': 3268.0, 'end': 3274.64, 'text': ' So for example these days psycho-linguists commonly use tree banks to get various kinds of', 'tokens': [407, 337, 1365, 613, 1708, 33355, 12, 1688, 84, 1751, 12719, 764, 4230, 10237, 281, 483, 3683, 3685, 295], 'temperature': 0.0, 'avg_logprob': -0.1472195026486419, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 5.539220728678629e-05}, {'id': 424, 'seek': 326800, 'start': 3274.64, 'end': 3281.52, 'text': ' statistics about data for thinking about psycho-linguistic models. Linguists use tree banks for', 'tokens': [12523, 466, 1412, 337, 1953, 466, 33355, 12, 1688, 84, 3142, 5245, 13, 441, 7050, 1751, 764, 4230, 10237, 337], 'temperature': 0.0, 'avg_logprob': -0.1472195026486419, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 5.539220728678629e-05}, {'id': 425, 'seek': 326800, 'start': 3281.52, 'end': 3287.52, 'text': \" looking at patterns of different syntactic constructions that occur that there's just been a lot\", 'tokens': [1237, 412, 8294, 295, 819, 23980, 19892, 7690, 626, 300, 5160, 300, 456, 311, 445, 668, 257, 688], 'temperature': 0.0, 'avg_logprob': -0.1472195026486419, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 5.539220728678629e-05}, {'id': 426, 'seek': 326800, 'start': 3287.52, 'end': 3295.12, 'text': ' of reuse of this data for all kinds of purposes but they have other advantages that I mentioned here', 'tokens': [295, 26225, 295, 341, 1412, 337, 439, 3685, 295, 9932, 457, 436, 362, 661, 14906, 300, 286, 2835, 510], 'temperature': 0.0, 'avg_logprob': -0.1472195026486419, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 5.539220728678629e-05}, {'id': 427, 'seek': 329512, 'start': 3295.12, 'end': 3300.3199999999997, 'text': ' you know when people are just sitting around saying oh what sentences are good they tend to', 'tokens': [291, 458, 562, 561, 366, 445, 3798, 926, 1566, 1954, 437, 16579, 366, 665, 436, 3928, 281], 'temperature': 0.0, 'avg_logprob': -0.07678574607485816, 'compression_ratio': 1.8095238095238095, 'no_speech_prob': 0.0001213386858580634}, {'id': 428, 'seek': 329512, 'start': 3300.3199999999997, 'end': 3306.24, 'text': ' only think of the core of language where lots of weird things happen in language and so if you', 'tokens': [787, 519, 295, 264, 4965, 295, 2856, 689, 3195, 295, 3657, 721, 1051, 294, 2856, 293, 370, 498, 291], 'temperature': 0.0, 'avg_logprob': -0.07678574607485816, 'compression_ratio': 1.8095238095238095, 'no_speech_prob': 0.0001213386858580634}, {'id': 429, 'seek': 329512, 'start': 3306.24, 'end': 3312.08, 'text': ' actually just have some sentences and you have to go off and parse them then you actually have to', 'tokens': [767, 445, 362, 512, 16579, 293, 291, 362, 281, 352, 766, 293, 48377, 552, 550, 291, 767, 362, 281], 'temperature': 0.0, 'avg_logprob': -0.07678574607485816, 'compression_ratio': 1.8095238095238095, 'no_speech_prob': 0.0001213386858580634}, {'id': 430, 'seek': 329512, 'start': 3312.08, 'end': 3319.92, 'text': \" deal with the totality of language. Since you're parsing actual sentences you get statistics so\", 'tokens': [2028, 365, 264, 1993, 1860, 295, 2856, 13, 4162, 291, 434, 21156, 278, 3539, 16579, 291, 483, 12523, 370], 'temperature': 0.0, 'avg_logprob': -0.07678574607485816, 'compression_ratio': 1.8095238095238095, 'no_speech_prob': 0.0001213386858580634}, {'id': 431, 'seek': 331992, 'start': 3319.92, 'end': 3325.44, 'text': ' you naturally get the kind of statistics that are useful to machine learning systems by', 'tokens': [291, 8195, 483, 264, 733, 295, 12523, 300, 366, 4420, 281, 3479, 2539, 3652, 538], 'temperature': 0.0, 'avg_logprob': -0.09989641919548128, 'compression_ratio': 1.614678899082569, 'no_speech_prob': 3.158051913487725e-05}, {'id': 432, 'seek': 331992, 'start': 3325.44, 'end': 3331.28, 'text': \" constructing a tree bank where you don't get them for free if you handwrite a grammar but then a\", 'tokens': [39969, 257, 4230, 3765, 689, 291, 500, 380, 483, 552, 337, 1737, 498, 291, 1011, 21561, 257, 22317, 457, 550, 257], 'temperature': 0.0, 'avg_logprob': -0.09989641919548128, 'compression_ratio': 1.614678899082569, 'no_speech_prob': 3.158051913487725e-05}, {'id': 433, 'seek': 331992, 'start': 3331.28, 'end': 3341.44, 'text': ' final way which is perhaps the most important of all is if you actually want to be able to do', 'tokens': [2572, 636, 597, 307, 4317, 264, 881, 1021, 295, 439, 307, 498, 291, 767, 528, 281, 312, 1075, 281, 360], 'temperature': 0.0, 'avg_logprob': -0.09989641919548128, 'compression_ratio': 1.614678899082569, 'no_speech_prob': 3.158051913487725e-05}, {'id': 434, 'seek': 331992, 'start': 3343.2000000000003, 'end': 3349.6, 'text': ' science of building systems you need a way to evaluate these NLP systems.', 'tokens': [3497, 295, 2390, 3652, 291, 643, 257, 636, 281, 13059, 613, 426, 45196, 3652, 13], 'temperature': 0.0, 'avg_logprob': -0.09989641919548128, 'compression_ratio': 1.614678899082569, 'no_speech_prob': 3.158051913487725e-05}, {'id': 435, 'seek': 334960, 'start': 3349.6, 'end': 3359.68, 'text': ' I mean it seems hard to believe now but you know back in the 90s 80s when people built NLP', 'tokens': [286, 914, 309, 2544, 1152, 281, 1697, 586, 457, 291, 458, 646, 294, 264, 4289, 82, 4688, 82, 562, 561, 3094, 426, 45196], 'temperature': 0.0, 'avg_logprob': -0.1401535537507799, 'compression_ratio': 1.5245901639344261, 'no_speech_prob': 0.0002445974969305098}, {'id': 436, 'seek': 334960, 'start': 3359.68, 'end': 3367.44, 'text': ' parsers it was literally the case that the way they were evaluated was you said to your friend', 'tokens': [21156, 433, 309, 390, 3736, 264, 1389, 300, 264, 636, 436, 645, 25509, 390, 291, 848, 281, 428, 1277], 'temperature': 0.0, 'avg_logprob': -0.1401535537507799, 'compression_ratio': 1.5245901639344261, 'no_speech_prob': 0.0002445974969305098}, {'id': 437, 'seek': 334960, 'start': 3367.44, 'end': 3372.64, 'text': \" oh I built this parser type in a sentence on the terminal and see what it gives you back it's\", 'tokens': [1954, 286, 3094, 341, 21156, 260, 2010, 294, 257, 8174, 322, 264, 14709, 293, 536, 437, 309, 2709, 291, 646, 309, 311], 'temperature': 0.0, 'avg_logprob': -0.1401535537507799, 'compression_ratio': 1.5245901639344261, 'no_speech_prob': 0.0002445974969305098}, {'id': 438, 'seek': 337264, 'start': 3372.64, 'end': 3379.92, 'text': \" pretty good hey and that was just the way business was done whereas what we'd like to know is well\", 'tokens': [1238, 665, 4177, 293, 300, 390, 445, 264, 636, 1606, 390, 1096, 9735, 437, 321, 1116, 411, 281, 458, 307, 731], 'temperature': 0.0, 'avg_logprob': -0.08284687090523635, 'compression_ratio': 1.6875, 'no_speech_prob': 5.815066833747551e-05}, {'id': 439, 'seek': 337264, 'start': 3379.92, 'end': 3385.92, 'text': ' as I showed you earlier English sentences can have lots of different parsers commonly can this', 'tokens': [382, 286, 4712, 291, 3071, 3669, 16579, 393, 362, 3195, 295, 819, 21156, 433, 12719, 393, 341], 'temperature': 0.0, 'avg_logprob': -0.08284687090523635, 'compression_ratio': 1.6875, 'no_speech_prob': 5.815066833747551e-05}, {'id': 440, 'seek': 337264, 'start': 3387.52, 'end': 3393.8399999999997, 'text': ' system choose the right parsers for particular sentences and therefore have the basis of', 'tokens': [1185, 2826, 264, 558, 21156, 433, 337, 1729, 16579, 293, 4412, 362, 264, 5143, 295], 'temperature': 0.0, 'avg_logprob': -0.08284687090523635, 'compression_ratio': 1.6875, 'no_speech_prob': 5.815066833747551e-05}, {'id': 441, 'seek': 337264, 'start': 3394.72, 'end': 3400.72, 'text': ' interpreting them as a human being would and well we can only systematically do that evaluation', 'tokens': [37395, 552, 382, 257, 1952, 885, 576, 293, 731, 321, 393, 787, 39531, 360, 300, 13344], 'temperature': 0.0, 'avg_logprob': -0.08284687090523635, 'compression_ratio': 1.6875, 'no_speech_prob': 5.815066833747551e-05}, {'id': 442, 'seek': 340072, 'start': 3400.72, 'end': 3406.48, 'text': ' if we have a whole bunch of sentences that have been handparsed by humans with their correct', 'tokens': [498, 321, 362, 257, 1379, 3840, 295, 16579, 300, 362, 668, 1011, 79, 685, 292, 538, 6255, 365, 641, 3006], 'temperature': 0.0, 'avg_logprob': -0.12104809284210205, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 3.759486207854934e-05}, {'id': 443, 'seek': 340072, 'start': 3406.48, 'end': 3414.72, 'text': ' interpretations so the rise of tree banks turned parser building into an empirical science where people', 'tokens': [37547, 370, 264, 6272, 295, 4230, 10237, 3574, 21156, 260, 2390, 666, 364, 31886, 3497, 689, 561], 'temperature': 0.0, 'avg_logprob': -0.12104809284210205, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 3.759486207854934e-05}, {'id': 444, 'seek': 340072, 'start': 3414.72, 'end': 3422.7999999999997, 'text': ' could then compete rigorously on the basis of look my parser has 2% higher accuracy than your parser', 'tokens': [727, 550, 11831, 42191, 5098, 322, 264, 5143, 295, 574, 452, 21156, 260, 575, 568, 4, 2946, 14170, 813, 428, 21156, 260], 'temperature': 0.0, 'avg_logprob': -0.12104809284210205, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 3.759486207854934e-05}, {'id': 445, 'seek': 340072, 'start': 3422.7999999999997, 'end': 3430.64, 'text': ' in choosing the correct parsers for sentences. Okay so well how do we build a parser', 'tokens': [294, 10875, 264, 3006, 21156, 433, 337, 16579, 13, 1033, 370, 731, 577, 360, 321, 1322, 257, 21156, 260], 'temperature': 0.0, 'avg_logprob': -0.12104809284210205, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 3.759486207854934e-05}, {'id': 446, 'seek': 343064, 'start': 3430.64, 'end': 3436.7999999999997, 'text': \" once we've got dependencies so there's sort of a bunch of sources of information that you could\", 'tokens': [1564, 321, 600, 658, 36606, 370, 456, 311, 1333, 295, 257, 3840, 295, 7139, 295, 1589, 300, 291, 727], 'temperature': 0.0, 'avg_logprob': -0.08793421397133479, 'compression_ratio': 1.6826347305389222, 'no_speech_prob': 7.654555520275608e-05}, {'id': 447, 'seek': 343064, 'start': 3436.7999999999997, 'end': 3445.8399999999997, 'text': ' hope to use so one source of information is looking at the words on either end of the dependency', 'tokens': [1454, 281, 764, 370, 472, 4009, 295, 1589, 307, 1237, 412, 264, 2283, 322, 2139, 917, 295, 264, 33621], 'temperature': 0.0, 'avg_logprob': -0.08793421397133479, 'compression_ratio': 1.6826347305389222, 'no_speech_prob': 7.654555520275608e-05}, {'id': 448, 'seek': 343064, 'start': 3445.8399999999997, 'end': 3453.52, 'text': \" so discussing issues that seems a reasonable thing to say and so it's likely that issues\", 'tokens': [370, 10850, 2663, 300, 2544, 257, 10585, 551, 281, 584, 293, 370, 309, 311, 3700, 300, 2663], 'temperature': 0.0, 'avg_logprob': -0.08793421397133479, 'compression_ratio': 1.6826347305389222, 'no_speech_prob': 7.654555520275608e-05}, {'id': 449, 'seek': 345352, 'start': 3453.52, 'end': 3463.04, 'text': ' could be the object of discussing whereas if it was some other word right if you were thinking of', 'tokens': [727, 312, 264, 2657, 295, 10850, 9735, 498, 309, 390, 512, 661, 1349, 558, 498, 291, 645, 1953, 295], 'temperature': 0.0, 'avg_logprob': -0.1465270245661501, 'compression_ratio': 1.7455621301775148, 'no_speech_prob': 9.846375178312883e-05}, {'id': 450, 'seek': 345352, 'start': 3463.04, 'end': 3470.96, 'text': \" making you know outstanding the object of discussion discussing outstanding that doesn't sound right\", 'tokens': [1455, 291, 458, 14485, 264, 2657, 295, 5017, 10850, 14485, 300, 1177, 380, 1626, 558], 'temperature': 0.0, 'avg_logprob': -0.1465270245661501, 'compression_ratio': 1.7455621301775148, 'no_speech_prob': 9.846375178312883e-05}, {'id': 451, 'seek': 345352, 'start': 3470.96, 'end': 3478.72, 'text': \" so that wouldn't be so good. A second source of information is distance so most dependencies are\", 'tokens': [370, 300, 2759, 380, 312, 370, 665, 13, 316, 1150, 4009, 295, 1589, 307, 4560, 370, 881, 36606, 366], 'temperature': 0.0, 'avg_logprob': -0.1465270245661501, 'compression_ratio': 1.7455621301775148, 'no_speech_prob': 9.846375178312883e-05}, {'id': 452, 'seek': 347872, 'start': 3478.72, 'end': 3485.12, 'text': \" relatively short distance some of them aren't some of long distance dependencies but they're\", 'tokens': [7226, 2099, 4560, 512, 295, 552, 3212, 380, 512, 295, 938, 4560, 36606, 457, 436, 434], 'temperature': 0.0, 'avg_logprob': -0.12545730417424983, 'compression_ratio': 1.7710843373493976, 'no_speech_prob': 4.7439658374059945e-05}, {'id': 453, 'seek': 347872, 'start': 3485.12, 'end': 3492.9599999999996, 'text': ' relatively rare the vast majority of dependencies nearby and another source of information is the', 'tokens': [7226, 5892, 264, 8369, 6286, 295, 36606, 11184, 293, 1071, 4009, 295, 1589, 307, 264], 'temperature': 0.0, 'avg_logprob': -0.12545730417424983, 'compression_ratio': 1.7710843373493976, 'no_speech_prob': 4.7439658374059945e-05}, {'id': 454, 'seek': 347872, 'start': 3492.9599999999996, 'end': 3503.9199999999996, 'text': ' intervening material so there are certain things that dependencies rarely span so clauses and sentences', 'tokens': [17104, 278, 2527, 370, 456, 366, 1629, 721, 300, 36606, 13752, 16174, 370, 49072, 293, 16579], 'temperature': 0.0, 'avg_logprob': -0.12545730417424983, 'compression_ratio': 1.7710843373493976, 'no_speech_prob': 4.7439658374059945e-05}, {'id': 455, 'seek': 350392, 'start': 3503.92, 'end': 3512.56, 'text': ' are normally organized around verbs and so dependencies rarely span across intervening verbs.', 'tokens': [366, 5646, 9983, 926, 30051, 293, 370, 36606, 13752, 16174, 2108, 17104, 278, 30051, 13], 'temperature': 0.0, 'avg_logprob': -0.10972020543854812, 'compression_ratio': 1.6222222222222222, 'no_speech_prob': 0.00011028940207324922}, {'id': 456, 'seek': 350392, 'start': 3513.6800000000003, 'end': 3519.52, 'text': ' We can also use punctuation and written language things like commas which can give some indication', 'tokens': [492, 393, 611, 764, 27006, 16073, 293, 3720, 2856, 721, 411, 800, 296, 597, 393, 976, 512, 18877], 'temperature': 0.0, 'avg_logprob': -0.10972020543854812, 'compression_ratio': 1.6222222222222222, 'no_speech_prob': 0.00011028940207324922}, {'id': 457, 'seek': 350392, 'start': 3519.52, 'end': 3527.44, 'text': ' of the structure and so punctuation may also indicate bad places to have long distance dependencies', 'tokens': [295, 264, 3877, 293, 370, 27006, 16073, 815, 611, 13330, 1578, 3190, 281, 362, 938, 4560, 36606], 'temperature': 0.0, 'avg_logprob': -0.10972020543854812, 'compression_ratio': 1.6222222222222222, 'no_speech_prob': 0.00011028940207324922}, {'id': 458, 'seek': 352744, 'start': 3527.44, 'end': 3536.56, 'text': \" over and there's one final source of information which is what's referred to as valency which is\", 'tokens': [670, 293, 456, 311, 472, 2572, 4009, 295, 1589, 597, 307, 437, 311, 10839, 281, 382, 1323, 3020, 597, 307], 'temperature': 0.0, 'avg_logprob': -0.09762921409001427, 'compression_ratio': 1.6848484848484848, 'no_speech_prob': 4.604632704285905e-05}, {'id': 459, 'seek': 352744, 'start': 3536.56, 'end': 3543.36, 'text': ' forehead what kind of information does it usually have around it so if you have a noun', 'tokens': [20472, 437, 733, 295, 1589, 775, 309, 2673, 362, 926, 309, 370, 498, 291, 362, 257, 23307], 'temperature': 0.0, 'avg_logprob': -0.09762921409001427, 'compression_ratio': 1.6848484848484848, 'no_speech_prob': 4.604632704285905e-05}, {'id': 460, 'seek': 352744, 'start': 3545.2000000000003, 'end': 3551.6, 'text': \" there are things that you just know about what kinds of dependence nouns normally have so it's\", 'tokens': [456, 366, 721, 300, 291, 445, 458, 466, 437, 3685, 295, 31704, 48184, 5646, 362, 370, 309, 311], 'temperature': 0.0, 'avg_logprob': -0.09762921409001427, 'compression_ratio': 1.6848484848484848, 'no_speech_prob': 4.604632704285905e-05}, {'id': 461, 'seek': 355160, 'start': 3551.6, 'end': 3561.12, 'text': \" common that it will have a determiner to the left the cat on the other hand it's not going to be the\", 'tokens': [2689, 300, 309, 486, 362, 257, 3618, 4564, 281, 264, 1411, 264, 3857, 322, 264, 661, 1011, 309, 311, 406, 516, 281, 312, 264], 'temperature': 0.0, 'avg_logprob': -0.10909084371618323, 'compression_ratio': 1.709090909090909, 'no_speech_prob': 9.00782470125705e-05}, {'id': 462, 'seek': 355160, 'start': 3561.12, 'end': 3566.96, 'text': \" case that there's a determiner to the right cat that that's just not what you get in English\", 'tokens': [1389, 300, 456, 311, 257, 3618, 4564, 281, 264, 558, 3857, 300, 300, 311, 445, 406, 437, 291, 483, 294, 3669], 'temperature': 0.0, 'avg_logprob': -0.10909084371618323, 'compression_ratio': 1.709090909090909, 'no_speech_prob': 9.00782470125705e-05}, {'id': 463, 'seek': 355160, 'start': 3568.4, 'end': 3574.88, 'text': \" on the left you're also likely to have an adjective or modify that's where he had cuddly\", 'tokens': [322, 264, 1411, 291, 434, 611, 3700, 281, 362, 364, 44129, 420, 16927, 300, 311, 689, 415, 632, 269, 26656, 356], 'temperature': 0.0, 'avg_logprob': -0.10909084371618323, 'compression_ratio': 1.709090909090909, 'no_speech_prob': 9.00782470125705e-05}, {'id': 464, 'seek': 357488, 'start': 3574.88, 'end': 3582.8, 'text': \" but again it's not so likely you're going to have the adjective or modifier over on the right\", 'tokens': [457, 797, 309, 311, 406, 370, 3700, 291, 434, 516, 281, 362, 264, 44129, 420, 38011, 670, 322, 264, 558], 'temperature': 0.0, 'avg_logprob': -0.08922160850776421, 'compression_ratio': 1.7227272727272727, 'no_speech_prob': 1.9193037587683648e-05}, {'id': 465, 'seek': 357488, 'start': 3582.8, 'end': 3589.52, 'text': ' for cuddly so there are sort of facts about what things different kinds of words take on the left', 'tokens': [337, 269, 26656, 356, 370, 456, 366, 1333, 295, 9130, 466, 437, 721, 819, 3685, 295, 2283, 747, 322, 264, 1411], 'temperature': 0.0, 'avg_logprob': -0.08922160850776421, 'compression_ratio': 1.7227272727272727, 'no_speech_prob': 1.9193037587683648e-05}, {'id': 466, 'seek': 357488, 'start': 3589.52, 'end': 3595.2000000000003, 'text': \" and the right and so that's the valency of the heads and that's also a useful source of information\", 'tokens': [293, 264, 558, 293, 370, 300, 311, 264, 1323, 3020, 295, 264, 8050, 293, 300, 311, 611, 257, 4420, 4009, 295, 1589], 'temperature': 0.0, 'avg_logprob': -0.08922160850776421, 'compression_ratio': 1.7227272727272727, 'no_speech_prob': 1.9193037587683648e-05}, {'id': 467, 'seek': 357488, 'start': 3596.8, 'end': 3604.6400000000003, 'text': ' okay so what do we need to do using that information to build a parser well effectively', 'tokens': [1392, 370, 437, 360, 321, 643, 281, 360, 1228, 300, 1589, 281, 1322, 257, 21156, 260, 731, 8659], 'temperature': 0.0, 'avg_logprob': -0.08922160850776421, 'compression_ratio': 1.7227272727272727, 'no_speech_prob': 1.9193037587683648e-05}, {'id': 468, 'seek': 360464, 'start': 3604.64, 'end': 3610.8799999999997, 'text': \" what we do is have a sentence I'll give a talk tomorrow on your networks and what we have to do\", 'tokens': [437, 321, 360, 307, 362, 257, 8174, 286, 603, 976, 257, 751, 4153, 322, 428, 9590, 293, 437, 321, 362, 281, 360], 'temperature': 0.0, 'avg_logprob': -0.06642882029215495, 'compression_ratio': 1.9690721649484537, 'no_speech_prob': 0.0001015853849821724}, {'id': 469, 'seek': 360464, 'start': 3610.8799999999997, 'end': 3617.04, 'text': \" is say for every word in that sentence we have to choose some other word that it's a dependent of\", 'tokens': [307, 584, 337, 633, 1349, 294, 300, 8174, 321, 362, 281, 2826, 512, 661, 1349, 300, 309, 311, 257, 12334, 295], 'temperature': 0.0, 'avg_logprob': -0.06642882029215495, 'compression_ratio': 1.9690721649484537, 'no_speech_prob': 0.0001015853849821724}, {'id': 470, 'seek': 360464, 'start': 3617.7599999999998, 'end': 3625.04, 'text': \" where one possibility is it's a dependent of root so we're giving it a structure where we're\", 'tokens': [689, 472, 7959, 307, 309, 311, 257, 12334, 295, 5593, 370, 321, 434, 2902, 309, 257, 3877, 689, 321, 434], 'temperature': 0.0, 'avg_logprob': -0.06642882029215495, 'compression_ratio': 1.9690721649484537, 'no_speech_prob': 0.0001015853849821724}, {'id': 471, 'seek': 360464, 'start': 3625.04, 'end': 3633.44, 'text': \" saying okay for this word I've decided that it's a dependent on networks and then for this word\", 'tokens': [1566, 1392, 337, 341, 1349, 286, 600, 3047, 300, 309, 311, 257, 12334, 322, 9590, 293, 550, 337, 341, 1349], 'temperature': 0.0, 'avg_logprob': -0.06642882029215495, 'compression_ratio': 1.9690721649484537, 'no_speech_prob': 0.0001015853849821724}, {'id': 472, 'seek': 363344, 'start': 3633.44, 'end': 3644.64, 'text': \" it's also a dependent on networks and for this word it's a dependent on give so we're choosing\", 'tokens': [309, 311, 611, 257, 12334, 322, 9590, 293, 337, 341, 1349, 309, 311, 257, 12334, 322, 976, 370, 321, 434, 10875], 'temperature': 0.0, 'avg_logprob': -0.08050050606598726, 'compression_ratio': 1.8417721518987342, 'no_speech_prob': 3.266153362346813e-05}, {'id': 473, 'seek': 363344, 'start': 3645.68, 'end': 3653.2000000000003, 'text': ' one for each word and there are usually a few constraints so only one word is a dependent of root', 'tokens': [472, 337, 1184, 1349, 293, 456, 366, 2673, 257, 1326, 18491, 370, 787, 472, 1349, 307, 257, 12334, 295, 5593], 'temperature': 0.0, 'avg_logprob': -0.08050050606598726, 'compression_ratio': 1.8417721518987342, 'no_speech_prob': 3.266153362346813e-05}, {'id': 474, 'seek': 363344, 'start': 3653.2000000000003, 'end': 3660.2400000000002, 'text': \" we have a tree we don't want cycles so we don't want to say that word a is dependent on word b and\", 'tokens': [321, 362, 257, 4230, 321, 500, 380, 528, 17796, 370, 321, 500, 380, 528, 281, 584, 300, 1349, 257, 307, 12334, 322, 1349, 272, 293], 'temperature': 0.0, 'avg_logprob': -0.08050050606598726, 'compression_ratio': 1.8417721518987342, 'no_speech_prob': 3.266153362346813e-05}, {'id': 475, 'seek': 366024, 'start': 3660.24, 'end': 3671.52, 'text': \" word b is dependent on word a and then there's one final issue which is whether arrows can cross\", 'tokens': [1349, 272, 307, 12334, 322, 1349, 257, 293, 550, 456, 311, 472, 2572, 2734, 597, 307, 1968, 19669, 393, 3278], 'temperature': 0.0, 'avg_logprob': -0.08954079945882161, 'compression_ratio': 1.6132596685082874, 'no_speech_prob': 4.319893196225166e-05}, {'id': 476, 'seek': 366024, 'start': 3671.52, 'end': 3678.56, 'text': ' or not so in this particular sentence we actually have these crossing dependencies you can see there', 'tokens': [420, 406, 370, 294, 341, 1729, 8174, 321, 767, 362, 613, 14712, 36606, 291, 393, 536, 456], 'temperature': 0.0, 'avg_logprob': -0.08954079945882161, 'compression_ratio': 1.6132596685082874, 'no_speech_prob': 4.319893196225166e-05}, {'id': 477, 'seek': 366024, 'start': 3678.56, 'end': 3685.52, 'text': \" I'll give a talk tomorrow on neural networks and this is the correct dependency paths for this\", 'tokens': [286, 603, 976, 257, 751, 4153, 322, 18161, 9590, 293, 341, 307, 264, 3006, 33621, 14518, 337, 341], 'temperature': 0.0, 'avg_logprob': -0.08954079945882161, 'compression_ratio': 1.6132596685082874, 'no_speech_prob': 4.319893196225166e-05}, {'id': 478, 'seek': 368552, 'start': 3685.52, 'end': 3692.0, 'text': \" sentence because what we have here is that it's a talk and it's a talk on neural networks so the\", 'tokens': [8174, 570, 437, 321, 362, 510, 307, 300, 309, 311, 257, 751, 293, 309, 311, 257, 751, 322, 18161, 9590, 370, 264], 'temperature': 0.0, 'avg_logprob': -0.08830579651726617, 'compression_ratio': 1.875598086124402, 'no_speech_prob': 8.851144229993224e-05}, {'id': 479, 'seek': 368552, 'start': 3692.0, 'end': 3699.6, 'text': \" on neural networks modifies the talk but which leads to these crossing dependencies I didn't have to\", 'tokens': [322, 18161, 9590, 1072, 11221, 264, 751, 457, 597, 6689, 281, 613, 14712, 36606, 286, 994, 380, 362, 281], 'temperature': 0.0, 'avg_logprob': -0.08830579651726617, 'compression_ratio': 1.875598086124402, 'no_speech_prob': 8.851144229993224e-05}, {'id': 480, 'seek': 368552, 'start': 3699.6, 'end': 3706.08, 'text': \" say it like that I could have said I'll give a talk on neural networks tomorrow and then on your\", 'tokens': [584, 309, 411, 300, 286, 727, 362, 848, 286, 603, 976, 257, 751, 322, 18161, 9590, 4153, 293, 550, 322, 428], 'temperature': 0.0, 'avg_logprob': -0.08830579651726617, 'compression_ratio': 1.875598086124402, 'no_speech_prob': 8.851144229993224e-05}, {'id': 481, 'seek': 368552, 'start': 3706.08, 'end': 3715.04, 'text': ' networks would be next to the talk so most of the time in languages dependencies are projector of', 'tokens': [9590, 576, 312, 958, 281, 264, 751, 370, 881, 295, 264, 565, 294, 8650, 36606, 366, 39792, 295], 'temperature': 0.0, 'avg_logprob': -0.08830579651726617, 'compression_ratio': 1.875598086124402, 'no_speech_prob': 8.851144229993224e-05}, {'id': 482, 'seek': 371504, 'start': 3715.04, 'end': 3721.68, 'text': ' the things stay together so the dependencies have a kind of a nesting structure of the kind that', 'tokens': [264, 721, 1754, 1214, 370, 264, 36606, 362, 257, 733, 295, 257, 297, 8714, 3877, 295, 264, 733, 300], 'temperature': 0.0, 'avg_logprob': -0.0708698172901952, 'compression_ratio': 1.691304347826087, 'no_speech_prob': 7.936338079161942e-05}, {'id': 483, 'seek': 371504, 'start': 3721.68, 'end': 3728.72, 'text': ' you also see in context free grammars but most languages have at least a few phenomena where you', 'tokens': [291, 611, 536, 294, 4319, 1737, 17570, 685, 457, 881, 8650, 362, 412, 1935, 257, 1326, 22004, 689, 291], 'temperature': 0.0, 'avg_logprob': -0.0708698172901952, 'compression_ratio': 1.691304347826087, 'no_speech_prob': 7.936338079161942e-05}, {'id': 484, 'seek': 371504, 'start': 3728.72, 'end': 3736.48, 'text': ' ended up with these ability for phrases to be split apart which lead to non-projective dependencies', 'tokens': [4590, 493, 365, 613, 3485, 337, 20312, 281, 312, 7472, 4936, 597, 1477, 281, 2107, 12, 4318, 1020, 488, 36606], 'temperature': 0.0, 'avg_logprob': -0.0708698172901952, 'compression_ratio': 1.691304347826087, 'no_speech_prob': 7.936338079161942e-05}, {'id': 485, 'seek': 371504, 'start': 3736.48, 'end': 3743.68, 'text': ' so in particular one of them in English is that you can take modifying phrases and clauses like', 'tokens': [370, 294, 1729, 472, 295, 552, 294, 3669, 307, 300, 291, 393, 747, 42626, 20312, 293, 49072, 411], 'temperature': 0.0, 'avg_logprob': -0.0708698172901952, 'compression_ratio': 1.691304347826087, 'no_speech_prob': 7.936338079161942e-05}, {'id': 486, 'seek': 374368, 'start': 3743.68, 'end': 3749.44, 'text': \" the on neural networks here and shift them right towards the end of the sentence and get I'll give\", 'tokens': [264, 322, 18161, 9590, 510, 293, 5513, 552, 558, 3030, 264, 917, 295, 264, 8174, 293, 483, 286, 603, 976], 'temperature': 0.0, 'avg_logprob': -0.12281852544740189, 'compression_ratio': 1.7729468599033817, 'no_speech_prob': 4.74769294669386e-05}, {'id': 487, 'seek': 374368, 'start': 3749.44, 'end': 3755.12, 'text': ' a talk tomorrow on neural networks and that then leads to non-projective sentences', 'tokens': [257, 751, 4153, 322, 18161, 9590, 293, 300, 550, 6689, 281, 2107, 12, 4318, 1020, 488, 16579], 'temperature': 0.0, 'avg_logprob': -0.12281852544740189, 'compression_ratio': 1.7729468599033817, 'no_speech_prob': 4.74769294669386e-05}, {'id': 488, 'seek': 374368, 'start': 3757.6, 'end': 3763.3599999999997, 'text': ' so a pause is projected if there are no crossing dependency arcs when the words are laid out', 'tokens': [370, 257, 10465, 307, 26231, 498, 456, 366, 572, 14712, 33621, 10346, 82, 562, 264, 2283, 366, 9897, 484], 'temperature': 0.0, 'avg_logprob': -0.12281852544740189, 'compression_ratio': 1.7729468599033817, 'no_speech_prob': 4.74769294669386e-05}, {'id': 489, 'seek': 374368, 'start': 3763.3599999999997, 'end': 3770.0, 'text': ' and then in your order with all arcs above the words and if you have a dependency paths that', 'tokens': [293, 550, 294, 428, 1668, 365, 439, 10346, 82, 3673, 264, 2283, 293, 498, 291, 362, 257, 33621, 14518, 300], 'temperature': 0.0, 'avg_logprob': -0.12281852544740189, 'compression_ratio': 1.7729468599033817, 'no_speech_prob': 4.74769294669386e-05}, {'id': 490, 'seek': 377000, 'start': 3770.0, 'end': 3775.84, 'text': ' correspond to a context free grammar tree it actually has to be protective because context free', 'tokens': [6805, 281, 257, 4319, 1737, 22317, 4230, 309, 767, 575, 281, 312, 16314, 570, 4319, 1737], 'temperature': 0.0, 'avg_logprob': -0.0942164711330248, 'compression_ratio': 1.7035175879396984, 'no_speech_prob': 2.1087949789944105e-05}, {'id': 491, 'seek': 377000, 'start': 3775.84, 'end': 3781.84, 'text': ' grammars necessarily have this sort of nested tree structure following the linear order', 'tokens': [17570, 685, 4725, 362, 341, 1333, 295, 15646, 292, 4230, 3877, 3480, 264, 8213, 1668], 'temperature': 0.0, 'avg_logprob': -0.0942164711330248, 'compression_ratio': 1.7035175879396984, 'no_speech_prob': 2.1087949789944105e-05}, {'id': 492, 'seek': 377000, 'start': 3782.88, 'end': 3788.8, 'text': ' but dependency grammars normally allow non-projective structures to account for', 'tokens': [457, 33621, 17570, 685, 5646, 2089, 2107, 12, 4318, 1020, 488, 9227, 281, 2696, 337], 'temperature': 0.0, 'avg_logprob': -0.0942164711330248, 'compression_ratio': 1.7035175879396984, 'no_speech_prob': 2.1087949789944105e-05}, {'id': 493, 'seek': 377000, 'start': 3788.8, 'end': 3794.16, 'text': \" displacement constituents and you can't easily get the semantics of certain\", 'tokens': [21899, 30847, 293, 291, 393, 380, 3612, 483, 264, 4361, 45298, 295, 1629], 'temperature': 0.0, 'avg_logprob': -0.0942164711330248, 'compression_ratio': 1.7035175879396984, 'no_speech_prob': 2.1087949789944105e-05}, {'id': 494, 'seek': 379416, 'start': 3794.16, 'end': 3800.8799999999997, 'text': \" constructions right without these non-projective dependencies so here's another example in English\", 'tokens': [7690, 626, 558, 1553, 613, 2107, 12, 4318, 1020, 488, 36606, 370, 510, 311, 1071, 1365, 294, 3669], 'temperature': 0.0, 'avg_logprob': -0.09513291858491443, 'compression_ratio': 1.5401069518716577, 'no_speech_prob': 7.930669380584732e-05}, {'id': 495, 'seek': 379416, 'start': 3800.8799999999997, 'end': 3808.0, 'text': \" with question formation with what's called preposition stranding so the sentence is who did\", 'tokens': [365, 1168, 11723, 365, 437, 311, 1219, 2666, 5830, 14955, 278, 370, 264, 8174, 307, 567, 630], 'temperature': 0.0, 'avg_logprob': -0.09513291858491443, 'compression_ratio': 1.5401069518716577, 'no_speech_prob': 7.930669380584732e-05}, {'id': 496, 'seek': 379416, 'start': 3808.0, 'end': 3814.8799999999997, 'text': \" bill by the coffee from yesterday there's another way I could have said this it's less natural in\", 'tokens': [2961, 538, 264, 4982, 490, 5186, 456, 311, 1071, 636, 286, 727, 362, 848, 341, 309, 311, 1570, 3303, 294], 'temperature': 0.0, 'avg_logprob': -0.09513291858491443, 'compression_ratio': 1.5401069518716577, 'no_speech_prob': 7.930669380584732e-05}, {'id': 497, 'seek': 381488, 'start': 3814.88, 'end': 3825.52, 'text': ' English but I could have said from who did bill by the coffee yesterday in many languages of the', 'tokens': [3669, 457, 286, 727, 362, 848, 490, 567, 630, 2961, 538, 264, 4982, 5186, 294, 867, 8650, 295, 264], 'temperature': 0.0, 'avg_logprob': -0.07913357661320614, 'compression_ratio': 1.6045197740112995, 'no_speech_prob': 0.00012471424997784197}, {'id': 498, 'seek': 381488, 'start': 3825.52, 'end': 3833.36, 'text': \" world that's the only way you could have said it and when you do that from who is kept together\", 'tokens': [1002, 300, 311, 264, 787, 636, 291, 727, 362, 848, 309, 293, 562, 291, 360, 300, 490, 567, 307, 4305, 1214], 'temperature': 0.0, 'avg_logprob': -0.07913357661320614, 'compression_ratio': 1.6045197740112995, 'no_speech_prob': 0.00012471424997784197}, {'id': 499, 'seek': 381488, 'start': 3833.36, 'end': 3839.92, 'text': ' and you have a projective pause for the sentence but English allows and indeed much prefers', 'tokens': [293, 291, 362, 257, 1716, 488, 10465, 337, 264, 8174, 457, 3669, 4045, 293, 6451, 709, 44334], 'temperature': 0.0, 'avg_logprob': -0.07913357661320614, 'compression_ratio': 1.6045197740112995, 'no_speech_prob': 0.00012471424997784197}, {'id': 500, 'seek': 383992, 'start': 3839.92, 'end': 3847.28, 'text': ' you to do what is referred to as preposition stranding where you move the who but you just leave', 'tokens': [291, 281, 360, 437, 307, 10839, 281, 382, 2666, 5830, 14955, 278, 689, 291, 1286, 264, 567, 457, 291, 445, 1856], 'temperature': 0.0, 'avg_logprob': -0.10702867186471318, 'compression_ratio': 1.6542056074766356, 'no_speech_prob': 0.00012282541138119996}, {'id': 501, 'seek': 383992, 'start': 3847.28, 'end': 3854.4, 'text': ' the preposition behind and so you get who did bill by the coffee from yesterday and so then', 'tokens': [264, 2666, 5830, 2261, 293, 370, 291, 483, 567, 630, 2961, 538, 264, 4982, 490, 5186, 293, 370, 550], 'temperature': 0.0, 'avg_logprob': -0.10702867186471318, 'compression_ratio': 1.6542056074766356, 'no_speech_prob': 0.00012282541138119996}, {'id': 502, 'seek': 383992, 'start': 3854.4, 'end': 3859.52, 'text': \" we're ending up with this non-projective dependency structure as I've shown there\", 'tokens': [321, 434, 8121, 493, 365, 341, 2107, 12, 4318, 1020, 488, 33621, 3877, 382, 286, 600, 4898, 456], 'temperature': 0.0, 'avg_logprob': -0.10702867186471318, 'compression_ratio': 1.6542056074766356, 'no_speech_prob': 0.00012282541138119996}, {'id': 503, 'seek': 383992, 'start': 3861.92, 'end': 3868.4, 'text': \" okay I'll come back to non-projectivity in a little bit how do we go about building\", 'tokens': [1392, 286, 603, 808, 646, 281, 2107, 12, 4318, 1020, 4253, 294, 257, 707, 857, 577, 360, 321, 352, 466, 2390], 'temperature': 0.0, 'avg_logprob': -0.10702867186471318, 'compression_ratio': 1.6542056074766356, 'no_speech_prob': 0.00012282541138119996}, {'id': 504, 'seek': 386840, 'start': 3868.4, 'end': 3876.08, 'text': ' dependency parsers well there are a whole bunch of ways that you can build dependency parsers', 'tokens': [33621, 21156, 433, 731, 456, 366, 257, 1379, 3840, 295, 2098, 300, 291, 393, 1322, 33621, 21156, 433], 'temperature': 0.0, 'avg_logprob': -0.08289549485692438, 'compression_ratio': 1.891566265060241, 'no_speech_prob': 0.00015828058531042188}, {'id': 505, 'seek': 386840, 'start': 3876.8, 'end': 3882.32, 'text': \" very quickly I'll just say a few names and I'll tell you about one of them so you can use dynamic\", 'tokens': [588, 2661, 286, 603, 445, 584, 257, 1326, 5288, 293, 286, 603, 980, 291, 466, 472, 295, 552, 370, 291, 393, 764, 8546], 'temperature': 0.0, 'avg_logprob': -0.08289549485692438, 'compression_ratio': 1.891566265060241, 'no_speech_prob': 0.00015828058531042188}, {'id': 506, 'seek': 386840, 'start': 3882.32, 'end': 3888.2400000000002, 'text': ' programming methods to build dependency parsers so I showed earlier that you can have an exponential', 'tokens': [9410, 7150, 281, 1322, 33621, 21156, 433, 370, 286, 4712, 3071, 300, 291, 393, 362, 364, 21510], 'temperature': 0.0, 'avg_logprob': -0.08289549485692438, 'compression_ratio': 1.891566265060241, 'no_speech_prob': 0.00015828058531042188}, {'id': 507, 'seek': 386840, 'start': 3888.2400000000002, 'end': 3893.36, 'text': ' number of parsers for a sentence and that sounds like really bad news for building a system', 'tokens': [1230, 295, 21156, 433, 337, 257, 8174, 293, 300, 3263, 411, 534, 1578, 2583, 337, 2390, 257, 1185], 'temperature': 0.0, 'avg_logprob': -0.08289549485692438, 'compression_ratio': 1.891566265060241, 'no_speech_prob': 0.00015828058531042188}, {'id': 508, 'seek': 389336, 'start': 3893.36, 'end': 3898.96, 'text': ' well it turns out that you can be clever and you can work out a way to dynamic program finding', 'tokens': [731, 309, 4523, 484, 300, 291, 393, 312, 13494, 293, 291, 393, 589, 484, 257, 636, 281, 8546, 1461, 5006], 'temperature': 0.0, 'avg_logprob': -0.11366433682649033, 'compression_ratio': 1.742081447963801, 'no_speech_prob': 3.421109795453958e-05}, {'id': 509, 'seek': 389336, 'start': 3898.96, 'end': 3905.6800000000003, 'text': ' that exponential number of parsers and then you can have an oh and cubed algorithm so you could do that', 'tokens': [300, 21510, 1230, 295, 21156, 433, 293, 550, 291, 393, 362, 364, 1954, 293, 36510, 9284, 370, 291, 727, 360, 300], 'temperature': 0.0, 'avg_logprob': -0.11366433682649033, 'compression_ratio': 1.742081447963801, 'no_speech_prob': 3.421109795453958e-05}, {'id': 510, 'seek': 389336, 'start': 3907.2000000000003, 'end': 3913.52, 'text': \" you can use graph algorithms and I'll say a bit about that later by that may spill into next time\", 'tokens': [291, 393, 764, 4295, 14642, 293, 286, 603, 584, 257, 857, 466, 300, 1780, 538, 300, 815, 22044, 666, 958, 565], 'temperature': 0.0, 'avg_logprob': -0.11366433682649033, 'compression_ratio': 1.742081447963801, 'no_speech_prob': 3.421109795453958e-05}, {'id': 511, 'seek': 389336, 'start': 3914.4, 'end': 3922.2400000000002, 'text': \" so you can see since we're wanting to kind of connect up all the words into a tree using\", 'tokens': [370, 291, 393, 536, 1670, 321, 434, 7935, 281, 733, 295, 1745, 493, 439, 264, 2283, 666, 257, 4230, 1228], 'temperature': 0.0, 'avg_logprob': -0.11366433682649033, 'compression_ratio': 1.742081447963801, 'no_speech_prob': 3.421109795453958e-05}, {'id': 512, 'seek': 392224, 'start': 3922.24, 'end': 3927.9199999999996, 'text': ' graph edges that you could think of doing that using using a minimum spanning tree algorithm of', 'tokens': [4295, 8819, 300, 291, 727, 519, 295, 884, 300, 1228, 1228, 257, 7285, 47626, 4230, 9284, 295], 'temperature': 0.0, 'avg_logprob': -0.11467852872960707, 'compression_ratio': 1.6877828054298643, 'no_speech_prob': 5.998637789161876e-05}, {'id': 513, 'seek': 392224, 'start': 3927.9199999999996, 'end': 3934.08, 'text': ' the sort that you hopefully saw in CS 161 and so that idea has been used for parsing', 'tokens': [264, 1333, 300, 291, 4696, 1866, 294, 9460, 3165, 16, 293, 370, 300, 1558, 575, 668, 1143, 337, 21156, 278], 'temperature': 0.0, 'avg_logprob': -0.11467852872960707, 'compression_ratio': 1.6877828054298643, 'no_speech_prob': 5.998637789161876e-05}, {'id': 514, 'seek': 392224, 'start': 3934.8799999999997, 'end': 3941.4399999999996, 'text': ' constraint satisfaction ideas that you might have seen in CS 221 have been used for dependency parsing', 'tokens': [25534, 18715, 3487, 300, 291, 1062, 362, 1612, 294, 9460, 5853, 16, 362, 668, 1143, 337, 33621, 21156, 278], 'temperature': 0.0, 'avg_logprob': -0.11467852872960707, 'compression_ratio': 1.6877828054298643, 'no_speech_prob': 5.998637789161876e-05}, {'id': 515, 'seek': 392224, 'start': 3943.04, 'end': 3948.72, 'text': \" but the way I'm going to show now is transition based parsing or sometimes referred to as\", 'tokens': [457, 264, 636, 286, 478, 516, 281, 855, 586, 307, 6034, 2361, 21156, 278, 420, 2171, 10839, 281, 382], 'temperature': 0.0, 'avg_logprob': -0.11467852872960707, 'compression_ratio': 1.6877828054298643, 'no_speech_prob': 5.998637789161876e-05}, {'id': 516, 'seek': 394872, 'start': 3948.72, 'end': 3957.7599999999998, 'text': ' deterministic dependency parsing and the idea of this is once going to use a transition system', 'tokens': [15957, 3142, 33621, 21156, 278, 293, 264, 1558, 295, 341, 307, 1564, 516, 281, 764, 257, 6034, 1185], 'temperature': 0.0, 'avg_logprob': -0.1100319662401753, 'compression_ratio': 1.7329192546583851, 'no_speech_prob': 2.277178100484889e-05}, {'id': 517, 'seek': 394872, 'start': 3957.7599999999998, 'end': 3964.56, 'text': \" so that's like shift reduce parsing if you've seen shift reduce parsing in something like a\", 'tokens': [370, 300, 311, 411, 5513, 5407, 21156, 278, 498, 291, 600, 1612, 5513, 5407, 21156, 278, 294, 746, 411, 257], 'temperature': 0.0, 'avg_logprob': -0.1100319662401753, 'compression_ratio': 1.7329192546583851, 'no_speech_prob': 2.277178100484889e-05}, {'id': 518, 'seek': 394872, 'start': 3964.56, 'end': 3971.68, 'text': \" compiler's class or formal languages class that shift and reduce transition steps and so use\", 'tokens': [31958, 311, 1508, 420, 9860, 8650, 1508, 300, 5513, 293, 5407, 6034, 4439, 293, 370, 764], 'temperature': 0.0, 'avg_logprob': -0.1100319662401753, 'compression_ratio': 1.7329192546583851, 'no_speech_prob': 2.277178100484889e-05}, {'id': 519, 'seek': 397168, 'start': 3971.68, 'end': 3980.24, 'text': ' a transition system to guide the construction of parsers and so let me just explain about that', 'tokens': [257, 6034, 1185, 281, 5934, 264, 6435, 295, 21156, 433, 293, 370, 718, 385, 445, 2903, 466, 300], 'temperature': 0.0, 'avg_logprob': -0.18746671368998866, 'compression_ratio': 1.5375722543352601, 'no_speech_prob': 3.456545891822316e-05}, {'id': 520, 'seek': 397168, 'start': 3981.68, 'end': 3993.9199999999996, 'text': \" so let's see so this was an idea that was made prominent by Yorkin Nivre who's a Swedish\", 'tokens': [370, 718, 311, 536, 370, 341, 390, 364, 1558, 300, 390, 1027, 17034, 538, 398, 1284, 259, 426, 592, 265, 567, 311, 257, 23523], 'temperature': 0.0, 'avg_logprob': -0.18746671368998866, 'compression_ratio': 1.5375722543352601, 'no_speech_prob': 3.456545891822316e-05}, {'id': 521, 'seek': 399392, 'start': 3993.92, 'end': 4003.84, 'text': ' computational linguist who introduced this idea of greedy transition based parsing so his idea is', 'tokens': [28270, 21766, 468, 567, 7268, 341, 1558, 295, 28228, 6034, 2361, 21156, 278, 370, 702, 1558, 307], 'temperature': 0.0, 'avg_logprob': -0.11558825934111182, 'compression_ratio': 1.7058823529411764, 'no_speech_prob': 4.680592246586457e-05}, {'id': 522, 'seek': 399392, 'start': 4003.84, 'end': 4010.7200000000003, 'text': \" well what we're going to do for dependency parsing is we're going to be able to parse sentences\", 'tokens': [731, 437, 321, 434, 516, 281, 360, 337, 33621, 21156, 278, 307, 321, 434, 516, 281, 312, 1075, 281, 48377, 16579], 'temperature': 0.0, 'avg_logprob': -0.11558825934111182, 'compression_ratio': 1.7058823529411764, 'no_speech_prob': 4.680592246586457e-05}, {'id': 523, 'seek': 399392, 'start': 4010.7200000000003, 'end': 4017.92, 'text': \" by having a set of transitions which are kind of like shift reduce parser and it's going to just\", 'tokens': [538, 1419, 257, 992, 295, 23767, 597, 366, 733, 295, 411, 5513, 5407, 21156, 260, 293, 309, 311, 516, 281, 445], 'temperature': 0.0, 'avg_logprob': -0.11558825934111182, 'compression_ratio': 1.7058823529411764, 'no_speech_prob': 4.680592246586457e-05}, {'id': 524, 'seek': 401792, 'start': 4017.92, 'end': 4026.48, 'text': \" work left to right bottom up and parse a sentence so we're going to say we have a stack sigma\", 'tokens': [589, 1411, 281, 558, 2767, 493, 293, 48377, 257, 8174, 370, 321, 434, 516, 281, 584, 321, 362, 257, 8630, 12771], 'temperature': 0.0, 'avg_logprob': -0.10473262304547189, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 4.750000516651198e-05}, {'id': 525, 'seek': 401792, 'start': 4027.36, 'end': 4033.6800000000003, 'text': \" buffer beta of the words that we have to process and we're going to build up a set of dependency\", 'tokens': [21762, 9861, 295, 264, 2283, 300, 321, 362, 281, 1399, 293, 321, 434, 516, 281, 1322, 493, 257, 992, 295, 33621], 'temperature': 0.0, 'avg_logprob': -0.10473262304547189, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 4.750000516651198e-05}, {'id': 526, 'seek': 401792, 'start': 4033.6800000000003, 'end': 4040.88, 'text': ' arcs by using actions which are shift and reduce actions and putting those together this will give', 'tokens': [10346, 82, 538, 1228, 5909, 597, 366, 5513, 293, 5407, 5909, 293, 3372, 729, 1214, 341, 486, 976], 'temperature': 0.0, 'avg_logprob': -0.10473262304547189, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 4.750000516651198e-05}, {'id': 527, 'seek': 401792, 'start': 4040.88, 'end': 4047.76, 'text': ' us the ability to put parse structures over sentences and let me go through the details of', 'tokens': [505, 264, 3485, 281, 829, 48377, 9227, 670, 16579, 293, 718, 385, 352, 807, 264, 4365, 295], 'temperature': 0.0, 'avg_logprob': -0.10473262304547189, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 4.750000516651198e-05}, {'id': 528, 'seek': 404776, 'start': 4047.76, 'end': 4054.88, 'text': \" this and this is a little bit hairy when you first see it that's not so complex really and\", 'tokens': [341, 293, 341, 307, 257, 707, 857, 42346, 562, 291, 700, 536, 309, 300, 311, 406, 370, 3997, 534, 293], 'temperature': 0.0, 'avg_logprob': -0.10591143019059125, 'compression_ratio': 1.6608187134502923, 'no_speech_prob': 2.0771629351656884e-05}, {'id': 529, 'seek': 404776, 'start': 4055.6800000000003, 'end': 4064.7200000000003, 'text': \" it's this kind of transition based dependency parser is what we'll use in assignment 3 so what we\", 'tokens': [309, 311, 341, 733, 295, 6034, 2361, 33621, 21156, 260, 307, 437, 321, 603, 764, 294, 15187, 805, 370, 437, 321], 'temperature': 0.0, 'avg_logprob': -0.10591143019059125, 'compression_ratio': 1.6608187134502923, 'no_speech_prob': 2.0771629351656884e-05}, {'id': 530, 'seek': 404776, 'start': 4064.7200000000003, 'end': 4071.6000000000004, 'text': ' have so this is our transition system we have a starting point where we start with a stack that', 'tokens': [362, 370, 341, 307, 527, 6034, 1185, 321, 362, 257, 2891, 935, 689, 321, 722, 365, 257, 8630, 300], 'temperature': 0.0, 'avg_logprob': -0.10591143019059125, 'compression_ratio': 1.6608187134502923, 'no_speech_prob': 2.0771629351656884e-05}, {'id': 531, 'seek': 407160, 'start': 4071.6, 'end': 4077.7599999999998, 'text': \" just has the root symbol on it and a buffer that has the sentence that's about to parse we're about\", 'tokens': [445, 575, 264, 5593, 5986, 322, 309, 293, 257, 21762, 300, 575, 264, 8174, 300, 311, 466, 281, 48377, 321, 434, 466], 'temperature': 0.0, 'avg_logprob': -0.06056543245707473, 'compression_ratio': 1.716763005780347, 'no_speech_prob': 1.804069324862212e-05}, {'id': 532, 'seek': 407160, 'start': 4077.7599999999998, 'end': 4087.2, 'text': \" to parse and so far we haven't built any dependency arcs and so at each point in time we can choose one\", 'tokens': [281, 48377, 293, 370, 1400, 321, 2378, 380, 3094, 604, 33621, 10346, 82, 293, 370, 412, 1184, 935, 294, 565, 321, 393, 2826, 472], 'temperature': 0.0, 'avg_logprob': -0.06056543245707473, 'compression_ratio': 1.716763005780347, 'no_speech_prob': 1.804069324862212e-05}, {'id': 533, 'seek': 407160, 'start': 4087.2, 'end': 4099.68, 'text': ' of three actions we can shift which moves the next word onto the stack we can then do actions', 'tokens': [295, 1045, 5909, 321, 393, 5513, 597, 6067, 264, 958, 1349, 3911, 264, 8630, 321, 393, 550, 360, 5909], 'temperature': 0.0, 'avg_logprob': -0.06056543245707473, 'compression_ratio': 1.716763005780347, 'no_speech_prob': 1.804069324862212e-05}, {'id': 534, 'seek': 409968, 'start': 4099.68, 'end': 4106.8, 'text': ' that are the reduce actions so there are two reduce actions to make it a dependency grammar we', 'tokens': [300, 366, 264, 5407, 5909, 370, 456, 366, 732, 5407, 5909, 281, 652, 309, 257, 33621, 22317, 321], 'temperature': 0.0, 'avg_logprob': -0.05665230751037598, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 6.748691521352157e-05}, {'id': 535, 'seek': 409968, 'start': 4106.8, 'end': 4114.8, 'text': ' can either do a left arc reduce or a right arc reduce so when we do either of those we take', 'tokens': [393, 2139, 360, 257, 1411, 10346, 5407, 420, 257, 558, 10346, 5407, 370, 562, 321, 360, 2139, 295, 729, 321, 747], 'temperature': 0.0, 'avg_logprob': -0.05665230751037598, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 6.748691521352157e-05}, {'id': 536, 'seek': 409968, 'start': 4114.8, 'end': 4122.64, 'text': ' the top two items on the stack and we make one of them a dependent of the other one so we can', 'tokens': [264, 1192, 732, 4754, 322, 264, 8630, 293, 321, 652, 472, 295, 552, 257, 12334, 295, 264, 661, 472, 370, 321, 393], 'temperature': 0.0, 'avg_logprob': -0.05665230751037598, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 6.748691521352157e-05}, {'id': 537, 'seek': 412264, 'start': 4122.64, 'end': 4130.400000000001, 'text': \" either say okay let's make wi a dependent of wj or else we can say okay let's make wj a dependent\", 'tokens': [2139, 584, 1392, 718, 311, 652, 26393, 257, 12334, 295, 261, 73, 420, 1646, 321, 393, 584, 1392, 718, 311, 652, 261, 73, 257, 12334], 'temperature': 0.0, 'avg_logprob': -0.06364646710847553, 'compression_ratio': 1.8774193548387097, 'no_speech_prob': 5.7215125707443804e-05}, {'id': 538, 'seek': 412264, 'start': 4130.400000000001, 'end': 4140.400000000001, 'text': \" of wi and so the result of when we do that is the one that's the dependent disappears from the stack\", 'tokens': [295, 26393, 293, 370, 264, 1874, 295, 562, 321, 360, 300, 307, 264, 472, 300, 311, 264, 12334, 25527, 490, 264, 8630], 'temperature': 0.0, 'avg_logprob': -0.06364646710847553, 'compression_ratio': 1.8774193548387097, 'no_speech_prob': 5.7215125707443804e-05}, {'id': 539, 'seek': 412264, 'start': 4140.400000000001, 'end': 4147.52, 'text': \" and so in the stacks over here there's one less item but then we add a dependency arc to our\", 'tokens': [293, 370, 294, 264, 30792, 670, 510, 456, 311, 472, 1570, 3174, 457, 550, 321, 909, 257, 33621, 10346, 281, 527], 'temperature': 0.0, 'avg_logprob': -0.06364646710847553, 'compression_ratio': 1.8774193548387097, 'no_speech_prob': 5.7215125707443804e-05}, {'id': 540, 'seek': 414752, 'start': 4147.52, 'end': 4154.72, 'text': \" arc set so that we say that we've got either a dependency from j to i or a dependency from i to j\", 'tokens': [10346, 992, 370, 300, 321, 584, 300, 321, 600, 658, 2139, 257, 33621, 490, 361, 281, 741, 420, 257, 33621, 490, 741, 281, 361], 'temperature': 0.0, 'avg_logprob': -0.06709837212282069, 'compression_ratio': 1.6647398843930636, 'no_speech_prob': 3.7617974157910794e-05}, {'id': 541, 'seek': 414752, 'start': 4155.4400000000005, 'end': 4162.72, 'text': ' and commonly when we do this we actually also specify what grammatical relation connects the two', 'tokens': [293, 12719, 562, 321, 360, 341, 321, 767, 611, 16500, 437, 17570, 267, 804, 9721, 16967, 264, 732], 'temperature': 0.0, 'avg_logprob': -0.06709837212282069, 'compression_ratio': 1.6647398843930636, 'no_speech_prob': 3.7617974157910794e-05}, {'id': 542, 'seek': 414752, 'start': 4162.72, 'end': 4171.280000000001, 'text': \" such as subject object now modifier and so we also have here a relation that's still probably\", 'tokens': [1270, 382, 3983, 2657, 586, 38011, 293, 370, 321, 611, 362, 510, 257, 9721, 300, 311, 920, 1391], 'temperature': 0.0, 'avg_logprob': -0.06709837212282069, 'compression_ratio': 1.6647398843930636, 'no_speech_prob': 3.7617974157910794e-05}, {'id': 543, 'seek': 417128, 'start': 4171.28, 'end': 4180.16, 'text': \" still very abstract so let's go through an example so this is how a simple transition based dependency\", 'tokens': [920, 588, 12649, 370, 718, 311, 352, 807, 364, 1365, 370, 341, 307, 577, 257, 2199, 6034, 2361, 33621], 'temperature': 0.0, 'avg_logprob': -0.07790076595613327, 'compression_ratio': 1.8202764976958525, 'no_speech_prob': 4.673379589803517e-05}, {'id': 544, 'seek': 417128, 'start': 4180.16, 'end': 4186.24, 'text': \" parser what's referred to as an arc standard transition based dependency parser would parse up i8\", 'tokens': [21156, 260, 437, 311, 10839, 281, 382, 364, 10346, 3832, 6034, 2361, 33621, 21156, 260, 576, 48377, 493, 741, 23], 'temperature': 0.0, 'avg_logprob': -0.07790076595613327, 'compression_ratio': 1.8202764976958525, 'no_speech_prob': 4.673379589803517e-05}, {'id': 545, 'seek': 417128, 'start': 4186.24, 'end': 4192.32, 'text': ' the fish so remember these are the different operations that we can apply so to start off with we', 'tokens': [264, 3506, 370, 1604, 613, 366, 264, 819, 7705, 300, 321, 393, 3079, 370, 281, 722, 766, 365, 321], 'temperature': 0.0, 'avg_logprob': -0.07790076595613327, 'compression_ratio': 1.8202764976958525, 'no_speech_prob': 4.673379589803517e-05}, {'id': 546, 'seek': 417128, 'start': 4192.32, 'end': 4199.2, 'text': ' have root on the stack and the sentence in the buffer and we have no dependency arcs constructed', 'tokens': [362, 5593, 322, 264, 8630, 293, 264, 8174, 294, 264, 21762, 293, 321, 362, 572, 33621, 10346, 82, 17083], 'temperature': 0.0, 'avg_logprob': -0.07790076595613327, 'compression_ratio': 1.8202764976958525, 'no_speech_prob': 4.673379589803517e-05}, {'id': 547, 'seek': 419920, 'start': 4199.2, 'end': 4205.84, 'text': \" so we have to choose one of the three actions and when there's only one thing on the stack the only\", 'tokens': [370, 321, 362, 281, 2826, 472, 295, 264, 1045, 5909, 293, 562, 456, 311, 787, 472, 551, 322, 264, 8630, 264, 787], 'temperature': 0.0, 'avg_logprob': -0.06617008149623871, 'compression_ratio': 1.8186046511627907, 'no_speech_prob': 2.6259936930728145e-05}, {'id': 548, 'seek': 419920, 'start': 4205.84, 'end': 4213.92, 'text': ' thing we can do is shift so we shift now the stack looks like this so now we have to take another', 'tokens': [551, 321, 393, 360, 307, 5513, 370, 321, 5513, 586, 264, 8630, 1542, 411, 341, 370, 586, 321, 362, 281, 747, 1071], 'temperature': 0.0, 'avg_logprob': -0.06617008149623871, 'compression_ratio': 1.8186046511627907, 'no_speech_prob': 2.6259936930728145e-05}, {'id': 549, 'seek': 419920, 'start': 4213.92, 'end': 4220.72, 'text': ' action and at this point we have a choice because we could immediately reduce so you know we could', 'tokens': [3069, 293, 412, 341, 935, 321, 362, 257, 3922, 570, 321, 727, 4258, 5407, 370, 291, 458, 321, 727], 'temperature': 0.0, 'avg_logprob': -0.06617008149623871, 'compression_ratio': 1.8186046511627907, 'no_speech_prob': 2.6259936930728145e-05}, {'id': 550, 'seek': 419920, 'start': 4220.72, 'end': 4228.639999999999, 'text': \" say okay let's just make i a dependent of root and we'd get a stack size of one again but that\", 'tokens': [584, 1392, 718, 311, 445, 652, 741, 257, 12334, 295, 5593, 293, 321, 1116, 483, 257, 8630, 2744, 295, 472, 797, 457, 300], 'temperature': 0.0, 'avg_logprob': -0.06617008149623871, 'compression_ratio': 1.8186046511627907, 'no_speech_prob': 2.6259936930728145e-05}, {'id': 551, 'seek': 422864, 'start': 4228.64, 'end': 4236.0, 'text': \" would be the wrong thing to do because i isn't the head of the sentence so what we should instead do\", 'tokens': [576, 312, 264, 2085, 551, 281, 360, 570, 741, 1943, 380, 264, 1378, 295, 264, 8174, 370, 437, 321, 820, 2602, 360], 'temperature': 0.0, 'avg_logprob': -0.04170680673498856, 'compression_ratio': 1.6647727272727273, 'no_speech_prob': 7.13231202098541e-05}, {'id': 552, 'seek': 422864, 'start': 4236.0, 'end': 4244.0, 'text': ' is shift again and get i8 on the stack and fish still in the buffer well at that point we keep', 'tokens': [307, 5513, 797, 293, 483, 741, 23, 322, 264, 8630, 293, 3506, 920, 294, 264, 21762, 731, 412, 300, 935, 321, 1066], 'temperature': 0.0, 'avg_logprob': -0.04170680673498856, 'compression_ratio': 1.6647727272727273, 'no_speech_prob': 7.13231202098541e-05}, {'id': 553, 'seek': 422864, 'start': 4244.0, 'end': 4253.12, 'text': ' on parsing a bit further and so now what we can do is say well wait a minute now i is a dependent', 'tokens': [322, 21156, 278, 257, 857, 3052, 293, 370, 586, 437, 321, 393, 360, 307, 584, 731, 1699, 257, 3456, 586, 741, 307, 257, 12334], 'temperature': 0.0, 'avg_logprob': -0.04170680673498856, 'compression_ratio': 1.6647727272727273, 'no_speech_prob': 7.13231202098541e-05}, {'id': 554, 'seek': 425312, 'start': 4253.12, 'end': 4262.32, 'text': \" of eight and so we can do a left arc reduce and so i disappears from the stack so here's our new stack\", 'tokens': [295, 3180, 293, 370, 321, 393, 360, 257, 1411, 10346, 5407, 293, 370, 741, 25527, 490, 264, 8630, 370, 510, 311, 527, 777, 8630], 'temperature': 0.0, 'avg_logprob': -0.0887649276039817, 'compression_ratio': 1.7705882352941176, 'no_speech_prob': 3.6979876313125715e-05}, {'id': 555, 'seek': 425312, 'start': 4262.32, 'end': 4270.4, 'text': \" but we add to the set of arcs that we've added that i is the subject of eight okay well after that\", 'tokens': [457, 321, 909, 281, 264, 992, 295, 10346, 82, 300, 321, 600, 3869, 300, 741, 307, 264, 3983, 295, 3180, 1392, 731, 934, 300], 'temperature': 0.0, 'avg_logprob': -0.0887649276039817, 'compression_ratio': 1.7705882352941176, 'no_speech_prob': 3.6979876313125715e-05}, {'id': 556, 'seek': 425312, 'start': 4271.28, 'end': 4276.32, 'text': \" we could have we could reduce again because there's still two things on the stack but that'd be the\", 'tokens': [321, 727, 362, 321, 727, 5407, 797, 570, 456, 311, 920, 732, 721, 322, 264, 8630, 457, 300, 1116, 312, 264], 'temperature': 0.0, 'avg_logprob': -0.0887649276039817, 'compression_ratio': 1.7705882352941176, 'no_speech_prob': 3.6979876313125715e-05}, {'id': 557, 'seek': 427632, 'start': 4276.32, 'end': 4284.719999999999, 'text': ' wrong thing to do the right thing to do next would be to shift fish onto the stack and then at that', 'tokens': [2085, 551, 281, 360, 264, 558, 551, 281, 360, 958, 576, 312, 281, 5513, 3506, 3911, 264, 8630, 293, 550, 412, 300], 'temperature': 0.0, 'avg_logprob': -0.06785320255854359, 'compression_ratio': 1.9477124183006536, 'no_speech_prob': 1.7700207536108792e-05}, {'id': 558, 'seek': 427632, 'start': 4284.719999999999, 'end': 4295.12, 'text': ' point we can do a right arc reduce saying that eight is the object of fish and add a new dependency', 'tokens': [935, 321, 393, 360, 257, 558, 10346, 5407, 1566, 300, 3180, 307, 264, 2657, 295, 3506, 293, 909, 257, 777, 33621], 'temperature': 0.0, 'avg_logprob': -0.06785320255854359, 'compression_ratio': 1.9477124183006536, 'no_speech_prob': 1.7700207536108792e-05}, {'id': 559, 'seek': 427632, 'start': 4295.12, 'end': 4304.88, 'text': ' to our dependency set and then we can one more time do a right arc reduce to say that eight is the', 'tokens': [281, 527, 33621, 992, 293, 550, 321, 393, 472, 544, 565, 360, 257, 558, 10346, 5407, 281, 584, 300, 3180, 307, 264], 'temperature': 0.0, 'avg_logprob': -0.06785320255854359, 'compression_ratio': 1.9477124183006536, 'no_speech_prob': 1.7700207536108792e-05}, {'id': 560, 'seek': 430488, 'start': 4304.88, 'end': 4311.6, 'text': ' root of the whole sentence and add in that extra root relation with our pseudo root and at that', 'tokens': [5593, 295, 264, 1379, 8174, 293, 909, 294, 300, 2857, 5593, 9721, 365, 527, 35899, 5593, 293, 412, 300], 'temperature': 0.0, 'avg_logprob': -0.066322897419785, 'compression_ratio': 1.8662420382165605, 'no_speech_prob': 1.5666106264689006e-05}, {'id': 561, 'seek': 430488, 'start': 4311.6, 'end': 4318.4800000000005, 'text': \" point we reach the end condition so the end condition was the buffer was empty and there's one thing\", 'tokens': [935, 321, 2524, 264, 917, 4188, 370, 264, 917, 4188, 390, 264, 21762, 390, 6707, 293, 456, 311, 472, 551], 'temperature': 0.0, 'avg_logprob': -0.066322897419785, 'compression_ratio': 1.8662420382165605, 'no_speech_prob': 1.5666106264689006e-05}, {'id': 562, 'seek': 430488, 'start': 4318.4800000000005, 'end': 4326.08, 'text': ' the root on the stack and at that point we can finish so this little transition machine does the', 'tokens': [264, 5593, 322, 264, 8630, 293, 412, 300, 935, 321, 393, 2413, 370, 341, 707, 6034, 3479, 775, 264], 'temperature': 0.0, 'avg_logprob': -0.066322897419785, 'compression_ratio': 1.8662420382165605, 'no_speech_prob': 1.5666106264689006e-05}, {'id': 563, 'seek': 432608, 'start': 4326.08, 'end': 4336.0, 'text': \" parsing up of the sentence but there's one thing that's left to explain still here which is how do\", 'tokens': [21156, 278, 493, 295, 264, 8174, 457, 456, 311, 472, 551, 300, 311, 1411, 281, 2903, 920, 510, 597, 307, 577, 360], 'temperature': 0.0, 'avg_logprob': -0.05264114340146383, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 1.2577496818266809e-05}, {'id': 564, 'seek': 432608, 'start': 4336.0, 'end': 4342.96, 'text': ' you choose the next action so as soon as you have two things or more on the stack what you do next', 'tokens': [291, 2826, 264, 958, 3069, 370, 382, 2321, 382, 291, 362, 732, 721, 420, 544, 322, 264, 8630, 437, 291, 360, 958], 'temperature': 0.0, 'avg_logprob': -0.05264114340146383, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 1.2577496818266809e-05}, {'id': 565, 'seek': 432608, 'start': 4343.6, 'end': 4348.24, 'text': \" you've always got a choice you could keep shifting at least if there's still things on the buffer\", 'tokens': [291, 600, 1009, 658, 257, 3922, 291, 727, 1066, 17573, 412, 1935, 498, 456, 311, 920, 721, 322, 264, 21762], 'temperature': 0.0, 'avg_logprob': -0.05264114340146383, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 1.2577496818266809e-05}, {'id': 566, 'seek': 432608, 'start': 4348.24, 'end': 4354.96, 'text': ' or you can do a left arc or you can do a right arc and how do you know what choices correct', 'tokens': [420, 291, 393, 360, 257, 1411, 10346, 420, 291, 393, 360, 257, 558, 10346, 293, 577, 360, 291, 458, 437, 7994, 3006], 'temperature': 0.0, 'avg_logprob': -0.05264114340146383, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 1.2577496818266809e-05}, {'id': 567, 'seek': 435496, 'start': 4354.96, 'end': 4360.16, 'text': \" and well one answer to that is to say well you don't know what choices correct and that's why\", 'tokens': [293, 731, 472, 1867, 281, 300, 307, 281, 584, 731, 291, 500, 380, 458, 437, 7994, 3006, 293, 300, 311, 983], 'temperature': 0.0, 'avg_logprob': -0.06570784250895183, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 6.1025679315207526e-05}, {'id': 568, 'seek': 435496, 'start': 4360.16, 'end': 4367.36, 'text': ' parsing is hard and sentences are ambiguous you can do any of those things you have to explore', 'tokens': [21156, 278, 307, 1152, 293, 16579, 366, 39465, 291, 393, 360, 604, 295, 729, 721, 291, 362, 281, 6839], 'temperature': 0.0, 'avg_logprob': -0.06570784250895183, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 6.1025679315207526e-05}, {'id': 569, 'seek': 435496, 'start': 4367.36, 'end': 4374.32, 'text': ' all of them and well if you naively explore all of them then you do an exponential amount of work', 'tokens': [439, 295, 552, 293, 731, 498, 291, 1667, 3413, 6839, 439, 295, 552, 550, 291, 360, 364, 21510, 2372, 295, 589], 'temperature': 0.0, 'avg_logprob': -0.06570784250895183, 'compression_ratio': 1.7125748502994012, 'no_speech_prob': 6.1025679315207526e-05}, {'id': 570, 'seek': 437432, 'start': 4374.32, 'end': 4385.2, 'text': \" to parse the sentence so in the early 2000s you're a commandeer phrase and you know that's essentially\", 'tokens': [281, 48377, 264, 8174, 370, 294, 264, 2440, 8132, 82, 291, 434, 257, 5622, 68, 260, 9535, 293, 291, 458, 300, 311, 4476], 'temperature': 0.0, 'avg_logprob': -0.19933610213430306, 'compression_ratio': 1.734463276836158, 'no_speech_prob': 6.776284863008186e-05}, {'id': 571, 'seek': 437432, 'start': 4385.2, 'end': 4394.719999999999, 'text': \" what people have done in the 80s and 90s is explore every path but in the early 2000s you're a commandeer\", 'tokens': [437, 561, 362, 1096, 294, 264, 4688, 82, 293, 4289, 82, 307, 6839, 633, 3100, 457, 294, 264, 2440, 8132, 82, 291, 434, 257, 5622, 68, 260], 'temperature': 0.0, 'avg_logprob': -0.19933610213430306, 'compression_ratio': 1.734463276836158, 'no_speech_prob': 6.776284863008186e-05}, {'id': 572, 'seek': 437432, 'start': 4394.719999999999, 'end': 4402.24, 'text': \" phrase essential observation was but wait a minute we know about machine learning now so why don't\", 'tokens': [9535, 7115, 14816, 390, 457, 1699, 257, 3456, 321, 458, 466, 3479, 2539, 586, 370, 983, 500, 380], 'temperature': 0.0, 'avg_logprob': -0.19933610213430306, 'compression_ratio': 1.734463276836158, 'no_speech_prob': 6.776284863008186e-05}, {'id': 573, 'seek': 440224, 'start': 4402.24, 'end': 4411.44, 'text': ' I try and train a classifier which predicts what the next action I should take is given this stack', 'tokens': [286, 853, 293, 3847, 257, 1508, 9902, 597, 6069, 82, 437, 264, 958, 3069, 286, 820, 747, 307, 2212, 341, 8630], 'temperature': 0.0, 'avg_logprob': -0.04903603345155716, 'compression_ratio': 1.7294117647058824, 'no_speech_prob': 0.00019990139116998762}, {'id': 574, 'seek': 440224, 'start': 4411.44, 'end': 4420.32, 'text': ' and buffer configuration because if I can write a machine learning classifier which can nearly', 'tokens': [293, 21762, 11694, 570, 498, 286, 393, 2464, 257, 3479, 2539, 1508, 9902, 597, 393, 6217], 'temperature': 0.0, 'avg_logprob': -0.04903603345155716, 'compression_ratio': 1.7294117647058824, 'no_speech_prob': 0.00019990139116998762}, {'id': 575, 'seek': 440224, 'start': 4420.32, 'end': 4429.599999999999, 'text': \" always correctly predict the next action given a stack and buffer then I'm in a really good position\", 'tokens': [1009, 8944, 6069, 264, 958, 3069, 2212, 257, 8630, 293, 21762, 550, 286, 478, 294, 257, 534, 665, 2535], 'temperature': 0.0, 'avg_logprob': -0.04903603345155716, 'compression_ratio': 1.7294117647058824, 'no_speech_prob': 0.00019990139116998762}, {'id': 576, 'seek': 442960, 'start': 4429.6, 'end': 4436.320000000001, 'text': \" because then I can build what's referred to as a greedy dependency parser which just goes\", 'tokens': [570, 550, 286, 393, 1322, 437, 311, 10839, 281, 382, 257, 28228, 33621, 21156, 260, 597, 445, 1709], 'temperature': 0.0, 'avg_logprob': -0.06577698886394501, 'compression_ratio': 1.8791946308724832, 'no_speech_prob': 5.300805059960112e-05}, {'id': 577, 'seek': 442960, 'start': 4436.320000000001, 'end': 4444.64, 'text': \" bang bang bang word at a time okay here's the next thing run classifier choose next action run\", 'tokens': [8550, 8550, 8550, 1349, 412, 257, 565, 1392, 510, 311, 264, 958, 551, 1190, 1508, 9902, 2826, 958, 3069, 1190], 'temperature': 0.0, 'avg_logprob': -0.06577698886394501, 'compression_ratio': 1.8791946308724832, 'no_speech_prob': 5.300805059960112e-05}, {'id': 578, 'seek': 442960, 'start': 4444.64, 'end': 4450.88, 'text': ' classifier choose next action run classifier choose next action so that the amount of work that', 'tokens': [1508, 9902, 2826, 958, 3069, 1190, 1508, 9902, 2826, 958, 3069, 370, 300, 264, 2372, 295, 589, 300], 'temperature': 0.0, 'avg_logprob': -0.06577698886394501, 'compression_ratio': 1.8791946308724832, 'no_speech_prob': 5.300805059960112e-05}, {'id': 579, 'seek': 445088, 'start': 4450.88, 'end': 4459.6, 'text': \" we're doing becomes linear in the length of the sentence rather than that being cubic in the length\", 'tokens': [321, 434, 884, 3643, 8213, 294, 264, 4641, 295, 264, 8174, 2831, 813, 300, 885, 28733, 294, 264, 4641], 'temperature': 0.0, 'avg_logprob': -0.05769856159503643, 'compression_ratio': 1.943298969072165, 'no_speech_prob': 3.6446115700528026e-05}, {'id': 580, 'seek': 445088, 'start': 4459.6, 'end': 4464.64, 'text': ' of the sentence using dynamic programming or exponential in the length of the sentence if you', 'tokens': [295, 264, 8174, 1228, 8546, 9410, 420, 21510, 294, 264, 4641, 295, 264, 8174, 498, 291], 'temperature': 0.0, 'avg_logprob': -0.05769856159503643, 'compression_ratio': 1.943298969072165, 'no_speech_prob': 3.6446115700528026e-05}, {'id': 581, 'seek': 445088, 'start': 4464.64, 'end': 4472.8, 'text': \" don't use dynamic programming so for each at each step we predict the next action using some\", 'tokens': [500, 380, 764, 8546, 9410, 370, 337, 1184, 412, 1184, 1823, 321, 6069, 264, 958, 3069, 1228, 512], 'temperature': 0.0, 'avg_logprob': -0.05769856159503643, 'compression_ratio': 1.943298969072165, 'no_speech_prob': 3.6446115700528026e-05}, {'id': 582, 'seek': 445088, 'start': 4472.8, 'end': 4478.400000000001, 'text': ' discriminative classifier so starting off he was using things like support vector machines', 'tokens': [20828, 1166, 1508, 9902, 370, 2891, 766, 415, 390, 1228, 721, 411, 1406, 8062, 8379], 'temperature': 0.0, 'avg_logprob': -0.05769856159503643, 'compression_ratio': 1.943298969072165, 'no_speech_prob': 3.6446115700528026e-05}, {'id': 583, 'seek': 447840, 'start': 4478.4, 'end': 4483.679999999999, 'text': \" but it can be anything at all like a softmax classifier that's closer to our neural networks\", 'tokens': [457, 309, 393, 312, 1340, 412, 439, 411, 257, 2787, 41167, 1508, 9902, 300, 311, 4966, 281, 527, 18161, 9590], 'temperature': 0.0, 'avg_logprob': -0.10955308278401693, 'compression_ratio': 1.7397260273972603, 'no_speech_prob': 0.00026064697885885835}, {'id': 584, 'seek': 447840, 'start': 4483.679999999999, 'end': 4490.16, 'text': \" and there are either for what I presented three classes if you're just thinking of the two\", 'tokens': [293, 456, 366, 2139, 337, 437, 286, 8212, 1045, 5359, 498, 291, 434, 445, 1953, 295, 264, 732], 'temperature': 0.0, 'avg_logprob': -0.10955308278401693, 'compression_ratio': 1.7397260273972603, 'no_speech_prob': 0.00026064697885885835}, {'id': 585, 'seek': 447840, 'start': 4490.16, 'end': 4495.759999999999, 'text': \" reduces in the shift or if you're thinking of you're also assigning a relation and you have a set\", 'tokens': [18081, 294, 264, 5513, 420, 498, 291, 434, 1953, 295, 291, 434, 611, 49602, 257, 9721, 293, 291, 362, 257, 992], 'temperature': 0.0, 'avg_logprob': -0.10955308278401693, 'compression_ratio': 1.7397260273972603, 'no_speech_prob': 0.00026064697885885835}, {'id': 586, 'seek': 447840, 'start': 4495.759999999999, 'end': 4503.599999999999, 'text': \" of our relations like 20 relations then that's be sort of 41 moves that you could decide on at each\", 'tokens': [295, 527, 2299, 411, 945, 2299, 550, 300, 311, 312, 1333, 295, 18173, 6067, 300, 291, 727, 4536, 322, 412, 1184], 'temperature': 0.0, 'avg_logprob': -0.10955308278401693, 'compression_ratio': 1.7397260273972603, 'no_speech_prob': 0.00026064697885885835}, {'id': 587, 'seek': 450360, 'start': 4503.6, 'end': 4510.4800000000005, 'text': \" point and the features are effectively the configurations I was showing before what's the top of the\", 'tokens': [935, 293, 264, 4122, 366, 8659, 264, 31493, 286, 390, 4099, 949, 437, 311, 264, 1192, 295, 264], 'temperature': 0.0, 'avg_logprob': -0.08077348904176192, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 6.003313683322631e-05}, {'id': 588, 'seek': 450360, 'start': 4510.4800000000005, 'end': 4515.68, 'text': \" stack word what part of speech is it what's the first word in the buffer what's that words part\", 'tokens': [8630, 1349, 437, 644, 295, 6218, 307, 309, 437, 311, 264, 700, 1349, 294, 264, 21762, 437, 311, 300, 2283, 644], 'temperature': 0.0, 'avg_logprob': -0.08077348904176192, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 6.003313683322631e-05}, {'id': 589, 'seek': 450360, 'start': 4515.68, 'end': 4521.92, 'text': \" of speech etc and so in the simplest way of doing this you're now doing no search at all you\", 'tokens': [295, 6218, 5183, 293, 370, 294, 264, 22811, 636, 295, 884, 341, 291, 434, 586, 884, 572, 3164, 412, 439, 291], 'temperature': 0.0, 'avg_logprob': -0.08077348904176192, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 6.003313683322631e-05}, {'id': 590, 'seek': 450360, 'start': 4521.92, 'end': 4528.240000000001, 'text': ' would just sort of take each configuration and turn decide the most likely next move and you make', 'tokens': [576, 445, 1333, 295, 747, 1184, 11694, 293, 1261, 4536, 264, 881, 3700, 958, 1286, 293, 291, 652], 'temperature': 0.0, 'avg_logprob': -0.08077348904176192, 'compression_ratio': 1.808411214953271, 'no_speech_prob': 6.003313683322631e-05}, {'id': 591, 'seek': 452824, 'start': 4528.24, 'end': 4535.44, 'text': \" it and that's a greedy dependency parser which is widely used you can do better if you want to do\", 'tokens': [309, 293, 300, 311, 257, 28228, 33621, 21156, 260, 597, 307, 13371, 1143, 291, 393, 360, 1101, 498, 291, 528, 281, 360], 'temperature': 0.0, 'avg_logprob': -0.06588834783305293, 'compression_ratio': 1.7692307692307692, 'no_speech_prob': 0.00016808719374239445}, {'id': 592, 'seek': 452824, 'start': 4535.44, 'end': 4542.32, 'text': \" a lot more work so you can do what's called a beam search where you maintain a number of fairly\", 'tokens': [257, 688, 544, 589, 370, 291, 393, 360, 437, 311, 1219, 257, 14269, 3164, 689, 291, 6909, 257, 1230, 295, 6457], 'temperature': 0.0, 'avg_logprob': -0.06588834783305293, 'compression_ratio': 1.7692307692307692, 'no_speech_prob': 0.00016808719374239445}, {'id': 593, 'seek': 452824, 'start': 4542.32, 'end': 4549.76, 'text': ' good parse prefixes at each step and you can extend them out further and then you can evaluate', 'tokens': [665, 48377, 18417, 36005, 412, 1184, 1823, 293, 291, 393, 10101, 552, 484, 3052, 293, 550, 291, 393, 13059], 'temperature': 0.0, 'avg_logprob': -0.06588834783305293, 'compression_ratio': 1.7692307692307692, 'no_speech_prob': 0.00016808719374239445}, {'id': 594, 'seek': 452824, 'start': 4549.76, 'end': 4556.719999999999, 'text': ' later on which of those seems to be the best and so beam search is one technique to improve dependency', 'tokens': [1780, 322, 597, 295, 729, 2544, 281, 312, 264, 1151, 293, 370, 14269, 3164, 307, 472, 6532, 281, 3470, 33621], 'temperature': 0.0, 'avg_logprob': -0.06588834783305293, 'compression_ratio': 1.7692307692307692, 'no_speech_prob': 0.00016808719374239445}, {'id': 595, 'seek': 455672, 'start': 4556.72, 'end': 4566.8, 'text': ' parsing by doing a lot of work and it turns out that although these greedy transition based parsers', 'tokens': [21156, 278, 538, 884, 257, 688, 295, 589, 293, 309, 4523, 484, 300, 4878, 613, 28228, 6034, 2361, 21156, 433], 'temperature': 0.0, 'avg_logprob': -0.1022888845013034, 'compression_ratio': 1.6464088397790055, 'no_speech_prob': 7.95240921434015e-05}, {'id': 596, 'seek': 455672, 'start': 4567.92, 'end': 4574.56, 'text': ' are a fraction worse than the best possible ways known to parse sentences that they actually work', 'tokens': [366, 257, 14135, 5324, 813, 264, 1151, 1944, 2098, 2570, 281, 48377, 16579, 300, 436, 767, 589], 'temperature': 0.0, 'avg_logprob': -0.1022888845013034, 'compression_ratio': 1.6464088397790055, 'no_speech_prob': 7.95240921434015e-05}, {'id': 597, 'seek': 455672, 'start': 4574.56, 'end': 4583.6, 'text': ' very accurately almost as well and they have this wonderful advantage that they give you linear time', 'tokens': [588, 20095, 1920, 382, 731, 293, 436, 362, 341, 3715, 5002, 300, 436, 976, 291, 8213, 565], 'temperature': 0.0, 'avg_logprob': -0.1022888845013034, 'compression_ratio': 1.6464088397790055, 'no_speech_prob': 7.95240921434015e-05}, {'id': 598, 'seek': 458360, 'start': 4583.6, 'end': 4591.200000000001, 'text': ' parsing in terms of the length of your sentences and text and so if you want to do a huge amount of', 'tokens': [21156, 278, 294, 2115, 295, 264, 4641, 295, 428, 16579, 293, 2487, 293, 370, 498, 291, 528, 281, 360, 257, 2603, 2372, 295], 'temperature': 0.0, 'avg_logprob': -0.06400772813078645, 'compression_ratio': 1.6096256684491979, 'no_speech_prob': 5.4678370361216366e-05}, {'id': 599, 'seek': 458360, 'start': 4591.200000000001, 'end': 4599.68, 'text': \" parsing they're just a fantastic thing to use because you've then got an algorithm that scales to\", 'tokens': [21156, 278, 436, 434, 445, 257, 5456, 551, 281, 764, 570, 291, 600, 550, 658, 364, 9284, 300, 17408, 281], 'temperature': 0.0, 'avg_logprob': -0.06400772813078645, 'compression_ratio': 1.6096256684491979, 'no_speech_prob': 5.4678370361216366e-05}, {'id': 600, 'seek': 458360, 'start': 4599.68, 'end': 4608.160000000001, 'text': \" the size of the web okay so I'm kind of a little bit behind so I guess I'm not going to get through all\", 'tokens': [264, 2744, 295, 264, 3670, 1392, 370, 286, 478, 733, 295, 257, 707, 857, 2261, 370, 286, 2041, 286, 478, 406, 516, 281, 483, 807, 439], 'temperature': 0.0, 'avg_logprob': -0.06400772813078645, 'compression_ratio': 1.6096256684491979, 'no_speech_prob': 5.4678370361216366e-05}, {'id': 601, 'seek': 460816, 'start': 4608.16, 'end': 4614.88, 'text': \" these slides today and we'll have to finish out the final slides tomorrow but just to push a teeny\", 'tokens': [613, 9788, 965, 293, 321, 603, 362, 281, 2413, 484, 264, 2572, 9788, 4153, 457, 445, 281, 2944, 257, 48232], 'temperature': 0.0, 'avg_logprob': -0.13433192326472357, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 2.1080270016682334e-05}, {'id': 602, 'seek': 460816, 'start': 4614.88, 'end': 4623.12, 'text': \" bit further I'll just save a couple more on the sort of what Neyfrit did for dependency parser and\", 'tokens': [857, 3052, 286, 603, 445, 3155, 257, 1916, 544, 322, 264, 1333, 295, 437, 1734, 88, 69, 3210, 630, 337, 33621, 21156, 260, 293], 'temperature': 0.0, 'avg_logprob': -0.13433192326472357, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 2.1080270016682334e-05}, {'id': 603, 'seek': 460816, 'start': 4623.12, 'end': 4629.44, 'text': \" then I'll sort of introduce the neural form of that in the next class so conventionally you had this\", 'tokens': [550, 286, 603, 1333, 295, 5366, 264, 18161, 1254, 295, 300, 294, 264, 958, 1508, 370, 10286, 379, 291, 632, 341], 'temperature': 0.0, 'avg_logprob': -0.13433192326472357, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 2.1080270016682334e-05}, {'id': 604, 'seek': 460816, 'start': 4629.44, 'end': 4635.92, 'text': ' sort of stack and buffer configuration and you wanted to build a machine learning classifier', 'tokens': [1333, 295, 8630, 293, 21762, 11694, 293, 291, 1415, 281, 1322, 257, 3479, 2539, 1508, 9902], 'temperature': 0.0, 'avg_logprob': -0.13433192326472357, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 2.1080270016682334e-05}, {'id': 605, 'seek': 463592, 'start': 4635.92, 'end': 4644.72, 'text': ' and so the way that was done was by using symbolic features of this configuration and what kind of', 'tokens': [293, 370, 264, 636, 300, 390, 1096, 390, 538, 1228, 25755, 4122, 295, 341, 11694, 293, 437, 733, 295], 'temperature': 0.0, 'avg_logprob': -0.0737215294895402, 'compression_ratio': 1.8317307692307692, 'no_speech_prob': 2.2060708943172358e-05}, {'id': 606, 'seek': 463592, 'start': 4645.6, 'end': 4653.28, 'text': ' symbolic features did you use use these indicator features that picked out a small subset normally', 'tokens': [25755, 4122, 630, 291, 764, 764, 613, 16961, 4122, 300, 6183, 484, 257, 1359, 25993, 5646], 'temperature': 0.0, 'avg_logprob': -0.0737215294895402, 'compression_ratio': 1.8317307692307692, 'no_speech_prob': 2.2060708943172358e-05}, {'id': 607, 'seek': 463592, 'start': 4653.28, 'end': 4659.36, 'text': \" one to three elements of the configuration so you'd have a feature that could be something like\", 'tokens': [472, 281, 1045, 4959, 295, 264, 11694, 370, 291, 1116, 362, 257, 4111, 300, 727, 312, 746, 411], 'temperature': 0.0, 'avg_logprob': -0.0737215294895402, 'compression_ratio': 1.8317307692307692, 'no_speech_prob': 2.2060708943172358e-05}, {'id': 608, 'seek': 463592, 'start': 4660.08, 'end': 4665.36, 'text': ' the thing on the top of the stack is the word good which is an adjective or it could be', 'tokens': [264, 551, 322, 264, 1192, 295, 264, 8630, 307, 264, 1349, 665, 597, 307, 364, 44129, 420, 309, 727, 312], 'temperature': 0.0, 'avg_logprob': -0.0737215294895402, 'compression_ratio': 1.8317307692307692, 'no_speech_prob': 2.2060708943172358e-05}, {'id': 609, 'seek': 466536, 'start': 4665.36, 'end': 4670.88, 'text': \" the thing on the top of the stack is an adjective and the thing that's first and the buffer is an\", 'tokens': [264, 551, 322, 264, 1192, 295, 264, 8630, 307, 364, 44129, 293, 264, 551, 300, 311, 700, 293, 264, 21762, 307, 364], 'temperature': 0.0, 'avg_logprob': -0.11683614863905796, 'compression_ratio': 1.9444444444444444, 'no_speech_prob': 5.994398452457972e-05}, {'id': 610, 'seek': 466536, 'start': 4670.88, 'end': 4676.32, 'text': ' noun or it could just be looking at one thing and saying the first thing and the buffer is a verb', 'tokens': [23307, 420, 309, 727, 445, 312, 1237, 412, 472, 551, 293, 1566, 264, 700, 551, 293, 264, 21762, 307, 257, 9595], 'temperature': 0.0, 'avg_logprob': -0.11683614863905796, 'compression_ratio': 1.9444444444444444, 'no_speech_prob': 5.994398452457972e-05}, {'id': 611, 'seek': 466536, 'start': 4677.04, 'end': 4684.4, 'text': \" so you'd have all of these features and because these features commonly involved words and commonly\", 'tokens': [370, 291, 1116, 362, 439, 295, 613, 4122, 293, 570, 613, 4122, 12719, 3288, 2283, 293, 12719], 'temperature': 0.0, 'avg_logprob': -0.11683614863905796, 'compression_ratio': 1.9444444444444444, 'no_speech_prob': 5.994398452457972e-05}, {'id': 612, 'seek': 466536, 'start': 4684.4, 'end': 4692.08, 'text': ' involved conjunctions of several conditions you had a lot of features and you know having', 'tokens': [3288, 18244, 3916, 295, 2940, 4487, 291, 632, 257, 688, 295, 4122, 293, 291, 458, 1419], 'temperature': 0.0, 'avg_logprob': -0.11683614863905796, 'compression_ratio': 1.9444444444444444, 'no_speech_prob': 5.994398452457972e-05}, {'id': 613, 'seek': 469208, 'start': 4692.08, 'end': 4699.44, 'text': ' mentions of words and conjunctions and conditions definitely helped to make these parsers work better', 'tokens': [23844, 295, 2283, 293, 18244, 3916, 293, 4487, 2138, 4254, 281, 652, 613, 21156, 433, 589, 1101], 'temperature': 0.0, 'avg_logprob': -0.10506815788073418, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 3.2633346563670784e-05}, {'id': 614, 'seek': 469208, 'start': 4700.24, 'end': 4706.72, 'text': ' but nevertheless because you had all of these sort of one zero symbolic features that you had a', 'tokens': [457, 26924, 570, 291, 632, 439, 295, 613, 1333, 295, 472, 4018, 25755, 4122, 300, 291, 632, 257], 'temperature': 0.0, 'avg_logprob': -0.10506815788073418, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 3.2633346563670784e-05}, {'id': 615, 'seek': 469208, 'start': 4706.72, 'end': 4714.0, 'text': ' ton of such features so commonly these parsers were built using something like you know a million', 'tokens': [2952, 295, 1270, 4122, 370, 12719, 613, 21156, 433, 645, 3094, 1228, 746, 411, 291, 458, 257, 2459], 'temperature': 0.0, 'avg_logprob': -0.10506815788073418, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 3.2633346563670784e-05}, {'id': 616, 'seek': 469208, 'start': 4714.0, 'end': 4721.44, 'text': ' to ten million different features of sentences and I mentioned already the importance of evaluation', 'tokens': [281, 2064, 2459, 819, 4122, 295, 16579, 293, 286, 2835, 1217, 264, 7379, 295, 13344], 'temperature': 0.0, 'avg_logprob': -0.10506815788073418, 'compression_ratio': 1.7954545454545454, 'no_speech_prob': 3.2633346563670784e-05}, {'id': 617, 'seek': 472144, 'start': 4721.44, 'end': 4730.24, 'text': ' let me just sort of quickly say how these parsers were evaluated so to evaluate a', 'tokens': [718, 385, 445, 1333, 295, 2661, 584, 577, 613, 21156, 433, 645, 25509, 370, 281, 13059, 257], 'temperature': 0.0, 'avg_logprob': -0.20203791091691203, 'compression_ratio': 1.6727272727272726, 'no_speech_prob': 3.3164978958666325e-05}, {'id': 618, 'seek': 472144, 'start': 4731.36, 'end': 4739.04, 'text': ' a parser for a particular sentence it was handpars our test set was handpars in the tree banks so we', 'tokens': [257, 21156, 260, 337, 257, 1729, 8174, 309, 390, 1011, 79, 685, 527, 1500, 992, 390, 1011, 79, 685, 294, 264, 4230, 10237, 370, 321], 'temperature': 0.0, 'avg_logprob': -0.20203791091691203, 'compression_ratio': 1.6727272727272726, 'no_speech_prob': 3.3164978958666325e-05}, {'id': 619, 'seek': 472144, 'start': 4739.04, 'end': 4745.919999999999, 'text': ' have gold dependencies what the human thought were right and so we can write those down those', 'tokens': [362, 3821, 36606, 437, 264, 1952, 1194, 645, 558, 293, 370, 321, 393, 2464, 729, 760, 729], 'temperature': 0.0, 'avg_logprob': -0.20203791091691203, 'compression_ratio': 1.6727272727272726, 'no_speech_prob': 3.3164978958666325e-05}, {'id': 620, 'seek': 474592, 'start': 4745.92, 'end': 4753.84, 'text': ' dependencies down as statements of saying the first word is the dependent of the second word', 'tokens': [36606, 760, 382, 12363, 295, 1566, 264, 700, 1349, 307, 264, 12334, 295, 264, 1150, 1349], 'temperature': 0.0, 'avg_logprob': -0.09591292185955737, 'compression_ratio': 1.8676470588235294, 'no_speech_prob': 2.0758763639605604e-05}, {'id': 621, 'seek': 474592, 'start': 4753.84, 'end': 4761.28, 'text': \" fire a subject dependency and then the parser is also going to make similar claims as to what's\", 'tokens': [2610, 257, 3983, 33621, 293, 550, 264, 21156, 260, 307, 611, 516, 281, 652, 2531, 9441, 382, 281, 437, 311], 'temperature': 0.0, 'avg_logprob': -0.09591292185955737, 'compression_ratio': 1.8676470588235294, 'no_speech_prob': 2.0758763639605604e-05}, {'id': 622, 'seek': 474592, 'start': 4761.28, 'end': 4768.4800000000005, 'text': ' a dependent on what and so there are two common metrics that are used one is just are you getting', 'tokens': [257, 12334, 322, 437, 293, 370, 456, 366, 732, 2689, 16367, 300, 366, 1143, 472, 307, 445, 366, 291, 1242], 'temperature': 0.0, 'avg_logprob': -0.09591292185955737, 'compression_ratio': 1.8676470588235294, 'no_speech_prob': 2.0758763639605604e-05}, {'id': 623, 'seek': 474592, 'start': 4768.4800000000005, 'end': 4775.76, 'text': \" these dependency facts right so both of these dependency facts match and so that's referred to\", 'tokens': [613, 33621, 9130, 558, 370, 1293, 295, 613, 33621, 9130, 2995, 293, 370, 300, 311, 10839, 281], 'temperature': 0.0, 'avg_logprob': -0.09591292185955737, 'compression_ratio': 1.8676470588235294, 'no_speech_prob': 2.0758763639605604e-05}, {'id': 624, 'seek': 477576, 'start': 4775.76, 'end': 4782.320000000001, 'text': \" as the unlabeled accuracy score where we're just sort of measuring accuracies which are of all\", 'tokens': [382, 264, 32118, 18657, 292, 14170, 6175, 689, 321, 434, 445, 1333, 295, 13389, 5771, 20330, 597, 366, 295, 439], 'temperature': 0.0, 'avg_logprob': -0.08997969791806977, 'compression_ratio': 1.7914691943127963, 'no_speech_prob': 2.4642862626933493e-05}, {'id': 625, 'seek': 477576, 'start': 4782.320000000001, 'end': 4789.04, 'text': ' of the dependencies in the gold sentence and remember we have one dependency per word in the', 'tokens': [295, 264, 36606, 294, 264, 3821, 8174, 293, 1604, 321, 362, 472, 33621, 680, 1349, 294, 264], 'temperature': 0.0, 'avg_logprob': -0.08997969791806977, 'compression_ratio': 1.7914691943127963, 'no_speech_prob': 2.4642862626933493e-05}, {'id': 626, 'seek': 477576, 'start': 4789.04, 'end': 4795.280000000001, 'text': \" sentence so here we have five how many of them are correct and that's our unlabeled accuracy\", 'tokens': [8174, 370, 510, 321, 362, 1732, 577, 867, 295, 552, 366, 3006, 293, 300, 311, 527, 32118, 18657, 292, 14170], 'temperature': 0.0, 'avg_logprob': -0.08997969791806977, 'compression_ratio': 1.7914691943127963, 'no_speech_prob': 2.4642862626933493e-05}, {'id': 627, 'seek': 477576, 'start': 4795.280000000001, 'end': 4803.4400000000005, 'text': \" score of 80 percent but a slightly more rigorous and valuation is to say well no we're also going\", 'tokens': [6175, 295, 4688, 3043, 457, 257, 4748, 544, 29882, 293, 38546, 307, 281, 584, 731, 572, 321, 434, 611, 516], 'temperature': 0.0, 'avg_logprob': -0.08997969791806977, 'compression_ratio': 1.7914691943127963, 'no_speech_prob': 2.4642862626933493e-05}, {'id': 628, 'seek': 480344, 'start': 4803.44, 'end': 4809.28, 'text': \" to label them and we're going to say that this is the subject that's actually called the root\", 'tokens': [281, 7645, 552, 293, 321, 434, 516, 281, 584, 300, 341, 307, 264, 3983, 300, 311, 767, 1219, 264, 5593], 'temperature': 0.0, 'avg_logprob': -0.07039672678167169, 'compression_ratio': 1.6494252873563218, 'no_speech_prob': 5.043434794060886e-05}, {'id': 629, 'seek': 480344, 'start': 4809.28, 'end': 4819.44, 'text': \" this one's the object so these dependencies have labels and you also need to get the grammatical\", 'tokens': [341, 472, 311, 264, 2657, 370, 613, 36606, 362, 16949, 293, 291, 611, 643, 281, 483, 264, 17570, 267, 804], 'temperature': 0.0, 'avg_logprob': -0.07039672678167169, 'compression_ratio': 1.6494252873563218, 'no_speech_prob': 5.043434794060886e-05}, {'id': 630, 'seek': 480344, 'start': 4819.44, 'end': 4826.32, 'text': \" relation label right and so that's then referred to as labeled accuracy score and although I got\", 'tokens': [9721, 7645, 558, 293, 370, 300, 311, 550, 10839, 281, 382, 21335, 14170, 6175, 293, 4878, 286, 658], 'temperature': 0.0, 'avg_logprob': -0.07039672678167169, 'compression_ratio': 1.6494252873563218, 'no_speech_prob': 5.043434794060886e-05}, {'id': 631, 'seek': 482632, 'start': 4826.32, 'end': 4834.24, 'text': ' those two right for that is hmm I guess according to this example actually this is wrong it looks', 'tokens': [729, 732, 558, 337, 300, 307, 16478, 286, 2041, 4650, 281, 341, 1365, 767, 341, 307, 2085, 309, 1542], 'temperature': 0.0, 'avg_logprob': -0.09470578757199374, 'compression_ratio': 1.7725118483412323, 'no_speech_prob': 1.3815071724820882e-05}, {'id': 632, 'seek': 482632, 'start': 4834.24, 'end': 4841.5199999999995, 'text': \" like I got oh no this is wrong there sorry that one's wrong there okay so I only got two of the\", 'tokens': [411, 286, 658, 1954, 572, 341, 307, 2085, 456, 2597, 300, 472, 311, 2085, 456, 1392, 370, 286, 787, 658, 732, 295, 264], 'temperature': 0.0, 'avg_logprob': -0.09470578757199374, 'compression_ratio': 1.7725118483412323, 'no_speech_prob': 1.3815071724820882e-05}, {'id': 633, 'seek': 482632, 'start': 4842.16, 'end': 4848.4, 'text': ' dependencies correct in the sense that I both got what depends on what and the label correct', 'tokens': [36606, 3006, 294, 264, 2020, 300, 286, 1293, 658, 437, 5946, 322, 437, 293, 264, 7645, 3006], 'temperature': 0.0, 'avg_logprob': -0.09470578757199374, 'compression_ratio': 1.7725118483412323, 'no_speech_prob': 1.3815071724820882e-05}, {'id': 634, 'seek': 482632, 'start': 4848.4, 'end': 4855.679999999999, 'text': \" and so my labeled accuracy score is only 40 percent okay so I'll stop there now for the\", 'tokens': [293, 370, 452, 21335, 14170, 6175, 307, 787, 3356, 3043, 1392, 370, 286, 603, 1590, 456, 586, 337, 264], 'temperature': 0.0, 'avg_logprob': -0.09470578757199374, 'compression_ratio': 1.7725118483412323, 'no_speech_prob': 1.3815071724820882e-05}, {'id': 635, 'seek': 485568, 'start': 4855.68, 'end': 4863.12, 'text': ' introduction for dependency parsing and I still have an IOU which is how we can then bring', 'tokens': [9339, 337, 33621, 21156, 278, 293, 286, 920, 362, 364, 286, 4807, 597, 307, 577, 321, 393, 550, 1565], 'temperature': 0.0, 'avg_logprob': -0.09039834288300061, 'compression_ratio': 1.550561797752809, 'no_speech_prob': 1.9166771380696446e-05}, {'id': 636, 'seek': 485568, 'start': 4863.12, 'end': 4869.360000000001, 'text': \" neural nets into this picture and how they can be used to improve dependency parsing so I'll\", 'tokens': [18161, 36170, 666, 341, 3036, 293, 577, 436, 393, 312, 1143, 281, 3470, 33621, 21156, 278, 370, 286, 603], 'temperature': 0.0, 'avg_logprob': -0.09039834288300061, 'compression_ratio': 1.550561797752809, 'no_speech_prob': 1.9166771380696446e-05}, {'id': 637, 'seek': 486936, 'start': 4869.36, 'end': 4886.16, 'text': ' do that at the start of next time before then proceeding further into neural language models', 'tokens': [50364, 360, 300, 412, 264, 722, 295, 958, 565, 949, 550, 41163, 3052, 666, 18161, 2856, 5245, 51204], 'temperature': 0.0, 'avg_logprob': -0.2138073569849918, 'compression_ratio': 1.1948051948051948, 'no_speech_prob': 7.560840913356515e-06}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = 'Stanford_Speech_to_Text_Dump/Stanford CS224N - NLP w⧸ DL ｜ Winter 2021 ｜ Lecture 4 - Syntactic Structure and Dependency Parsing [PSGIodTN3KE].json'\n",
    "\n",
    "f = open(path)\n",
    "res = json.load(f)\n",
    "\n",
    "audio_segments = res['segments']\n",
    "print(audio_segments)\n",
    "# for segment in audio_segments['segments']:\n",
    "#   start_time = segment['start']\n",
    "#   end_time = segment['end']\n",
    "#   transcribed_text = segment['text']\n",
    "#   print(start_time, end_time, transcribed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d30f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 12.48  Okay, so for today, we're actually going to take a bit of a change of pace from what\n",
      "12.48 19.28  the last couple of lectures are have been about and we're going to focus much more on linguistics\n",
      "19.28 25.92  and natural language processing and so in particular, we're going to start looking at the topic\n",
      "25.92 35.44  of dependency parsing. And so this is the plan of what to go about through today. So I'm going\n",
      "35.44 40.96  to start out by going through some ideas that have been used in the syntactic structure of languages\n",
      "40.96 48.0  of constituency and dependency and introduce those and then focusing in more independent\n",
      "48.0 55.44  dependency structure, I'm then going to look at dependency grammars and dependency tree banks\n",
      "55.44 60.08  and then having done that, we're then going to move back into thinking about how to build\n",
      "60.08 65.52  natural language processing systems and so I'm going to introduce the idea of transition-based\n",
      "65.52 72.24  dependency parsing and then in particular having developed that idea, I'm going to talk about\n",
      "72.24 78.4  a way to build a simple but highly effective neural dependency parser. And so this simple\n",
      "78.4 83.44  highly effective neural dependency parser is essentially what we'll be asking you to build\n",
      "83.44 89.12  in the third assignment. So in some sense, we're getting a little bit ahead of ourselves here\n",
      "89.12 96.0  because in week two of the class, we teach you how to do both assignments two and three, but all\n",
      "96.0 102.64  of this material will come in really useful. Before I get underway, just a couple of announcements.\n",
      "104.56 111.84  So for a site, again for assignment two, you don't yet need to use the PyTorch framework,\n",
      "111.84 118.96000000000001  but now's a good time to work on getting PyTorch installed for your Python programming.\n",
      "119.76 125.60000000000001  Assignment three is in part also and in production using PyTorch, it's got a lot of scaffolding\n",
      "125.60000000000001 133.44  included in the assignment, but beyond that, this Friday, we've got a PyTorch tutorial and thoroughly\n",
      "133.44 141.52  encourage you to come along to that as well, look for it under the Zoom tab. And in the second half\n",
      "141.52 148.32000000000002  of the first day of week four, we have an explicit class that partly focuses on the final projects\n",
      "148.32000000000002 154.0  and what the choices are for those, but it's never too late to start thinking about the final project\n",
      "154.0 160.4  and what kind of things you want to do for the final project. So do come meet with people,\n",
      "160.4 166.08  there are sort of resources on the course pages about what different TAs know about. I've also\n",
      "166.08 170.8  talked to a number of people about final projects, but clearly I can't talk to everybody.\n",
      "170.8 174.4  So I encourage you to also be thinking about what you want to do for final projects.\n",
      "176.16000000000003 185.68  Okay, so what I wanted to do today was introduce how people think about the structure of sentences\n",
      "186.8 192.64000000000001  and put structure on top of them to explain how human language conveys meaning.\n",
      "193.44 199.52  And so our starting point for meaning and essentially what we've dealt with with word vectors\n",
      "199.52 206.88000000000002  up until now is we have words. And words are obviously an important part of the\n",
      "206.88000000000002 215.76000000000002  meaning of human languages. But for words in human languages, there's more that we can do with them\n",
      "215.76000000000002 222.56  in thinking about how to structure sentences. So in particular, the first most basic way that we\n",
      "222.56 229.12  think about words when we are thinking about how sentences are structured is we give to them what's\n",
      "229.12 238.0  called a part of speech. We can say that cat is a noun, buy is a preposition, doors and\n",
      "238.0 247.68  other noun, cuddly is an adjective. And then for the word, if it was given a different part of\n",
      "247.68 252.8  speech, if you saw any parts of speech in school, it was probably your told it was an article.\n",
      "252.8 260.64  Sometimes that is just put into the class of adjectives in modern linguistics and what you'll see\n",
      "260.64 266.64  in the resources that we use, words like that are referred to as determiners. And the idea is\n",
      "266.64 271.92  that there's a bunch of words includes art and art, but also other words like this and that,\n",
      "273.92 282.64  or even every which are words that occur at the beginning of something like the cuddly cat\n",
      "282.64 289.59999999999997  which have a determinative function of sort of picking out which cats that they're referring to.\n",
      "289.59999999999997 295.76  And so we refer to those as determiners. But it's not the case that when we want to communicate with\n",
      "295.76 303.76  language that we just have this word salad where we say a bunch of words, we just say you know whatever\n",
      "304.4 312.15999999999997  leaking kitchen tap and let the other person put it together, we put words together in a particular\n",
      "312.16 319.84000000000003  way to express meanings. And so therefore, languages have larger units of putting meaning together.\n",
      "319.84000000000003 330.24  And the question is how we represent and think about those. Now in modern work and particular\n",
      "330.24 337.84000000000003  in modern United States linguistics or even what you see in computer science classes when thinking\n",
      "337.84 344.64  about formal languages, the most common way to approach this is with the idea of context-free\n",
      "344.64 350.96  grammars which you see at least a little bit of in 103 if you've done 103, what a linguist would\n",
      "350.96 357.67999999999995  often refer to as free structure grammars. And the idea there is to say well there are bigger\n",
      "357.67999999999995 365.12  units in languages that we refer to as phrases. So something like the cuddly cat is a cat\n",
      "365.12 373.2  with some other words modifying it. And so we refer to that as a noun phrase. But then we have\n",
      "374.32 384.32  ways in which phrases can get larger by building things inside phrases. So the door here is also a\n",
      "384.32 390.48  noun phrase. But then we can build something bigger around it with a preposition. So this is a\n",
      "390.48 397.04  preposition. And then we have a prepositional phrase. And in general we can keep going. So we\n",
      "397.04 403.92  can then make something like the cuddly cat by the door. And then the door is a noun phrase.\n",
      "403.92 411.6  The cuddly cat is a noun phrase by the door is a prepositional phrase. But then when we put it\n",
      "411.6 418.8  all together the whole of this thing becomes a bigger noun phrase. And so it's working with these\n",
      "418.8 427.12  ideas of nested phrases, what in context free grammar terms you would refer to as non-terminals.\n",
      "428.16 433.92  So noun phrase and prepositional phrase would be non-terminals in the context free grammar.\n",
      "433.92 440.64  We can build up a bigger structure of human languages. So let's just do that for a little bit\n",
      "440.64 449.2  to review what happens here. So we start off saying okay you can say the cat and a dog. And so those\n",
      "449.2 455.68  are noun phrases. And so we want a rule that can explain those. So we could say a noun phrase goes\n",
      "455.68 463.68  to the termina noun. And then somewhere over the side we'd have a lexicon. And in our lexicon\n",
      "463.68 474.32  we'd say that dog is a noun and cat is a noun and is a determiner and that is a determiner.\n",
      "475.04 482.88  Okay so then we notice you can do a bit more than that. So you can say things like the large cat\n",
      "483.92 491.12  a barking dog. So that suggests we can have a noun phrase after the determiner.\n",
      "491.12 497.76  There can optionally be an adjective and then there's the noun and that can explain some things we\n",
      "497.76 506.72  can say. But we can also say the cat by the door or a barking dog in a crate. And so we can also\n",
      "506.72 514.48  put a prepositional phrase at the end and that's optional. But you can combine it together with an\n",
      "514.48 522.5600000000001  adjective for the example I gave like a barking dog on the table. And so that a grids grammar can\n",
      "522.5600000000001 531.2  handle that. So then we'll keep on and say well actually you can use multiple adjectives so you\n",
      "531.2 540.24  can say a large barking dog or a large barking cuddly cat. No maybe not. Well sentences like that.\n",
      "540.24 545.04  So we have any number of adjectives which we can represent with a star. Pots referred to as the\n",
      "545.04 554.24  cleanie star. So that's good. But I forgot a bit actually. For by the door I have to have a rule\n",
      "554.24 561.04  for producing by the door. So I also need a rule that's a prepositional phrase goes to a preposition\n",
      "561.04 569.28  followed by a noun phrase. And so then I also have to have prepositions and that can be in or on\n",
      "569.28 577.8399999999999  or by. Okay. And I can make other sentences of course with this as well like the large crate on\n",
      "577.8399999999999 585.52  the table or something like that or the large crate on the large table. Okay. So I chug along\n",
      "586.48 592.9599999999999  and then well I could have something like talk to the cat. And so now I need more stuff. So talk\n",
      "592.96 600.48  is a verb and two is still looks like a preposition. So I need to be able to make up something\n",
      "601.2 611.44  with that as well. Okay. So what I can do is say I can also have a rule for a verb phrase that\n",
      "611.44 619.52  goes to a verb and then after that for something like talk to the cat that it can take a prepositional\n",
      "619.52 632.72  phrase after it. And then I can say that the verb goes to talk or walked. Okay. Then I can pass\n",
      "632.72 640.16  and then I can cover those sentences. Oops. Okay. So that's that's the end of what I have here.\n",
      "640.16 649.1999999999999  So in this sort of a way I'm handwriting a grammar. So here is now I have this grammar\n",
      "650.8 659.6  and a lexicon. And for the examples that I've written down here, this grammar and this lexicon\n",
      "659.6 670.0  is sufficient to pause these sort of fragments of showing expansion that I just wrote down. I mean,\n",
      "670.0 676.4  of course there's a lot more to English than what you see here. Right. So if I have something like\n",
      "677.12 689.2  the cat walked behind the dog, then I need some more grammar rules. So it seems then I need a rule\n",
      "689.2 694.48  that says I can have a sentence that goes to a noun phrase followed by a verb phrase.\n",
      "695.5200000000001 706.08  And I can keep on doing things of this sort. Let's see one question that Ruth Ann asked was about\n",
      "706.88 713.2800000000001  what do the brackets mean and is the first np different from the second.\n",
      "713.28 723.04  And so for this notation on the brackets here, I mean, this is actually a common notation that's\n",
      "723.04 730.3199999999999  used in linguistics. It's sort of in some sense a little bit different to traditional computer\n",
      "730.3199999999999 738.4  science notation since the star is used in both to mean zero or more of something. So you could have\n",
      "738.4 745.28  zero one two three four five adjectives. Somehow it's usual in linguistics that when you're using\n",
      "745.28 752.0  the star, you also put parentheses around it to mean it's optional. So sort of parentheses and\n",
      "752.0 758.8  star are used together to mean any number of something. When it's parentheses just by themselves,\n",
      "758.8 769.4399999999999  that's then meaning zero or one. And then four are these two noun phrases different? No, they're both\n",
      "769.4399999999999 776.56  noun phrase rules. And so in our grammar, we can have multiple rules that expand noun phrase in\n",
      "776.56 784.88  different ways. But, you know, actually in my example here, my second rule because I wrote it\n",
      "784.88 791.28  quite generally, it actually covers the first rule as well. So actually at that point, I can cross out\n",
      "791.28 797.04  this first rule because I don't actually need it in my grammar. But in general, you know, you have\n",
      "797.04 805.28  a choice between writing multiple rules for noun phrase goes to categories, which effectively gives\n",
      "805.28 815.28  your disjunction or working out by various syntactic conventions how to compress them together. Okay.\n",
      "816.48 822.72  So that was what gets referred to in natural language processing as constituency grammars,\n",
      "823.68 830.0799999999999  where the standard form of constituency grammar is a context-free grammar of the sort that\n",
      "830.08 836.88  I trust you saw at least a teeny bit of either in CS 103 or something like a programming language\n",
      "836.88 843.5200000000001  as compilers formal languages class. There are other forms of grammars that also pick out constituency.\n",
      "844.1600000000001 849.2800000000001  There are things like tree adjoining grammars, but I'm not going to really talk about any of those now.\n",
      "849.2800000000001 855.84  What I actually want to present is a somewhat different way of looking at grammar, which is referred to\n",
      "855.84 864.1600000000001  as dependency grammar, which puts a dependency structure over sentences. Now actually,\n",
      "864.1600000000001 870.1600000000001  it's not that these two ways of looking at grammar have nothing to do with each other. I mean,\n",
      "870.1600000000001 876.5600000000001  there's a whole formal theory about the relationships between different kinds of grammars,\n",
      "876.5600000000001 884.24  and you can very precisely state relationships and isomorphisms between different grammars of\n",
      "884.24 890.96  different kinds. But on the surface, these two kinds of grammars look sort of different and\n",
      "890.96 900.8  emphasize different things. And for reasons of their sort of closeness to picking out relationships\n",
      "900.8 909.2  and sentences and their ease of use, it turns out that in modern natural language processing,\n",
      "909.2 917.36  starting, I guess, around 2000, sort of really in the last 20 years, NLP people have really swung\n",
      "917.36 923.84  behind dependency grammars. So if you look around now where people are using grammars in NLP,\n",
      "923.84 929.76  by far the most common thing that's being used is dependency grammars. So I'm going to teach us\n",
      "929.76 936.24  today a bit about those. And for what we're going to build in assignment three is building\n",
      "936.24 944.4  using supervised learning and neural dependency parser. So the idea of dependency grammar is that\n",
      "944.4 952.64  when we have a sentence, what we're going to do is we're going to say for each word, what other\n",
      "952.64 960.5600000000001  words modify it. So what we're going to do is when we say the large crate, we're going to say,\n",
      "960.56 969.04  okay, well, large is modifying crate and that is modifying crate in the kitchen. That is modifying\n",
      "969.04 978.0799999999999  kitchen by the door. That is modifying door. And so I'm showing modification, a dependency or\n",
      "978.0799999999999 985.3599999999999  an attachment relationship by drawing an arrow from the head to what's referred to\n",
      "985.36 993.44  in dependency grammar as the dependent. The thing that modifies further specifies or attaches\n",
      "994.24 1004.16  to the head. Okay, so that's the start of this. Well, another dependency that is that, well,\n",
      "1006.16 1012.4  looking in the large crate, that where you're looking is in the large crate. So you're going to\n",
      "1012.4 1020.24  want to have the large in the large crate as being a dependent of look. And so that's also going\n",
      "1020.24 1028.96  to be a dependency relationship here. And then there's one final bit that might seem a little bit\n",
      "1028.96 1035.12  confusing to people. And that's actually when we have these prepositions, there are two ways that\n",
      "1035.12 1043.12  you can think that this might work. So if it was something like look in the crate,\n",
      "1045.76 1053.12  that seems like that is a dependent of crate, but you could think that you want to say look in\n",
      "1053.76 1059.6  and it's in the crate and give this dependency relationship with the sort of preposition\n",
      "1059.6 1066.8799999999999  as sort of thinking of it as the head of what was before our prepositional phrase. And that's\n",
      "1066.8799999999999 1075.6799999999998  a possible strategy in the dependency grammar. But what I'm going to show you today and what you're\n",
      "1075.6799999999998 1082.8  going to use in this assignment is dependency grammars that follow the representation of universal\n",
      "1082.8 1089.04  dependencies. And universal dependencies is a framework which actually I was involved in creating,\n",
      "1089.04 1095.84  which was set up to try and give a common dependency grammar over many different human languages.\n",
      "1095.84 1103.12  And in the design decisions that were made in the context of designing universal dependencies,\n",
      "1104.48 1112.96  what we decided was that for what in some languages you use prepositions, lots of other\n",
      "1112.96 1119.92  languages make much more use of case marking. So if you've seen something like German, you've seen\n",
      "1119.92 1128.48  more case markings like genitive and date of cases. And in other languages like Latin or Finnish,\n",
      "1129.76 1135.28  lots of Native American languages, you have many more case markings again, which cover most of\n",
      "1135.28 1143.68  the role of prepositions. So in universal dependencies, essentially in the crate is treated like a\n",
      "1143.68 1152.56  case marked noun. And so what we say is that the in is also a dependent of crate and then you're\n",
      "1152.56 1161.44  looking in the crate. So in the structure we adopt in as dependent of crate, this in as a\n",
      "1161.44 1170.16  dependent of kitchen, this by as a dependent of door. And then we have these prepositional phrases\n",
      "1170.16 1177.2  in the kitchen by the door and we want to work out well what they modify. Well in the kitchen\n",
      "1178.3200000000002 1183.8400000000001  is modifying crate right because it's a crate in the kitchen. So we're going to say that it's\n",
      "1183.84 1192.56  this piece is a dependent of crate. And then well what about by the door? Well it's not really\n",
      "1192.56 1199.6799999999998  meaning that's a kitchen by the door and it's not meaning to look by the door. Again it's a crate\n",
      "1199.6799999999998 1206.32  by the door. And so what we're going to have is the crate also has door as a dependent. And so\n",
      "1206.32 1220.96  that gives us our full dependency structure of this sentence. Okay. And so that's a teeny introduction\n",
      "1220.96 1227.04  to syntactic structure. I'm going to say a bit more about it and give a few more examples.\n",
      "1227.6 1233.76  But let me just for a moment sort of say a little bit about why are we interested in syntactic\n",
      "1233.76 1241.28  structure? Why do we need to know the structure of sentences? And this gets into how does human\n",
      "1241.28 1250.64  languages work? So human languages can can communicate very complex ideas. I mean in fact you know\n",
      "1250.64 1257.36  anything that humans know how to communicate to one another they communicate pretty much by\n",
      "1257.36 1265.76  using words. So we can structure and communicate very complex ideas. But we can't communicate a\n",
      "1265.76 1274.9599999999998  really complex idea by one word. We can't just you know choose a word like you know empathy and say\n",
      "1274.9599999999998 1279.6  it with a lot of meaning and say empathy and the other person's meant to understand everything\n",
      "1279.6 1286.56  about what that means. Right. We have to compose a complex meaning that explains things by putting\n",
      "1286.56 1294.0  words together into bigger units. And the syntax of a language allows us to put words together\n",
      "1294.0 1302.08  into bigger units where we can build up and convey to other people a complex meaning. And so\n",
      "1302.08 1308.6399999999999  then the listener doesn't get this syntactic structure. Right. The syntactic structure of the\n",
      "1308.6399999999999 1316.0  sentence is hidden from the listener. All the listener gets is a sequence of words one after another\n",
      "1316.0 1323.6  bang bang bang. So the listener has to be able to do what I was just trying to do in this example\n",
      "1323.6 1331.52  that as the sequence of words comes in that the listener works out which words modify which\n",
      "1331.52 1338.72  are the words and therefore can construct the structure of the sentence and hence the meaning of\n",
      "1338.72 1347.2  the sentence. And so in the same way if we want to build clever neural net models that can understand\n",
      "1347.2 1354.24  the meaning of sentences those clever neural net models also have to understand what is this\n",
      "1354.24 1359.68  structure of the sentence so that they can interpret the language correctly. And we'll go through\n",
      "1359.68 1366.64  some examples and see more of that. Okay. So the fundamental point that we're going to sort of\n",
      "1366.64 1375.3600000000001  spend a bit more time on is that these choices of how you build up the structure of a language\n",
      "1376.3200000000002 1384.48  change the interpretation of the language and a human listener or equally a natural language\n",
      "1384.48 1393.2800000000002  understanding program has to make in a sort of probabilistic fashion choices as to which words\n",
      "1393.28 1399.84  modify I depend upon which other words so that they're coming up with the interpretation of the\n",
      "1399.84 1408.96  sentence that they think was intended by the person who said it. Okay. So to get a sense of this\n",
      "1408.96 1416.6399999999999  and how sentence structure is interesting and difficult what I'm going to go through now is a\n",
      "1416.64 1424.0800000000002  few examples of different ambiguities that you find in natural language and I've got some funny\n",
      "1424.0800000000002 1432.88  examples from newspaper headlines but these are all real natural language ambiguities that you find\n",
      "1432.88 1439.76  throughout natural language. Well at this point I should say this is where I'm being guilty of\n",
      "1439.76 1447.12  saying natural language but I'm meaning in English. Some of these ambiguities you find in lots of\n",
      "1447.12 1454.72  other languages as well but which ambiguities that are for syntactic structure partly depend on the\n",
      "1454.72 1461.12  details of the language. So different languages have different syntactic constructions, different\n",
      "1461.12 1469.28  word orders, different amounts of words having different forms of words like case markings. And so\n",
      "1469.28 1477.12  depending on those details there might be different ambiguities. So here's one ambiguity which is\n",
      "1477.12 1485.76  one of the commentest ambiguities in English. So San Jose cops kill man with knife. So this sentence\n",
      "1485.76 1495.6  has two meanings either it's the San Jose cops who are killing a man and they're killing a man\n",
      "1495.6 1503.76  with a knife. And so that corresponds to a dependency structure where the San Jose cops\n",
      "1503.76 1513.6799999999998  are the subject of killing the man is the object of killing and then the knife is then the instrument\n",
      "1513.6799999999998 1521.04  with which they're doing the killing so that the knife is an oblique modifier for the instrument\n",
      "1521.04 1528.48  of killing. And so that's one possible structure for this sentence but it's probably not the right one.\n",
      "1529.36 1537.52  So what it actually probably was was that it was a man with a knife and the San Jose cops killed\n",
      "1537.52 1549.12  the man. So that corresponds to the knife then being a noun modifier of the man and then kill\n",
      "1549.12 1555.6799999999998  is still killing the man. So the man is the object of killing and the cops are still the subject.\n",
      "1557.1999999999998 1565.12  And so whenever you have a prepositional phrase like this that's coming further on in a sentence\n",
      "1566.0 1573.36  there's a choice of how to interpret it. It could be either interpreted as modifying a noun\n",
      "1573.36 1580.24  phrase that comes before it or it can be interpreted as modifying a verb that comes before it.\n",
      "1580.24 1586.56  So systematically in English you get these prepositional phrase attachment ambiguities\n",
      "1586.56 1594.0  throughout all of our sentences but you know to give two further observations on that you know\n",
      "1594.0 1603.28  the first observation is you know you encounter sentences with prepositional phrase attachment\n",
      "1604.56 1611.2  ambiguities every time you read a newspaper article every time you talk to somebody but most of\n",
      "1611.2 1618.96  the time you never notice them and that's because our human brains are incredibly good at considering\n",
      "1618.96 1625.28  the possible interpretations and going with the one that makes sense according to context.\n",
      "1627.44 1634.96  The second comment as I said different human languages expose different ambiguities. So for\n",
      "1634.96 1640.88  example this is an ambiguity that you normally don't get in Chinese because in Chinese\n",
      "1641.76 1648.48  prepositional phrases modifying a verb are normally placed before the verb and so there you\n",
      "1648.48 1655.76  don't standedly get this ambiguity but you know there are different other ambiguities that you find\n",
      "1655.76 1663.2  commonly in Chinese sentences. Okay so this ambiguity you find everywhere because prepositional\n",
      "1663.2 1669.52  phrases are really common at the right ends of sentences so here's another one scientist count\n",
      "1669.52 1676.32  whales from space so that gives us these two possible interpretations that there are whales from\n",
      "1676.32 1684.72  space and scientists accounting them and then the other one is how the scientists accounting the\n",
      "1684.72 1691.04  whales is that they're counting them from space and they're using satellites to count the\n",
      "1691.04 1698.1599999999999  sales which is the correct interpretation that the newspaper hopes that you're getting.\n",
      "1698.16 1710.72  And this problem gets much much more complex because many sentences in English have prepositional\n",
      "1710.72 1717.76  phrases all over the place so here's the kind of boring sentence that you find in the financial\n",
      "1717.76 1725.76  news the board approved its acquisition by Royal Trust Co Ltd of Toronto for $27 a share at its\n",
      "1725.76 1731.6  monthly meeting and while if you look at the structure of this sentence what we find is you know\n",
      "1731.6 1741.6  here's a verb then here's the object noun phrase so we've got the object noun phrase here and then\n",
      "1741.6 1748.32  after that what do we find well we find a prepositional phrase another prepositional phrase another\n",
      "1748.32 1755.2  prepositional phrase and another prepositional phrase and how to attach each of these is then\n",
      "1755.2 1762.4  ambiguous so the basic rule of how you can attach them is you can attach them to things to the left\n",
      "1763.1200000000001 1771.2  providing you don't create crossing attachments so in principle by Royal Trust Co Ltd\n",
      "1771.2 1779.1200000000001  could be attached to either approved or acquisition but in this case by Royal Trust Co Ltd it is\n",
      "1779.12 1792.3999999999999  the acquirer so it's a modifier of the acquisition okay so then we have of Toronto so of Toronto\n",
      "1792.3999999999999 1799.1999999999998  could be modifying Royal Trust Co Ltd it could be modifying the acquisition or it can be modifying\n",
      "1799.1999999999998 1806.1599999999999  the approved and in this case the of Toronto is telling you more about the company and so\n",
      "1806.16 1815.28  it's a modifier of Royal Trust Co Ltd okay so then the next one is for $27 a share and that could\n",
      "1815.28 1823.8400000000001  be modifying Toronto Royal Trust Co Ltd the acquisition or the approving and well in this case\n",
      "1825.1200000000001 1832.24  that's talking about the price of the acquisition so this one is mod go jumps back and this is now\n",
      "1832.24 1841.52  prepositional phrase that's modifying the acquisition and then at the end at its monthly meeting\n",
      "1842.48 1849.6  well that's where the approval is happening by the by the board so rather than any of these\n",
      "1849.6 1858.72  preceding four noun phrases at its monthly meeting is modifying the approval and so it\n",
      "1858.72 1865.76  attaches right back there and this example is kind of too big and so I couldn't fit it in one line\n",
      "1865.76 1871.92  but as I think maybe you can see that you know none of these dependencies cross each other\n",
      "1871.92 1879.28  and they connect at different places ambiguously so because we can chain these prepositions like this\n",
      "1879.28 1886.72  and attach them at different places like this human language sentences are actually extremely\n",
      "1886.72 1897.92  ambiguous so the number if you have a sentence with K prepositional phrases at the end of\n",
      "1897.92 1904.96  earth where here we have K equals four the number of parses this sentence has the number of different\n",
      "1904.96 1910.96  ways you can make these attachments is given by the cutler numbers so the cutler numbers are\n",
      "1910.96 1918.0  an exponentially growing series which arises in many tree like context so if you're doing something\n",
      "1918.0 1924.8  like triangulations of a polygon you get cutler numbers if you're doing triangulation and graphical\n",
      "1924.8 1931.28  models in CS228 you get cutler numbers but we don't need to worry about the details here the central\n",
      "1931.28 1938.0  point is this is an exponential series and so you're getting an exponential number of parses in terms\n",
      "1938.0 1944.64  of the number of prepositional phrases and so in general you know the number of parses human\n",
      "1944.64 1951.84  languages have is exponential in their length which is kind of bad news because if you're then trying\n",
      "1951.84 1959.28  to enumerate all the parses it you might fear that you really have to do a ton of work the thing to\n",
      "1959.28 1968.24  notice about structures like these prepositional phrase attachment ambiguities is that there's nothing\n",
      "1968.24 1977.04  that resolves these ambiguities in terms of the structure of the sentence so if you've done something\n",
      "1977.04 1983.84  like looked at the kind of grammars that are used in compilers that the grammars used in compilings\n",
      "1983.84 1991.04  and compilers for programming languages are mainly made to be unambiguous and to the extent that\n",
      "1991.04 1998.56  there are any ambiguities there are default rules that are used to say choose this one particular\n",
      "1999.4399999999998 2007.04  parse tree for your piece of a programming language and human languages just aren't like that\n",
      "2007.04 2013.6  they're globally ambiguous and the listening human is just meant to be smart enough to figure out\n",
      "2013.6 2024.24  what was intended so the analogy would be that you know in programming languages when you're working\n",
      "2024.24 2034.3999999999999  out what does an else clause modify well you've got the answer that you can either look at the\n",
      "2034.4 2040.3200000000002  curly braces to work out what the else clause modifies or if you're using Python you look at the\n",
      "2040.3200000000002 2047.68  indentation and it tells you what the else clause modifies where by contrast for human languages\n",
      "2048.8 2056.32  the it would be just write down else something doesn't matter how you do it you don't need parentheses\n",
      "2056.32 2061.6800000000003  you don't need indentation the human being will just figure out what the else clause is meant to\n",
      "2061.68 2071.6  pair up with okay lots of other forms of ambiguities in human languages so let's look at a few others\n",
      "2071.6 2078.0  another one that is very common over all sorts of languages is coordination scope ambiguities\n",
      "2079.04 2084.72  so here's a sentence shuttle veteran and long time that's your executive Fred Gregory appointed\n",
      "2084.72 2093.7599999999998  to board well this is an ambiguous sentence there are two possible readings of this one reading\n",
      "2093.7599999999998 2099.12  is that there are two people there's a shuttle veteran and there's a long time that's your\n",
      "2099.12 2108.3999999999996  executive Fred Gregory and they were both appointed to the board two people and the other possibility\n",
      "2108.4 2117.04  is there's someone named Fred Gregory who's a shuttle veteran and long time that's your executive\n",
      "2117.84 2126.48  and they're appointed to the verb one person and these two interpretations again correspond to having\n",
      "2126.48 2136.08  different paths structures so in one structure we've got a coordination of the shuttle veteran\n",
      "2136.08 2143.7599999999998  and the long time that's your executive Fred Gregory coordinated together in one case these\n",
      "2143.7599999999998 2153.2799999999997  are coordinated and then Fred Gregory specifies the name of the Nassar executive so it's then\n",
      "2154.64 2161.04  specifying who that executive is where the what in the other one the shuttle veteran and long time\n",
      "2161.04 2169.7599999999998  Nassar executive all together is then something that is a modifier of Fred Gregory\n",
      "2172.88 2180.48  okay so one time this is the unit that modifies Fred Gregory in the other one up here just long time\n",
      "2180.48 2187.12  Nassar executive modifies Fred Gregory and then that's can join together with the shuttle veteran\n",
      "2187.12 2195.04  and so that also gives different interpretations so this is a slightly reduced example of the\n",
      "2195.04 2204.0  I mean in newspaper headlines tend to be more ambiguous than many other pieces of text because\n",
      "2204.0 2210.08  they're written in this short and formed get things to fit and this isn't especially short and\n",
      "2210.08 2219.7599999999998  form whereas actually left out in explicit conjunction but this headline says doctor no heart cognitive\n",
      "2219.7599999999998 2226.72  issues and this was after I guess one of Trump it was after Trump's first physical and while this\n",
      "2226.72 2232.4  is an ambiguity because there are two ways that you can read this you can either read this as saying\n",
      "2232.4 2242.08  doctor no heart and cognitive issues which gives you one interpretation instead of that the way we\n",
      "2242.08 2252.4  should read it is that it's heart or cognitive and so it's then saying no heart or cognitive issues\n",
      "2252.4 2261.76  and we have a different narrower scope of the coordination and then we get a different reading.\n",
      "2263.84 2271.28  Okay I want to give a couple more examples of different kinds of ambiguities another one you see\n",
      "2271.28 2277.76  quite a bit is when you have modifiers that are adjectives and adverbs that there are different\n",
      "2277.76 2284.2400000000002  ways that you don't have things modifying other things this example is a little bit not safe for\n",
      "2284.2400000000002 2294.6400000000003  work but here goes students get first hand job experience so this is an ambiguous sentence and again\n",
      "2294.6400000000003 2302.96  we can think of it as a syntactic ambiguity in terms of which things modify which other things\n",
      "2302.96 2314.96  so the nice polite way to render this sentence is that first is modifying hand so we've got first hand\n",
      "2314.96 2323.12  it's job experience so job is a compound now modifying experience and it's first hand experience\n",
      "2323.12 2333.6  so first hand is then modifying experience and then get is the object of our first hand job\n",
      "2333.6 2342.64  experience is the object of get and the students are the subject of get but if you have a smarty\n",
      "2342.64 2353.2799999999997  a mind you can interpret this a different way and in the alternative interpretation you then have hand\n",
      "2353.2799999999997 2364.64  going together with job and the first is then a modifier of experience and job is still a\n",
      "2364.64 2370.8799999999997  modifier of experience and so then you get this different power structure and different interpretation\n",
      "2370.88 2380.08  there okay one more example in a way this example similar to the previous one it's sort of having\n",
      "2381.36 2387.76  modifier pieces that can modify different things but rather than just being with individual adjectives\n",
      "2387.76 2396.7200000000003  or individual adverbs is then much larger units such as verb phrases can often have attachment\n",
      "2396.72 2404.3199999999997  ambiguities so this sentence headline is mutilated body washes up on Rio Beach to be used for\n",
      "2404.3199999999997 2411.6  Olympics Beach volleyball so we have this big verb phrase here of to be used for Olympics Beach\n",
      "2411.6 2421.2  volleyball and then again we have this attachment decision that we could either say that that\n",
      "2421.2 2433.4399999999996  big verb phrase is modifying i is attached to the Rio Beach or we could say no no the to be used\n",
      "2433.4399999999996 2443.7599999999998  for Olympics Beach volleyball that that is modifying the mutilated body and it's a body that's\n",
      "2443.7599999999998 2450.7999999999997  to be used for the Olympics Beach volleyball which gives the funny reading yeah so I hope that's\n",
      "2450.8 2458.8  giving you at least a little bit of a sense of how human language syntactic structure is complex\n",
      "2458.8 2466.1600000000003  and big u.s and to work out the intended interpretations you need to know something about that structure\n",
      "2467.44 2474.4  in terms of how much you need to understand i mean you know this is under linguistics class if\n",
      "2474.4 2479.6000000000004  you'd like to learn more about human language structure you can go off and do a syntax class\n",
      "2479.6 2486.08  but you know we're not really going to spend a lot of time working through language structure\n",
      "2486.08 2491.8399999999997  but there will be some questions on this in the assignment and so we're expecting that you can\n",
      "2491.8399999999997 2498.08  be at the level that you can have sort of some intuitions as to which words and phrases are\n",
      "2498.08 2504.0  modifying other words and phrases and therefore you could choose between two dependency analyses\n",
      "2504.0 2513.52  which ones correct okay i've spent quite a bit of time on that so better keep going okay so\n",
      "2514.4 2521.6  the general idea is that knowing this sort of syntactic structure of a sentence can help us\n",
      "2521.6 2527.84  with semantic interpretation i mean as well as just generally saying we can understand language\n",
      "2527.84 2533.68  it's also used in many cases for simple practical forms of semantic extraction so people\n",
      "2533.68 2539.2799999999997  such as in biomedical informatics often want to get out particular relations such as protein\n",
      "2539.2799999999997 2545.12  protein interactions and while here's a sentence the results demonstrated that kai c interacts\n",
      "2545.12 2555.04  rhythmically with sasa kai and kai b and commonly that people can get out those kind of relationships\n",
      "2555.04 2562.3999999999996  by looking at patterns of dependency relations with particular verbs so for the interacts verb\n",
      "2562.4 2568.4  if you have a pattern of something being the subject and something else being the noun modifier\n",
      "2568.4 2573.84  of interacts well that's an interaction relationship but it gets a bit more complicated than that\n",
      "2573.84 2579.52  as in this example because often there are conjunctions so you also have another pattern\n",
      "2579.52 2587.2000000000003  where you have also interactions between the subject and the noun modifiers conjunct\n",
      "2587.2 2597.12  which will allow us to also find the kai and kai b examples okay um so i've sort of given an informal\n",
      "2597.12 2604.7999999999997  tour of dependency grammar to just try and uh quickly um say a little bit more about formally\n",
      "2604.7999999999997 2613.7599999999998  what a dependency grammar is so in dependency syntax what we say is that the syntactic structure\n",
      "2613.76 2622.7200000000003  of a sentence consists of relations between pairs of words um and it's a binary asymmetric relation\n",
      "2622.7200000000003 2630.7200000000003  i we draw arrows between pairs of words which we call dependencies now normally dependency\n",
      "2630.7200000000003 2636.88  grammars then type those grammatical relation type those arrows to express what kind of\n",
      "2636.88 2642.8  relation that there is and so that they have some kind of taxonomy of grammatical relation so we\n",
      "2642.8 2648.4  might have a subject grammatical relation of verbal auxiliary grammatical relation and a bleak\n",
      "2648.4 2656.88  modifier grammatical relation we have some kind of typology of grammatical relations um so and we\n",
      "2656.88 2665.76  refer to the arrows going between the head is the head here and something that is a dependent of\n",
      "2665.76 2674.0800000000004  it so the subject of a verb is the dependent of the verb or when you have a noun modifier like\n",
      "2674.0800000000004 2685.36  our sort of cuddly cat we say that um cuddly is a dependent of cat and so cat is the head of cuddly\n",
      "2685.36 2695.5200000000004  cat and so normally um dependencies like in these examples form a tree which is formal it so\n",
      "2695.52 2704.16  it's not just any graph with arrows we have an graph which is connected a cyclic and has a single\n",
      "2704.16 2713.68  root so here's the root of the graph um and so that gives us a dependency tree analysis um dependency\n",
      "2713.68 2723.12  grammars have a really really long history um so the famous first linguist um was panini um who\n",
      "2723.12 2730.88  wrote about the structure of Sanskrit um and mainly he worked on the sound system of Sanskrit\n",
      "2730.88 2736.24  and how sounds change in various contexts which what linguists call phonology and the different\n",
      "2736.24 2743.04  forms of Sanskrit words Sanskrit has rich morphology of inflecting nouns and verbs for different\n",
      "2743.04 2750.88  cases and forms um but he also worked a little on the syntactic structure of Sanskrit censors\n",
      "2750.88 2758.32  and essentially what he proposed was the dependency grammar over Sanskrit sentences and it turns out\n",
      "2758.32 2765.44  that sort of from most of recorded history when then when people have then um gone on and tried to\n",
      "2765.44 2773.36  put structures over human sentences um what they have used is dependency grammars um so there was a\n",
      "2773.36 2780.4  lot of work in the first millennium by Arabic grammarians of trying to work out the grammar um\n",
      "2780.4 2786.8  structure of sentences and effectively what they used was but you know kind what I've just presented\n",
      "2786.8 2795.92  as a dependency grammar so compared to you know 2500 years of history the ideas of having context\n",
      "2795.92 2801.6800000000003  free grammars and having constituency grammars is actually a really really recent invention so it\n",
      "2801.6800000000003 2808.1600000000003  was really sort of in the middle of the 20th century that the ideas of um constituency grammar and\n",
      "2808.16 2814.0  context free grammars would develop first by wells in the forties and then by known chomsky in the\n",
      "2814.0 2821.12  early 50s leading to things like the chomsky hierarchy that you might see um CS 103 or formal\n",
      "2821.12 2830.3199999999997  languages class um so for modern work on dependency grammar using kind of the terminology and um\n",
      "2830.3199999999997 2836.3199999999997  notation that I've just introduced that's normally attributed to Lucian Tania who was a French\n",
      "2836.32 2844.0  linguist um in around the sort of middle of the 20th century as well um dependency grammar was\n",
      "2844.0 2851.1200000000003  widely used in the 20th century um in a number of places I mean in particular it tends to be\n",
      "2851.1200000000003 2857.52  sort of much more natural and easier to think about for languages that have a lot of different\n",
      "2857.52 2864.0  case markings on nouns like nomad of accused of genitive data of instrumental kind of cases like\n",
      "2864.0 2869.28  you get in the language like Latin or Russian and a lot of those languages have much\n",
      "2869.28 2875.44  free word order than English so the subject or objective you know in English the subject has to\n",
      "2875.44 2880.4  be before the verb and the object has to be after the verb but lots of other languages have much\n",
      "2880.4 2887.12  free word order and instead use different forms of nouns to show you what's the subject or the\n",
      "2887.12 2893.52  object of the sentence and dependency grammars can often seem much more natural for those kinds of\n",
      "2893.52 2899.52  languages dependency grammars were also prominent at the very beginnings of computational linguistics so\n",
      "2900.32 2907.12  one of the first people working computational linguistics in the US was David Hayes so the\n",
      "2907.12 2912.48  professional society for computational linguistics is called the association for computational linguistics\n",
      "2912.48 2917.12  and he was actually one of the founders of the association for computational linguistics\n",
      "2917.12 2924.64  and he published in the early 1960s and early perhaps the first dependency grammar past how\n",
      "2924.64 2934.0  you dependency parser okay yeah a little teeny note just in case you see other things when\n",
      "2934.0 2940.64  when you have these arrows you can draw them in either direction you either draw arrows from their\n",
      "2940.64 2946.88  head or to the dependent or from the dependent to the head and actually different people have\n",
      "2946.88 2953.44  done one and the other right so the way ten year drew them was to draw them from the head to the\n",
      "2953.44 2958.08  the dependent and we're following that convention but you know if you're looking at something that\n",
      "2958.08 2964.32  somebody else has written with dependency arrows the first thing you have to work out is are they\n",
      "2964.32 2971.76  using the arrow heads at the heads or the dependence now and not one other thing here is that\n",
      "2971.76 2979.6800000000003  we a sentence is seen as having the overall head word of the sentence which every other word of\n",
      "2979.6800000000003 2986.48  the sentence hangs off it's a common convention to add this sort of fake route to every sentence\n",
      "2986.48 2994.48  that then points to the head word of the whole sentence here completed that just tends to make\n",
      "2994.48 3001.44  the algorithmic stuff easier because then you can say that every word of the sentence is dependent\n",
      "3001.44 3008.0  on precisely one other node where what you can be dependent on is either another word on the\n",
      "3008.0 3014.48  sentence or the fake route of the sentence and when we build our parsers we will introduce that\n",
      "3014.48 3027.04  fake route okay so that's sort of dependency grammars and dependency structure I now want to\n",
      "3027.04 3036.96  get us back to natural language processing and starting to build parsers for dependency grammars\n",
      "3036.96 3045.36  but before doing that I just want to say yeah where do we get our data from and that's actually\n",
      "3045.36 3056.08  an interesting story in some sense so the answer to that is well what we do is get\n",
      "3056.08 3063.2  human beings commonly linguists or other people who are actually interested in the structure\n",
      "3063.2 3071.7599999999998  of human sentences and we get them to sit around and hand parse sentences and give them dependency\n",
      "3071.7599999999998 3082.64  structures and we collect a lot of those parsers and we call that a tree bank and so this is\n",
      "3082.64 3090.3199999999997  something that really only started happening in the late 80s and took off in a big away in the 90s\n",
      "3090.3199999999997 3096.7999999999997  until then no one had attempted to build tree banks lots of people had attempted to build parsers\n",
      "3096.7999999999997 3104.24  and it seemed like well if you want to build a parser the efficient way to do it is to start writing\n",
      "3104.24 3110.08  a grammar so you start writing some grammar rules and you start writing a lexicon with words and\n",
      "3110.08 3117.2799999999997  parts of speech and you sit around working on your grammar when I was a PhD student one of my first\n",
      "3117.2799999999997 3124.56  summer jobs was spending the summer handwriting a grammar and it sort of seems like writing a\n",
      "3124.56 3129.6  grammar is more efficient because you're writing this one general thing that tells you the structure\n",
      "3129.6 3135.7599999999998  of a human language but there's just been this massive sea change partly driven by the adoption\n",
      "3135.76 3142.6400000000003  of machine learning techniques where it's now seen as axiomatic that the way to make progress\n",
      "3142.6400000000003 3151.44  is to have annotated data namely here a tree bank that shows you the structure of sentences\n",
      "3151.44 3158.48  and so what I'm showing here is a teeny extract from a universal dependencies tree bank and so that's\n",
      "3158.48 3164.2400000000002  what I mentioned earlier that this has been this effort to try and have a common dependency\n",
      "3164.24 3169.4399999999996  grammar representation that you can apply to lots of different human languages and so you can go\n",
      "3169.4399999999996 3175.2799999999997  over to this URL and see that there's about 60 different languages at the moment which have universal\n",
      "3175.2799999999997 3185.2799999999997  dependencies tree banks. So why are tree banks good? I mean it sort of seems like it's bad news if\n",
      "3185.2799999999997 3192.08  you have to have people sitting around for weeks and months hand-posing sentences it seems a lot\n",
      "3192.08 3200.3199999999997  slower and actually a lot less useful than having somebody writing a grammar which just has\n",
      "3201.04 3210.4  you know a much bigger multiply factor in the utility of their effort. It turns out that although\n",
      "3210.4 3216.88  that initial feeling seems sort of valid that in practice there's just a lot more you can do with\n",
      "3216.88 3226.4  the tree bank. So why are tree banks great? You know one reason is the tree banks are highly reusable\n",
      "3226.4 3233.04  so typically when people have written grammars they've written grammars for you know one particular\n",
      "3233.84 3240.1600000000003  parser and the only thing it was ever used in is that one particular parser but when you build a\n",
      "3240.16 3249.3599999999997  tree bank that's just a useful data resource and people use it for all kinds of things. So the\n",
      "3249.3599999999997 3256.48  well-known tree banks have been used by hundreds and hundreds of people and although all tree banks\n",
      "3256.48 3262.72  were initially built for the purposes of hey let's help natural language processing systems\n",
      "3262.72 3268.0  it turns out that people have actually been able to do lots of other things with tree banks.\n",
      "3268.0 3274.64  So for example these days psycho-linguists commonly use tree banks to get various kinds of\n",
      "3274.64 3281.52  statistics about data for thinking about psycho-linguistic models. Linguists use tree banks for\n",
      "3281.52 3287.52  looking at patterns of different syntactic constructions that occur that there's just been a lot\n",
      "3287.52 3295.12  of reuse of this data for all kinds of purposes but they have other advantages that I mentioned here\n",
      "3295.12 3300.3199999999997  you know when people are just sitting around saying oh what sentences are good they tend to\n",
      "3300.3199999999997 3306.24  only think of the core of language where lots of weird things happen in language and so if you\n",
      "3306.24 3312.08  actually just have some sentences and you have to go off and parse them then you actually have to\n",
      "3312.08 3319.92  deal with the totality of language. Since you're parsing actual sentences you get statistics so\n",
      "3319.92 3325.44  you naturally get the kind of statistics that are useful to machine learning systems by\n",
      "3325.44 3331.28  constructing a tree bank where you don't get them for free if you handwrite a grammar but then a\n",
      "3331.28 3341.44  final way which is perhaps the most important of all is if you actually want to be able to do\n",
      "3343.2000000000003 3349.6  science of building systems you need a way to evaluate these NLP systems.\n",
      "3349.6 3359.68  I mean it seems hard to believe now but you know back in the 90s 80s when people built NLP\n",
      "3359.68 3367.44  parsers it was literally the case that the way they were evaluated was you said to your friend\n",
      "3367.44 3372.64  oh I built this parser type in a sentence on the terminal and see what it gives you back it's\n",
      "3372.64 3379.92  pretty good hey and that was just the way business was done whereas what we'd like to know is well\n",
      "3379.92 3385.92  as I showed you earlier English sentences can have lots of different parsers commonly can this\n",
      "3387.52 3393.8399999999997  system choose the right parsers for particular sentences and therefore have the basis of\n",
      "3394.72 3400.72  interpreting them as a human being would and well we can only systematically do that evaluation\n",
      "3400.72 3406.48  if we have a whole bunch of sentences that have been handparsed by humans with their correct\n",
      "3406.48 3414.72  interpretations so the rise of tree banks turned parser building into an empirical science where people\n",
      "3414.72 3422.7999999999997  could then compete rigorously on the basis of look my parser has 2% higher accuracy than your parser\n",
      "3422.7999999999997 3430.64  in choosing the correct parsers for sentences. Okay so well how do we build a parser\n",
      "3430.64 3436.7999999999997  once we've got dependencies so there's sort of a bunch of sources of information that you could\n",
      "3436.7999999999997 3445.8399999999997  hope to use so one source of information is looking at the words on either end of the dependency\n",
      "3445.8399999999997 3453.52  so discussing issues that seems a reasonable thing to say and so it's likely that issues\n",
      "3453.52 3463.04  could be the object of discussing whereas if it was some other word right if you were thinking of\n",
      "3463.04 3470.96  making you know outstanding the object of discussion discussing outstanding that doesn't sound right\n",
      "3470.96 3478.72  so that wouldn't be so good. A second source of information is distance so most dependencies are\n",
      "3478.72 3485.12  relatively short distance some of them aren't some of long distance dependencies but they're\n",
      "3485.12 3492.9599999999996  relatively rare the vast majority of dependencies nearby and another source of information is the\n",
      "3492.9599999999996 3503.9199999999996  intervening material so there are certain things that dependencies rarely span so clauses and sentences\n",
      "3503.92 3512.56  are normally organized around verbs and so dependencies rarely span across intervening verbs.\n",
      "3513.6800000000003 3519.52  We can also use punctuation and written language things like commas which can give some indication\n",
      "3519.52 3527.44  of the structure and so punctuation may also indicate bad places to have long distance dependencies\n",
      "3527.44 3536.56  over and there's one final source of information which is what's referred to as valency which is\n",
      "3536.56 3543.36  forehead what kind of information does it usually have around it so if you have a noun\n",
      "3545.2000000000003 3551.6  there are things that you just know about what kinds of dependence nouns normally have so it's\n",
      "3551.6 3561.12  common that it will have a determiner to the left the cat on the other hand it's not going to be the\n",
      "3561.12 3566.96  case that there's a determiner to the right cat that that's just not what you get in English\n",
      "3568.4 3574.88  on the left you're also likely to have an adjective or modify that's where he had cuddly\n",
      "3574.88 3582.8  but again it's not so likely you're going to have the adjective or modifier over on the right\n",
      "3582.8 3589.52  for cuddly so there are sort of facts about what things different kinds of words take on the left\n",
      "3589.52 3595.2000000000003  and the right and so that's the valency of the heads and that's also a useful source of information\n",
      "3596.8 3604.6400000000003  okay so what do we need to do using that information to build a parser well effectively\n",
      "3604.64 3610.8799999999997  what we do is have a sentence I'll give a talk tomorrow on your networks and what we have to do\n",
      "3610.8799999999997 3617.04  is say for every word in that sentence we have to choose some other word that it's a dependent of\n",
      "3617.7599999999998 3625.04  where one possibility is it's a dependent of root so we're giving it a structure where we're\n",
      "3625.04 3633.44  saying okay for this word I've decided that it's a dependent on networks and then for this word\n",
      "3633.44 3644.64  it's also a dependent on networks and for this word it's a dependent on give so we're choosing\n",
      "3645.68 3653.2000000000003  one for each word and there are usually a few constraints so only one word is a dependent of root\n",
      "3653.2000000000003 3660.2400000000002  we have a tree we don't want cycles so we don't want to say that word a is dependent on word b and\n",
      "3660.24 3671.52  word b is dependent on word a and then there's one final issue which is whether arrows can cross\n",
      "3671.52 3678.56  or not so in this particular sentence we actually have these crossing dependencies you can see there\n",
      "3678.56 3685.52  I'll give a talk tomorrow on neural networks and this is the correct dependency paths for this\n",
      "3685.52 3692.0  sentence because what we have here is that it's a talk and it's a talk on neural networks so the\n",
      "3692.0 3699.6  on neural networks modifies the talk but which leads to these crossing dependencies I didn't have to\n",
      "3699.6 3706.08  say it like that I could have said I'll give a talk on neural networks tomorrow and then on your\n",
      "3706.08 3715.04  networks would be next to the talk so most of the time in languages dependencies are projector of\n",
      "3715.04 3721.68  the things stay together so the dependencies have a kind of a nesting structure of the kind that\n",
      "3721.68 3728.72  you also see in context free grammars but most languages have at least a few phenomena where you\n",
      "3728.72 3736.48  ended up with these ability for phrases to be split apart which lead to non-projective dependencies\n",
      "3736.48 3743.68  so in particular one of them in English is that you can take modifying phrases and clauses like\n",
      "3743.68 3749.44  the on neural networks here and shift them right towards the end of the sentence and get I'll give\n",
      "3749.44 3755.12  a talk tomorrow on neural networks and that then leads to non-projective sentences\n",
      "3757.6 3763.3599999999997  so a pause is projected if there are no crossing dependency arcs when the words are laid out\n",
      "3763.3599999999997 3770.0  and then in your order with all arcs above the words and if you have a dependency paths that\n",
      "3770.0 3775.84  correspond to a context free grammar tree it actually has to be protective because context free\n",
      "3775.84 3781.84  grammars necessarily have this sort of nested tree structure following the linear order\n",
      "3782.88 3788.8  but dependency grammars normally allow non-projective structures to account for\n",
      "3788.8 3794.16  displacement constituents and you can't easily get the semantics of certain\n",
      "3794.16 3800.8799999999997  constructions right without these non-projective dependencies so here's another example in English\n",
      "3800.8799999999997 3808.0  with question formation with what's called preposition stranding so the sentence is who did\n",
      "3808.0 3814.8799999999997  bill by the coffee from yesterday there's another way I could have said this it's less natural in\n",
      "3814.88 3825.52  English but I could have said from who did bill by the coffee yesterday in many languages of the\n",
      "3825.52 3833.36  world that's the only way you could have said it and when you do that from who is kept together\n",
      "3833.36 3839.92  and you have a projective pause for the sentence but English allows and indeed much prefers\n",
      "3839.92 3847.28  you to do what is referred to as preposition stranding where you move the who but you just leave\n",
      "3847.28 3854.4  the preposition behind and so you get who did bill by the coffee from yesterday and so then\n",
      "3854.4 3859.52  we're ending up with this non-projective dependency structure as I've shown there\n",
      "3861.92 3868.4  okay I'll come back to non-projectivity in a little bit how do we go about building\n",
      "3868.4 3876.08  dependency parsers well there are a whole bunch of ways that you can build dependency parsers\n",
      "3876.8 3882.32  very quickly I'll just say a few names and I'll tell you about one of them so you can use dynamic\n",
      "3882.32 3888.2400000000002  programming methods to build dependency parsers so I showed earlier that you can have an exponential\n",
      "3888.2400000000002 3893.36  number of parsers for a sentence and that sounds like really bad news for building a system\n",
      "3893.36 3898.96  well it turns out that you can be clever and you can work out a way to dynamic program finding\n",
      "3898.96 3905.6800000000003  that exponential number of parsers and then you can have an oh and cubed algorithm so you could do that\n",
      "3907.2000000000003 3913.52  you can use graph algorithms and I'll say a bit about that later by that may spill into next time\n",
      "3914.4 3922.2400000000002  so you can see since we're wanting to kind of connect up all the words into a tree using\n",
      "3922.24 3927.9199999999996  graph edges that you could think of doing that using using a minimum spanning tree algorithm of\n",
      "3927.9199999999996 3934.08  the sort that you hopefully saw in CS 161 and so that idea has been used for parsing\n",
      "3934.8799999999997 3941.4399999999996  constraint satisfaction ideas that you might have seen in CS 221 have been used for dependency parsing\n",
      "3943.04 3948.72  but the way I'm going to show now is transition based parsing or sometimes referred to as\n",
      "3948.72 3957.7599999999998  deterministic dependency parsing and the idea of this is once going to use a transition system\n",
      "3957.7599999999998 3964.56  so that's like shift reduce parsing if you've seen shift reduce parsing in something like a\n",
      "3964.56 3971.68  compiler's class or formal languages class that shift and reduce transition steps and so use\n",
      "3971.68 3980.24  a transition system to guide the construction of parsers and so let me just explain about that\n",
      "3981.68 3993.9199999999996  so let's see so this was an idea that was made prominent by Yorkin Nivre who's a Swedish\n",
      "3993.92 4003.84  computational linguist who introduced this idea of greedy transition based parsing so his idea is\n",
      "4003.84 4010.7200000000003  well what we're going to do for dependency parsing is we're going to be able to parse sentences\n",
      "4010.7200000000003 4017.92  by having a set of transitions which are kind of like shift reduce parser and it's going to just\n",
      "4017.92 4026.48  work left to right bottom up and parse a sentence so we're going to say we have a stack sigma\n",
      "4027.36 4033.6800000000003  buffer beta of the words that we have to process and we're going to build up a set of dependency\n",
      "4033.6800000000003 4040.88  arcs by using actions which are shift and reduce actions and putting those together this will give\n",
      "4040.88 4047.76  us the ability to put parse structures over sentences and let me go through the details of\n",
      "4047.76 4054.88  this and this is a little bit hairy when you first see it that's not so complex really and\n",
      "4055.6800000000003 4064.7200000000003  it's this kind of transition based dependency parser is what we'll use in assignment 3 so what we\n",
      "4064.7200000000003 4071.6000000000004  have so this is our transition system we have a starting point where we start with a stack that\n",
      "4071.6 4077.7599999999998  just has the root symbol on it and a buffer that has the sentence that's about to parse we're about\n",
      "4077.7599999999998 4087.2  to parse and so far we haven't built any dependency arcs and so at each point in time we can choose one\n",
      "4087.2 4099.68  of three actions we can shift which moves the next word onto the stack we can then do actions\n",
      "4099.68 4106.8  that are the reduce actions so there are two reduce actions to make it a dependency grammar we\n",
      "4106.8 4114.8  can either do a left arc reduce or a right arc reduce so when we do either of those we take\n",
      "4114.8 4122.64  the top two items on the stack and we make one of them a dependent of the other one so we can\n",
      "4122.64 4130.400000000001  either say okay let's make wi a dependent of wj or else we can say okay let's make wj a dependent\n",
      "4130.400000000001 4140.400000000001  of wi and so the result of when we do that is the one that's the dependent disappears from the stack\n",
      "4140.400000000001 4147.52  and so in the stacks over here there's one less item but then we add a dependency arc to our\n",
      "4147.52 4154.72  arc set so that we say that we've got either a dependency from j to i or a dependency from i to j\n",
      "4155.4400000000005 4162.72  and commonly when we do this we actually also specify what grammatical relation connects the two\n",
      "4162.72 4171.280000000001  such as subject object now modifier and so we also have here a relation that's still probably\n",
      "4171.28 4180.16  still very abstract so let's go through an example so this is how a simple transition based dependency\n",
      "4180.16 4186.24  parser what's referred to as an arc standard transition based dependency parser would parse up i8\n",
      "4186.24 4192.32  the fish so remember these are the different operations that we can apply so to start off with we\n",
      "4192.32 4199.2  have root on the stack and the sentence in the buffer and we have no dependency arcs constructed\n",
      "4199.2 4205.84  so we have to choose one of the three actions and when there's only one thing on the stack the only\n",
      "4205.84 4213.92  thing we can do is shift so we shift now the stack looks like this so now we have to take another\n",
      "4213.92 4220.72  action and at this point we have a choice because we could immediately reduce so you know we could\n",
      "4220.72 4228.639999999999  say okay let's just make i a dependent of root and we'd get a stack size of one again but that\n",
      "4228.64 4236.0  would be the wrong thing to do because i isn't the head of the sentence so what we should instead do\n",
      "4236.0 4244.0  is shift again and get i8 on the stack and fish still in the buffer well at that point we keep\n",
      "4244.0 4253.12  on parsing a bit further and so now what we can do is say well wait a minute now i is a dependent\n",
      "4253.12 4262.32  of eight and so we can do a left arc reduce and so i disappears from the stack so here's our new stack\n",
      "4262.32 4270.4  but we add to the set of arcs that we've added that i is the subject of eight okay well after that\n",
      "4271.28 4276.32  we could have we could reduce again because there's still two things on the stack but that'd be the\n",
      "4276.32 4284.719999999999  wrong thing to do the right thing to do next would be to shift fish onto the stack and then at that\n",
      "4284.719999999999 4295.12  point we can do a right arc reduce saying that eight is the object of fish and add a new dependency\n",
      "4295.12 4304.88  to our dependency set and then we can one more time do a right arc reduce to say that eight is the\n",
      "4304.88 4311.6  root of the whole sentence and add in that extra root relation with our pseudo root and at that\n",
      "4311.6 4318.4800000000005  point we reach the end condition so the end condition was the buffer was empty and there's one thing\n",
      "4318.4800000000005 4326.08  the root on the stack and at that point we can finish so this little transition machine does the\n",
      "4326.08 4336.0  parsing up of the sentence but there's one thing that's left to explain still here which is how do\n",
      "4336.0 4342.96  you choose the next action so as soon as you have two things or more on the stack what you do next\n",
      "4343.6 4348.24  you've always got a choice you could keep shifting at least if there's still things on the buffer\n",
      "4348.24 4354.96  or you can do a left arc or you can do a right arc and how do you know what choices correct\n",
      "4354.96 4360.16  and well one answer to that is to say well you don't know what choices correct and that's why\n",
      "4360.16 4367.36  parsing is hard and sentences are ambiguous you can do any of those things you have to explore\n",
      "4367.36 4374.32  all of them and well if you naively explore all of them then you do an exponential amount of work\n",
      "4374.32 4385.2  to parse the sentence so in the early 2000s you're a commandeer phrase and you know that's essentially\n",
      "4385.2 4394.719999999999  what people have done in the 80s and 90s is explore every path but in the early 2000s you're a commandeer\n",
      "4394.719999999999 4402.24  phrase essential observation was but wait a minute we know about machine learning now so why don't\n",
      "4402.24 4411.44  I try and train a classifier which predicts what the next action I should take is given this stack\n",
      "4411.44 4420.32  and buffer configuration because if I can write a machine learning classifier which can nearly\n",
      "4420.32 4429.599999999999  always correctly predict the next action given a stack and buffer then I'm in a really good position\n",
      "4429.6 4436.320000000001  because then I can build what's referred to as a greedy dependency parser which just goes\n",
      "4436.320000000001 4444.64  bang bang bang word at a time okay here's the next thing run classifier choose next action run\n",
      "4444.64 4450.88  classifier choose next action run classifier choose next action so that the amount of work that\n",
      "4450.88 4459.6  we're doing becomes linear in the length of the sentence rather than that being cubic in the length\n",
      "4459.6 4464.64  of the sentence using dynamic programming or exponential in the length of the sentence if you\n",
      "4464.64 4472.8  don't use dynamic programming so for each at each step we predict the next action using some\n",
      "4472.8 4478.400000000001  discriminative classifier so starting off he was using things like support vector machines\n",
      "4478.4 4483.679999999999  but it can be anything at all like a softmax classifier that's closer to our neural networks\n",
      "4483.679999999999 4490.16  and there are either for what I presented three classes if you're just thinking of the two\n",
      "4490.16 4495.759999999999  reduces in the shift or if you're thinking of you're also assigning a relation and you have a set\n",
      "4495.759999999999 4503.599999999999  of our relations like 20 relations then that's be sort of 41 moves that you could decide on at each\n",
      "4503.6 4510.4800000000005  point and the features are effectively the configurations I was showing before what's the top of the\n",
      "4510.4800000000005 4515.68  stack word what part of speech is it what's the first word in the buffer what's that words part\n",
      "4515.68 4521.92  of speech etc and so in the simplest way of doing this you're now doing no search at all you\n",
      "4521.92 4528.240000000001  would just sort of take each configuration and turn decide the most likely next move and you make\n",
      "4528.24 4535.44  it and that's a greedy dependency parser which is widely used you can do better if you want to do\n",
      "4535.44 4542.32  a lot more work so you can do what's called a beam search where you maintain a number of fairly\n",
      "4542.32 4549.76  good parse prefixes at each step and you can extend them out further and then you can evaluate\n",
      "4549.76 4556.719999999999  later on which of those seems to be the best and so beam search is one technique to improve dependency\n",
      "4556.72 4566.8  parsing by doing a lot of work and it turns out that although these greedy transition based parsers\n",
      "4567.92 4574.56  are a fraction worse than the best possible ways known to parse sentences that they actually work\n",
      "4574.56 4583.6  very accurately almost as well and they have this wonderful advantage that they give you linear time\n",
      "4583.6 4591.200000000001  parsing in terms of the length of your sentences and text and so if you want to do a huge amount of\n",
      "4591.200000000001 4599.68  parsing they're just a fantastic thing to use because you've then got an algorithm that scales to\n",
      "4599.68 4608.160000000001  the size of the web okay so I'm kind of a little bit behind so I guess I'm not going to get through all\n",
      "4608.16 4614.88  these slides today and we'll have to finish out the final slides tomorrow but just to push a teeny\n",
      "4614.88 4623.12  bit further I'll just save a couple more on the sort of what Neyfrit did for dependency parser and\n",
      "4623.12 4629.44  then I'll sort of introduce the neural form of that in the next class so conventionally you had this\n",
      "4629.44 4635.92  sort of stack and buffer configuration and you wanted to build a machine learning classifier\n",
      "4635.92 4644.72  and so the way that was done was by using symbolic features of this configuration and what kind of\n",
      "4645.6 4653.28  symbolic features did you use use these indicator features that picked out a small subset normally\n",
      "4653.28 4659.36  one to three elements of the configuration so you'd have a feature that could be something like\n",
      "4660.08 4665.36  the thing on the top of the stack is the word good which is an adjective or it could be\n",
      "4665.36 4670.88  the thing on the top of the stack is an adjective and the thing that's first and the buffer is an\n",
      "4670.88 4676.32  noun or it could just be looking at one thing and saying the first thing and the buffer is a verb\n",
      "4677.04 4684.4  so you'd have all of these features and because these features commonly involved words and commonly\n",
      "4684.4 4692.08  involved conjunctions of several conditions you had a lot of features and you know having\n",
      "4692.08 4699.44  mentions of words and conjunctions and conditions definitely helped to make these parsers work better\n",
      "4700.24 4706.72  but nevertheless because you had all of these sort of one zero symbolic features that you had a\n",
      "4706.72 4714.0  ton of such features so commonly these parsers were built using something like you know a million\n",
      "4714.0 4721.44  to ten million different features of sentences and I mentioned already the importance of evaluation\n",
      "4721.44 4730.24  let me just sort of quickly say how these parsers were evaluated so to evaluate a\n",
      "4731.36 4739.04  a parser for a particular sentence it was handpars our test set was handpars in the tree banks so we\n",
      "4739.04 4745.919999999999  have gold dependencies what the human thought were right and so we can write those down those\n",
      "4745.92 4753.84  dependencies down as statements of saying the first word is the dependent of the second word\n",
      "4753.84 4761.28  fire a subject dependency and then the parser is also going to make similar claims as to what's\n",
      "4761.28 4768.4800000000005  a dependent on what and so there are two common metrics that are used one is just are you getting\n",
      "4768.4800000000005 4775.76  these dependency facts right so both of these dependency facts match and so that's referred to\n",
      "4775.76 4782.320000000001  as the unlabeled accuracy score where we're just sort of measuring accuracies which are of all\n",
      "4782.320000000001 4789.04  of the dependencies in the gold sentence and remember we have one dependency per word in the\n",
      "4789.04 4795.280000000001  sentence so here we have five how many of them are correct and that's our unlabeled accuracy\n",
      "4795.280000000001 4803.4400000000005  score of 80 percent but a slightly more rigorous and valuation is to say well no we're also going\n",
      "4803.44 4809.28  to label them and we're going to say that this is the subject that's actually called the root\n",
      "4809.28 4819.44  this one's the object so these dependencies have labels and you also need to get the grammatical\n",
      "4819.44 4826.32  relation label right and so that's then referred to as labeled accuracy score and although I got\n",
      "4826.32 4834.24  those two right for that is hmm I guess according to this example actually this is wrong it looks\n",
      "4834.24 4841.5199999999995  like I got oh no this is wrong there sorry that one's wrong there okay so I only got two of the\n",
      "4842.16 4848.4  dependencies correct in the sense that I both got what depends on what and the label correct\n",
      "4848.4 4855.679999999999  and so my labeled accuracy score is only 40 percent okay so I'll stop there now for the\n",
      "4855.68 4863.12  introduction for dependency parsing and I still have an IOU which is how we can then bring\n",
      "4863.12 4869.360000000001  neural nets into this picture and how they can be used to improve dependency parsing so I'll\n",
      "4869.36 4886.16  do that at the start of next time before then proceeding further into neural language models\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(audio_segments)):\n",
    "    start_audio = audio_segments[i]['start']\n",
    "    end_audio = audio_segments[i]['end']\n",
    "    transcribed_text = audio_segments[i]['text']\n",
    "    print(start_audio, end_audio, transcribed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f870ac84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Okay, so for today, we're actually going to take a bit of a change of pace from what\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_segments[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6994a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:12.480000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "res = datetime.timedelta(seconds=12.48)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd39e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:12'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.strftime('%H:%M:%S', time.gmtime(12.48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8833cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social _ Ethical Considerations [-Ldg4qFL6bU].json\n",
      "['Stanford CS224N NLP with Deep Learning ', ' Winter 2021 ', ' Lecture 16 - Social _ Ethical Considerations ']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "path = 'Stanford_Speech_to_Text_Dump/*'\n",
    "all_files = glob.glob(path)\n",
    "data = []\n",
    "for file in all_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    print(file_name)\n",
    "    split_data = file_name.split('.')\n",
    "    split_title = split_data[0].split('[')\n",
    "    youtube_link = split_title[-1].replace(']', '')\n",
    "    title = split_title[0].split('｜')\n",
    "    print(title)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0998b4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stanford_Speech_to_Text_Dump/Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social _ Ethical Considerations ']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = \"Stanford_Speech_to_Text_Dump/Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social _ Ethical Considerations \"\n",
    "str1.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b14eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Stanford_Speech_to_Text_Dump/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef0abaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stanford_Speech_to_Text_Dump/Stanford CS224N NLP with Deep Learning ｜ Winter 2021 ｜ Lecture 16 - Social _ Ethical Considerations [-Ldg4qFL6bU].json'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob(path)\n",
    "all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bba42da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stanford_Speech_to_Text_Dump/Stanford_CS224N_NLP_with_Deep_Learning_｜_Winter_2021_｜_Lecture_16_-_Social___Ethical_Considerations_[-Ldg4qFL6bU].json'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0].replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "207dfa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    os.rename(\"Stanford_Speech_to_Text_Dump/{}\".format(file_name), \"Stanford_Speech_to_Text_Dump/{}\".format(file_name.replace(' ', '_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72228d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd8d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
