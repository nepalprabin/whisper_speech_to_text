{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c861ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29503fef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "incorrect audio shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m mel \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mlog_mel_spectrogram(audio)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# detect the spoken language\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m _, probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected language: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(probs, key\u001b[38;5;241m=\u001b[39mprobs\u001b[38;5;241m.\u001b[39mget)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/whisper/decoding.py:43\u001b[0m, in \u001b[0;36mdetect_language\u001b[0;34m(model, mel, tokenizer)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# skip encoder forward pass if already-encoded audio features were given\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m!=\u001b[39m (model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx, model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_state):\n\u001b[0;32m---> 43\u001b[0m     mel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# forward pass using a single token, startoftranscript\u001b[39;00m\n\u001b[1;32m     46\u001b[0m n_audio \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/whisper/model.py:152\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m    150\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincorrect audio shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "\u001b[0;31mAssertionError\u001b[0m: incorrect audio shape"
     ]
    }
   ],
   "source": [
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"audio.mp3\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec32f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(fp16=False)\n",
    "result = whisper.decode(model, mel, options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50547cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e2b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytube\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db68482",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = YouTube('http://youtube.com/watch?v=9bZkp7q19f0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a29be0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = yt.streams.filter(only_audio=True)[0].download('audio.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfab9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e8db630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(audio, language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "912467a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Gyarimdashin It's coming now The Sad being You're right, you're right, you're right, you're right When you start, you'll see the same Open Gangnam Style Gangnam Style Open Gangnam Style Gangnam Style Open Gangnam Style Hey, sexy lady Open Gangnam Style Hey, sexy lady When you look clean, you're a woman When you put your head on the floor, your hair is a woman but when you put up a lot of clothes, you're a woman But if you like the more, you're a woman You're right, you're right, you're right, you're right But when you put your head on the floor, your hair is a woman When I call you exactly, you'll see a woman You're right, you're right, you're right, you're right Hey, I'm the one who loves you Yeah, hey, yeah, you're the one who loves me I'm the one who loves you Yeah, hey, yeah, you're the one who loves me Let's go to the end of the clock Oppa's Gangnam Style Gangnam Style Oppa's Gangnam Style Gangnam Style Oppa's Gangnam Style Hey, sexy lady Oppa's Gangnam Style Hey, sexy lady Oppa's Gangnam Style We're the know, we're the know Baby, baby, I'm more know I'm more know We're the know, we're the know Baby, baby, I'm more know I'm more know I'm the know, I'm the know Oppa's Gangnam Style Hey, sexy lady Oppa's Gangnam Style Hey, sexy lady Oppa's Gangnam Style Oppa's Gangnam Style\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448c2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "## pytube\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f9c4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_transcribe(model, url):\n",
    "    yt = YouTube(url)\n",
    "    audio = yt.streams.filter(only_audio=True)[0].download('audio.mp4')\n",
    "    model = whisper.load_model(model)\n",
    "    result = model.transcribe(audio, language='ne')\n",
    "    return result['text']\n",
    "\n",
    "# whisper_transcribe('base', 'http://youtube.com/watch?v=9bZkp7q19f0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a88b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prabinnepal/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=whisper_transcribe,\n",
    "    inputs=[\n",
    "        gr.Dropdown(\n",
    "            label=\"Select Model\",\n",
    "            choices=[\n",
    "                \"tiny.en\",\n",
    "                \"base.en\",\n",
    "                \"small.en\",\n",
    "                \"medium.en\",\n",
    "                \"tiny\",\n",
    "                \"base\",\n",
    "                \"small\",\n",
    "                \"medium\",\n",
    "                \"large\",\n",
    "            ],\n",
    "            value=\"base\",\n",
    "        ),\n",
    "        gr.Textbox(label=\"Paste YouTube link here\"),\n",
    "#         gr.Audio(label=\"Upload Audio File\", source=\"upload\", type=\"filepath\"),\n",
    "    ],\n",
    "#     inputs=gr.Textbox(lines=1, placeholder=\"Enter YouTube URL\"),\n",
    "    outputs=\"text\",\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77880ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3fa60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
